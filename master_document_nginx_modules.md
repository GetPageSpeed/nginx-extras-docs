# *accept-language*: NGINX Accept-Language module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-accept-language
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_accept_language_module.so;
```


This document describes nginx-module-accept-language [v1.0.0](https://github.com/dvershinin/nginx_accept_language_module/releases/tag/1.0.0){target=_blank} 
released on Oct 30 2018.

<hr />

This module parses the `Accept-Language` header and gives the most suitable locale for the user from a list of supported locales from your website.

## Syntax

    set_from_accept_language $lang en ja pl;
    
* `$lang` is the variable in which to store the locale
* `en ja pl` are the locales supported by your website
  
If none of the locales from `Accept-Language` is available on your website, it sets the variable to the first locale of your website's supported locales (in this case, `en`).
  
## Caveat

It currently assumes that the `Accept-Language` is sorted by quality values (from my tests it's the case for safari, firefox, opera and ie) and discards q (see http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html). 
In the situation where I'm using the module, this assumption works... but buyer beware :-)

## Example configuration

If you have different subdomains for each languages

```nginx
server {
    listen 80;
    server_name your_domain.com;
    set_from_accept_language $lang en ja zh;
    rewrite ^/(.*) http://$lang.your_domain.com redirect;
}
```


Or you could do something like this, redirecting people coming to '/' to /en (or /pt):

```nginx
location / {
    set_from_accept_language $lang pt en;
     if ( $request_uri ~ ^/$ ) {
       rewrite ^/$ /$lang redirect;
       break;
     }
}
```


## Why did I create it? 

I'm using page caching with merb on a multi-lingual website and I needed a way to serve the correct language page from the cache
I'll soon put an example on http://gom-jabbar.org

## Acknowledgement

Thanks to Evan Miller for his [guide on writing nginx modules](http://emiller.info/nginx-modules-guide.html).

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-accept-language](https://github.com/dvershinin/nginx_accept_language_module){target=_blank}.

# *ajp*: Support AJP protocol proxy with NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-ajp
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_ajp_module.so;
```


This document describes nginx-module-ajp [v0.3.3](https://github.com/dvershinin/nginx_ajp_module/releases/tag/v0.3.3){target=_blank} 
released on Dec 19 2023.

<hr />

__nginx\_ajp\_module__ - support AJP protocol proxy with Nginx

## Synopsis

        http {
                upstream tomcats {
                        server 127.0.0.1:8009;
                        keepalive 10;
                }

                server {

                        listen 80;

                        location / {
                                ajp_keep_conn on;
                                ajp_pass tomcats;
                        }
                }
        }

## Description

With this module, Nginx can connect to AJP port directly. The motivation of writing these modules is Nginx's high performance and robustness.

## Directives

## ajp\_buffers

__syntax:__ _ajp\_buffers the\_number is\_size;_

__default:__ _ajp\_buffers 8 4k/8k;_

__context:__ _http, server, location_

This directive specifies the number and the size of buffers, into which will be read the response, obtained from the AJP server. By default, the size of one buffer is equal to the size of a page. Depending on platform this is either 4K, 8K or 16K.

## ajp\_buffer\_size

__syntax:__ _ajp\_buffer\_size the\_size;_

__default:__ _ajp\_buffer\_size 4k/8k;_

__context:__ _http, server, location_

This directive sets the buffer size, into which will be read the first part of the response, obtained from the AJP server.

In this part of response the small response-header is located, as a rule.

By default, the buffersize is equal to the size of one buffer in directive `ajp_buffers`; however, it is possible to set it to less.

## ajp\_cache

__syntax:__ _ajp\_cache zone;_

__default:__ _off_

__context:__ _http, server, location_

The directive specifies the area which actually is the share memory's name for caching. The same area can be used in several places. You must set the `ajp_cache_path` first.

## ajp\_cache\_key

__syntax:__ _ajp\_cache\_key line;_

__default:__ _none_

__context:__ _http, server, location_

The directive specifies what information is included in the key for caching, for example

        ajp_cache_key "$host$request_uri$cookie_user";

Note that by default, the hostname of the server is not included in the cache key. If you are using subdomains for different locations on your website, you need to include it, e.g. by changing the cache key to something like

        ajp_cache_key "$scheme$host$request_uri";

## ajp\_cache\_methods

__syntax:__ _ajp\_cache\_methods \[GET HEAD POST\];_

__default:__ _ajp\_cache\_methods GET HEAD;_

__context:__ _main,http,location_

GET/HEAD is syntax sugar, i.e. you can not disable GET/HEAD even if you set just

        ajp_cache_methods  POST;

## ajp\_cache\_min\_uses

__syntax:__ _ajp\_cache\_min\_uses n;_

__default:__ _ajp\_cache\_min\_uses 1;_

__context:__ _http, server, location_

Sets the number of requests after which the response will be cached.

## ajp\_cache\_path

__syntax:__ _ajp\_cache\_path /path/to/cache \[levels=m:n keys\_zone=name:time inactive=time clean\_time=time\];_

__default:__ _none_

__context:__ _http, server, location_

This directive sets the cache path and other cache parameters. Cached data stored in files. Key and filename in cache is md5 of proxied URL. __Levels__ parameter set number of subdirectories in cache, for example for:

        ajp_cache_path  /data/nginx/cache  levels=1:2   keys_zone=one:10m;

file names will be like:

/data/nginx/cache/c/29/b7f54b2df7773722d382f4809d65029c

## ajp\_cache\_use\_stale

__syntax:__ _ajp\_cache\_use\_stale \[updating|error|timeout|invalid\_header|http\_500\];_

__default:__ _ajp\_cache\_use\_stale off;_

__context:__ _http, server, location_

If an error occurs while working with the AJP server it is possible to use a stale cached response. This directives determines in which cases it is permitted. The directive’s parameters match those of the `ajp_next_upstream` directive.

Additionally, the updating parameter permits to use a stale cached response if it is currently being updated. This allows to minimize the number of accesses to AJP servers when updating cached data.

## ajp\_cache\_valid

__syntax:__ _ajp\_cache\_valid \[http\_error\_code|time\];_

__default:__ _none_

__context:__ _http, server, location_

Sets caching time for different response codes. For example, the following directives

        ajp_cache_valid 200 302 10m;
        ajp_cache_valid 404      1m;

set 10 minutes of caching for responses with codes 200 and 302, and 1 minute for responses with code 404.

If only caching time is specified

        ajp_cache_valid 5m;

then only 200, 301, and 302 responses are cached.

In addition, it can be specified to cache any responses using the any parameter:

        ajp_cache_valid 200 302 10m;
        ajp_cache_valid 301      1h;
        ajp_cache_valid any      1m;

Parameters of caching can also be set directly in the response header. This has a higher precedence than setting of caching time using the directive. The “X-Accel-Expires” header field sets caching time of a response in seconds. The value 0 disables to cache a response. If a value starts with the prefix @, it sets an absolute time in seconds since Epoch, up to which the response may be cached. If header does not include the “X-Accel-Expires” field, parameters of caching may be set in the header fields “Expires” or “Cache-Control”. If a header includes the “Set-Cookie” field, such a response will not be cached. Processing of one or more of these response header fields can be disabled using the `ajp_ignore_headers` directive.

## ajp\_connect\_timeout

__syntax:__ _ajp\_connect\_timeout time;_

__default:__ _ajp\_connect\_timeout 60s;_

__context:__ _http, server, location_

This directive assigns a timeout for the connection to the upstream server. It is necessary to keep in mind that this time out cannot be more than 75 seconds.

This is not the time until the server returns the pages, this is the [ ajp\_read\_timeout](#ajp_read_timeout)  statement. If your upstream server is up, but hanging (e.g. it does not have enough threads to process your request so it puts you in the pool of connections to deal with later), then this statement will not help as the connection to the server has been made.

## ajp\_header\_packet\_buffer\_size

__syntax:__ _ajp\_header packet\_buffer\_size;_

__default:__ _ajp\_header\_packet\_buffer\_size 8k;_

__context:__ _http, server, location_

Set the buffer size of Forward Request packet. The range is (0, 2^16).

## ajp\_hide\_header

__syntax:__ _ajp\_hide\_header name;_

__context:__ _http, server, location_

By default, Nginx does not pass headers "Status" and "X-Accel-..." from the AJP process back to the client.  This directive can be used to hide other headers as well.

If the headers "Status" and "X-Accel-..." must be provided, then it is necessary to use directive ajp\_pass\_header to force them to be returned to the client.

## ajp\_ignore\_headers

__syntax:__ _ajp\_ignore\_headers name \[name ...\];_

__default:__ _none_

__context:__ _http, server, location_

This directive(0.7.54+) prohibits the processing of the header lines from the proxy server's response.

It can specify the string as "[X-Accel-Redirect](https://metacpan.org/pod/NginxXSendfile)", "X-Accel-Expires", "Expires" or "Cache-Control".

## ajp\_ignore\_client\_abort

__syntax:__ _ajp\_ignore\_client\_abort on|off;_

__default:__ _ajp\_ignore\_client\_abort off;_

__context:__ _http, server, location_

This directive determines if current request to the AJP-server must be aborted in case the client aborts the request to the server.

## ajp\_intercept\_errors

__syntax:__ _ajp\_intercept\_errors on|off;_

__default:__ _ajp\_intercept\_errors off;_

__context:__ _http, server, location_

This directive determines whether or not to transfer 4xx and 5xx errors back to the client or to allow Nginx to answer with directive error\_page.

Note: You need to explicitly define the error\_page handler for this for it to be useful. As Igor says, "nginx does not intercept an error if there is no custom handler for it it does not show its default pages. This allows to intercept some errors, while passing others as are."

## ajp\_keep\_conn

__syntax:__ _ajp\_keep\_conn on|off;_

__default:__ _ajp\_keep\_conn off;_

__context:__ _http, server, location_

This directive determines whether or not to keep the connection alive with backend server.

## ajp\_next\_upstream

__syntax:__ _ajp\_next\_upstream \[error|timeout|invalid\_header|http\_500|http\_502|http\_503|http\_504|http\_404|off\];_

__default:__ _ajp\_next\_upstream error timeout;_

__context:__ _http, server, location_

Directive determines, in what cases the request will be transmitted to the next server:

- error — an error has occurred while connecting to the server, sending a request to it, or reading its response;
- timeout — occurred timeout during the connection with the server, transfer the request or while reading response from the server;
- invalid\_header — server returned a empty or incorrect answer;
- http\_500 — server returned answer with code 500;
- http\_502 — server returned answer with code 502;
- http\_503 — server returned answer with code 503;
- http\_504 — server returned answer with code 504;
- http\_404 — server returned answer with code 404;
- off — it forbids the request transfer to the next server Transferring the request to the next server is only possible when nothing has been transferred to the client -- that is, if an error or timeout arises in the middle of the transfer of the request, then it is not possible to retry the current request on a different server.

## ajp\_max\_data\_packet\_size

__syntax:__ _ajp\_max\_data\_packet\_size size;_

__default:__ _ajp\_max\_data\_packet\_size 8k;_

__context:__ _http, server, location_

Set the maximum size of AJP's Data packet. The range is \[8k, 2^16\];

## ajp\_max\_temp\_file\_size

__syntax:__ _ajp\_max\_temp\_file\_size size;_

__default:__ _ajp\_max\_temp\_file\_size 1G;_

__context:__ _http, server, location, if_

The maximum size of a temporary file when the content is larger than the proxy buffer.  If file is larger than this size, it will be served synchronously from upstream server rather than buffered to disk.

If ajp\_max\_temp\_file\_size is equal to zero, temporary files usage will be disabled.

## ajp\_pass

__syntax:__ _ajp\_pass ajp-server_

__default:__ _none_

__context:__ _location, if in location_

Directive assigns the port or socket on which the AJP-server is listening. Port can be indicated by itself or as an address and port, for example:

        ajp_pass   localhost:9000;

using a Unix domain socket:

        ajp_pass   unix:/tmp/ajp.socket;

You may also use an upstream block.

        upstream backend  {
                server   localhost:1234;
        }

        ajp_pass   backend;

## ajp\_secret

__syntax:__ _ajp\_secret ajpsecret

__default:__ _none_

Directive assigns the secret of the AJP-server.

## ajp\_pass\_header

__syntax:__ _ajp\_pass\_header name;_

__context:__ _http, server, location_

Permits to pass specific header fields from the AJP server to a client.

## ajp\_pass\_request\_headers

__syntax:__ _ajp\_pass\_request\_headers \[ on | off \];_

__default:__ _ajp\_pass\_request\_headers on;_

__context:__ _http, server, location_

Permits to pass request header fields from the client to server.

## ajp\_pass\_request\_body

__syntax:__ _ajp\_pass\_request\_body \[ on | off \] ;_

__default:__ _ajp\_pass\_request\_body on;_

__context:__ _http, server, location_

Permits to pass request body from the client to server.

## ajp\_read\_timeout

__syntax:__ _ajp\_read\_timeout time;_

__default:__ _ajp\_read\_timeout\_time 60_

__context:__ _http, server, location_

Directive sets the amount of time for upstream to wait for a AJP process to send data.  Change this directive if you have long running AJP processes that do not produce output until they have finished processing.  If you are seeing an upstream timed out error in the error log, then increase this parameter to something more appropriate.

## ajp\_send\_lowat

__syntax:__ _ajp\_send\_lowat \[ on | off \];_

__default:__ _ajp\_send\_lowat off;_

__context:__ _http, server, location, if_

This directive set SO\_SNDLOWAT. This directive is only available on FreeBSD

## ajp\_send\_timeout

__syntax:__ _ajp\_send\_timeout time;_

__default:__ _ajp\_send\_timeout 60;_

__context:__ _http, server, location_

This directive assigns timeout with the transfer of request to the upstream server. Timeout is established not on entire transfer of request, but only between two write operations. If after this time the upstream server will not take new data, then nginx is shutdown the connection.

## ajp\_store

__syntax:__ _ajp\_store \[on | off | path\] ;_

__default:__ _ajp\_store off;_

__context:__ _http, server, location_

This directive sets the path in which upstream files are stored. The parameter "on" preserves files in accordance with path specified in directives _alias_ or _root_. The parameter "off" forbids storing. Furthermore, the name of the path can be clearly assigned with the aid of the line with the variables:

        ajp_store   /data/www$original_uri;

The time of modification for the file will be set to the date of "Last-Modified" header in the response. To be able to safe files in this directory it is necessary that the path is under the directory with temporary files, given by directive `ajp_temp_path` for the data location.

This directive can be used for creating the local copies for dynamic output of the backend which is not very often changed, for example:

        location /images/ {
                root                 /data/www;
                error_page           404 = @fetch;
        }

        location @fetch {
                internal;
                ajp_pass           backend;
                ajp_store          on;
                ajp_store_access   user:rw  group:rw  all:r;
                ajp_temp_path      /data/temp;

                root               /data/www;
        }

To be clear ajp\_store is not a cache, it's rather mirror on demand.

## ajp\_store\_access

__syntax:__ _ajp\_store\_access users:permissions \[users:permission ...\];_

__default:__ _ajp\_store\_access user:rw;_

__context:__ _http, server, location_

This directive assigns the permissions for the created files and directories, for example:

        ajp_store_access  user:rw  group:rw  all:r;

If any rights for groups or all are assigned, then it is not necessary to assign rights for user:

        ajp_store_access  group:rw  all:r;

## ajp\_temp\_path

__syntax:__ _ajp\_temp\_path dir-path \[ level1 \[ level2 \[ level3 \] \] \] ;_

__default:__ _$NGX\_PREFIX/ajp\_temp_

__context:__ _http, server, location_

This directive works like [client\_body\_temp\_path](https://metacpan.org/pod/NginxHttpCoreModule#client_body_temp_path)  to specify a location to buffer large proxied requests to the filesystem.

## ajp\_temp\_file\_write\_size

__syntax:__ _ajp\_temp\_file\_write\_size size;_

__default:__ _ajp\_temp\_file\_write\_size \["#ajp buffer size"\]  \* 2;_

__context:__ _http, server, location, if_

Sets the amount of data that will be flushed to the ajp\_temp\_path when writing. It may be used to prevent a worker process blocking for too long while spooling data.

## Known Issues

\*

## POD ERRORS

Hey! __The above document had some coding errors, which are explained below:__

- Around line 212:

    L<> starts or ends with whitespace

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-ajp](https://github.com/dvershinin/nginx_ajp_module){target=_blank}.

# *array-var*: Array-typed variables for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-array-var
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_array_var_module.so;
```


This document describes nginx-module-array-var [v0.6](https://github.com/openresty/array-var-nginx-module/releases/tag/v0.06){target=_blank} 
released on May 23 2022.

<hr />

array-var-nginx-module - Add support for array-typed variables to nginx config files

installation instructions.

## Status

This module is production ready.

## Synopsis

```nginx
location /foo {
    array_split ',' $arg_files to=$array;

    # use the set_quote_sql_str directive in the ngx_set_misc
    # module to map to each element in the array $array:
    array_map_op set_quote_sql_str $array;

    array_map "name = $array_it" $array;

    array_join ' or ' $array to=$sql_condition;

    # well, we could feed it to ngx_drizzle to talk to MySQL, for example ;)
    echo "select * from files where $sql_condition";
}
```

## Description

This module provides array typed nginx variables to `nginx.conf`.

Under the hood, this module just "abuses" the nginx string values to hold binary pointers
to C data structures (NGINX core's `ngx_array_t` struct on the C land).

The array type gives `nginx.onf` wonderful capabilities of handling value lists. Nowadays, however,
you are highly recommended to use the [ngx_lua](https://github.com/openresty/lua-nginx-module) module
so as to have the full scripting power provided by the Lua language in nginx.


## Directives


## array_split
**syntax:** *array_split &lt;separator&gt; &lt;subject&gt; to=$target_variable*

**default:** *no*

**context:** *http, server, server if, location, location if*

Splits the string value in the `subject` argument with the separator string specified by the
`separator` argument. The result is an array-typed value saved to the nginx variable specified by the `to=VAR` option.

For example,

```nginx
array_split "," $arg_names to=$names;
```

will split the string values in the URI query argument `names` into an array-typed value saved to the custom nginx variable
`$names`.

This directive creates an array-typed variable. Array-typed variables cannot be used outside
the directives offered by this module. If you want to use the values in an array-typed variable
in other contexts,
you must use the [array_join](#array_join) directive to produce a normal string value.


## array_join
**syntax:** *array_split &lt;separator&gt; $array_var*

**default:** *no*

**context:** *http, server, server if, location, location if*

Joins the elements in the array-typed nginx variable (`$array_var`) into a single string value
with the separator specified by the first argument.

For example,

```nginx
location /foo {
    array_split ',' $arg_names to=$names;
    array_join '+' $names;
    echo $names;
}
```

Then request `GET /foo?names=Bob,Marry,John` will yield the response body

```
Bob+Marry+John
```

In the example above, we use the [ngx_echo](https://github.com/openresty/echo-nginx-module) module's [echo](https://github.com/openresty/echo-nginx-module#echo) directive to output
the final result.


## array_map
**syntax:** *array_map &lt;template&gt; $array_var*

**syntax:** *array_map &lt;template&gt; $array_var to=$new_array_var*

**default:** *no*

**context:** *http, server, server if, location, location if*

Maps the string template to each element in the array-typed nginx variable specified. Within
the string template, you can use the special iterator variable `$array_it` to reference the current
array element in the array being mapped.

For example,

```nginx
array_map "[$array_it]" $names;
```

will change each element in the array variable `$names` by putting the square brackets around
each element's string value. The modification is in-place in this case.

If you do not want in-place modifications, you can use the `to=$var` option to specify a new nginx variable to hold the results. For instance,

```nginx
array_map "[$array_it]" $names to=$new_names;
```

where the results are saved into another (array-typed) nginx variable named `$new_names` while
the `$names` variable keeps intact.

Below is a complete example for this:

```nginx
location /foo {
    array_split ',' $arg_names to=$names;
    array_map '[$array_it]' $names;
    array_join '+' $names;
    echo "$names";
}
```

Then request `GET /foo?names=bob,marry,nomas` will yield the response body

```
[bob]+[marry]+[nomas]
```


## array_map_op
**syntax:** *array_map_op &lt;directive&gt; $array_var*

**syntax:** *array_map_op &lt;directive&gt; $array_var to=$new_array_var*

**default:** *no*

**context:** *http, server, server if, location, location if*

Similar to the [array_map](#array_map) directive but maps the specified nginx configuration directive instead of
a string template to each element in the array-typed nginx variable specified. The result
of applying the specified configuration directive becomes the result of the mapping.

The nginx configuration directive being used as the iterator must be implemented by [Nginx Devel Kit](https://github.com/simpl/ngx_devel_kit) (NDK)'s set_var submodule's `ndk_set_var_value`.
For example, the following [set-misc-nginx-module](http://github.com/openresty/set-misc-nginx-module) directives can be invoked this way:

* [set_quote_sql_str](http://github.com/openresty/set-misc-nginx-module#set_quote_sql_str)
* [set_quote_pgsql_str](http://github.com/openresty/set-misc-nginx-module#set_quote_pgsql_str)
* [set_quote_json_str](http://github.com/openresty/set-misc-nginx-module#set_quote_json_str)
* [set_unescape_uri](http://github.com/openresty/set-misc-nginx-module#set_unescape_uri)
* [set_escape_uri](http://github.com/openresty/set-misc-nginx-module#set_escape_uri)
* [set_encode_base32](http://github.com/openresty/set-misc-nginx-module#set_encode_base32)
* [set_decode_base32](http://github.com/openresty/set-misc-nginx-module#set_decode_base32)
* [set_encode_base64](http://github.com/openresty/set-misc-nginx-module#set_encode_base64)
* [set_decode_base64](http://github.com/openresty/set-misc-nginx-module#set_decode_base64)
* [set_encode_hex](http://github.com/openresty/set-misc-nginx-module#set_encode_base64)
* [set_decode_hex](http://github.com/openresty/set-misc-nginx-module#set_decode_base64)
* [set_sha1](http://github.com/openresty/set-misc-nginx-module#set_encode_base64)
* [set_md5](http://github.com/openresty/set-misc-nginx-module#set_decode_base64)

This is a higher-order operation where other nginx configuration directives can be used
as arguments for this `map_array_op` directive.

Consider the following example,

```nginx
array_map_op set_quote_sql_str $names;
```

This line changes each element in the array-typed nginx variable `$names` by applying the
[set_quote_sql_str](https://github.com/openresty/set-misc-nginx-module#set_quote_sql_str)
directive provided by the [ngx_set_misc](https://github.com/openresty/set-misc-nginx-module)
module one by one. The result is that each element in the array `$names` has been escaped as SQL string literal values.

You can also specify the `to=$var` option if you do not want in-place modifications of the input arrays. For instance,

```nginx
array_map_op set_quote_sql_str $names to=$quoted_names;
```

will save the escaped elements into a new (array-typed) nginx variable named `$quoted_names` with `$names` intact.

The following is a relatively complete example:

```nginx
location /foo {
    array_split ',' $arg_names to=$names;
    array_map_op set_quote_sql_str $names;
    array_join '+' $names to=$res;
    echo $res;
}
```

Then request `GET /foo?names=bob,marry,nomas` will yield the response body

```
'bob'+'marry'+'nomas'
```

Pretty cool, huh?


## Here we assume you would install you nginx under /opt/nginx/.
./configure --prefix=/opt/nginx \
  --add-module=/path/to/ngx_devel_kit \
  --add-module=/path/to/array-var-nginx-module

make -j2
make install
```

Download the latest version of the release tarball of this module from [array-var-nginx-module file list](https://github.com/openresty/array-var-nginx-module/tags), and the latest tarball for [ngx_devel_kit](https://github.com/simplresty/ngx_devel_kit) from its [file list](https://github.com/simplresty/ngx_devel_kit/tags).

Also, this module is included and enabled by default in the [OpenResty bundle](http://openresty.org).


## See Also

* [NDK](https://github.com/simpl/ngx_devel_kit)
* [ngx_lua](https://github.com/openresty/lua-nginx-module)
* [ngx_set_misc](https://github.com/openresty/set-misc-nginx-module)


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-array-var](https://github.com/openresty/array-var-nginx-module){target=_blank}.

# *auth-digest*: Digest Authentication for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-auth-digest
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_auth_digest_module.so;
```


This document describes nginx-module-auth-digest [v1.0.0](https://github.com/atomx/nginx-http-auth-digest/releases/tag/v1.0.0){target=_blank} 
released on Mar 23 2018.

<hr />

## Changes from other forks

Bug fixes
[1](https://github.com/samizdatco/nginx-http-auth-digest/commit/9d77dcc58420d5afb8aa5a8138b1bf22a1933dd6),
[2](https://github.com/samizdatco/nginx-http-auth-digest/commit/b98725d3d0506c895f6a9f9d38f9168d499275fc),
[3](https://github.com/samizdatco/nginx-http-auth-digest/commit/47d5bac13cf071b4dbe81048b0f12a742ba512ae)

[Added log message for invalid login
attempts](https://github.com/samizdatco/nginx-http-auth-digest/commit/9a402045082291c1f2f0a432ac24475277e2d176)

## Description

The `ngx_http_auth_digest` module supplements
[Nginx](http://nginx.net)'s built-in Basic Authentication
[module](http://wiki.nginx.org/HttpAuthBasicModule) by providing support
for [RFC](http://www.ietf.org/rfc/rfc2617.txt) 2617 [Digest
Authentication](http://en.wikipedia.org/wiki/Digest_access_authentication).
The module is currently functional but has only been tested and reviewed
by its author. And given that this is security code, one set of eyes is
almost certainly insufficient to guarantee that it's 100% correct. Until
a few bug reports come in and some of the ‘unknown unknowns’ in the code
are flushed out, consider this module an ‘alpha’ and treat it with the
appropriate amount of skepticism.

A listing of known issues with the module can be found in the `bugs.txt`
file as well as in the [Issue
Tracker](https://github.com/samizdatco/nginx-http-auth-digest/issues).
Please do consider contributing a patch if you have the time and
inclination. Any help fixing the bugs or changing the implementation to
a more idiomatically nginx-y one would be greatly appreciated.

## Example

You can password-protect a directory tree by adding the following lines
into a `server` section in your [Nginx](http://nginx.net) configuration
file:

    auth_digest_user_file /opt/httpd/conf/passwd.digest; # a file created with htdigest
    location /private{
      auth_digest 'this is not for you'; # set the realm for this location block
    }

The other directives control the lifespan defaults for the
authentication session. The following is equivalent to the previous
example but demonstrates all the directives:

    auth_digest_user_file /opt/httpd/conf/passwd.digest;
    auth_digest_shm_size 4m;   # the storage space allocated for tracking active sessions

    location /private {
      auth_digest 'this is not for you';
      auth_digest_timeout 60s; # allow users to wait 1 minute between receiving the
                               # challenge and hitting send in the browser dialog box
      auth_digest_expires 10s; # after a successful challenge/response, let the client
                               # continue to use the same nonce for additional requests
                               # for 10 seconds before generating a new challenge
      auth_digest_replays 20;  # also generate a new challenge if the client uses the
                               # same nonce more than 20 times before the expire time limit
    }

Adding digest authentication to a location will affect any uris that
match that block. To disable authentication for specific sub-branches
off a uri, set `auth_digest` to `off`:

    location / {
      auth_digest 'this is not for you';
      location /pub {
        auth_digest off; # this sub-tree will be accessible without authentication
      }
    }

## Directives

### auth_digest

Syntax  
`auth_digest` \[*realm-name* \| `off`\]

Default  
`off`

Context  
server, location

Description  
Enable or disable digest authentication for a server or location block.
The realm name should correspond to a realm used in the user file. Any
user within that realm will be able to access files after
authenticating.

To selectively disable authentication within a protected uri hierarchy,
set `auth_digest` to “`off`” within a more-specific location block (see
example).

### auth_digest_user_file

Syntax  
`auth_digest_user_file` */path/to/passwd/file*

Default  
*unset*

Context  
server, location

Description  
The password file should be of the form created by the apache `htdigest`
command (or the included
[htdigest.py](https://github.com/samizdatco/nginx-http-auth-digest/blob/master/htdigest.py)
script). Each line of the file is a colon-separated list composed of a
username, realm, and md5 hash combining name, realm, and password. For
example: `joi:enfield:ef25e85b34208c246cfd09ab76b01db7`

### auth_digest_timeout

Syntax  
`auth_digest_timeout` *delay-time*

Default  
`60s`

Context  
server, location

Description  
When a client first requests a protected page, the server returns a 401
status code along with a challenge in the `www-authenticate` header.

At this point most browsers will present a dialog box to the user
prompting them to log in. This directive defines how long challenges
will remain valid. If the user waits longer than this time before
submitting their name and password, the challenge will be considered
‘stale’ and they will be prompted to log in again.

### auth_digest_expires

Syntax  
`auth_digest_expires` *lifetime-in-seconds*

Default  
`10s`

Context  
server, location

Description  
Once a digest challenge has been successfully answered by the client,
subsequent requests will attempt to re-use the ‘nonce’ value from the
original challenge. To complicate
[MitM](http://en.wikipedia.org/wiki/Man-in-the-middle_attack) attacks,
it's best to limit the number of times a cached nonce will be accepted.
This directive sets the duration for this re-use period after the first
successful authentication.

### auth_digest_replays

Syntax  
`auth_digest_replays` *number-of-uses*

Default  
`20`

Context  
server, location

Description  
Nonce re-use should also be limited to a fixed number of requests. Note
that increasing this value will cause a proportional increase in memory
usage and the shm_size may have to be adjusted to keep up with heavy
traffic within the digest-protected location blocks.

### auth_digest_evasion_time

Syntax  
`auth_digest_evasion_time` *time-in-seconds*

Default  
`300s`

Context  
server, location

Description  
The amount of time for which the server will ignore authentication
requests from a client address once the number of failed authentications
from that client reaches `auth_digest_maxtries`.

### auth_digest_maxtries

Syntax  
`auth_digest_maxtries` *number-of-attempts*

Default  
`5`

Context  
server, location

Description  
The number of failed authentication attempts from a client address
before the module enters evasive tactics. For evasion purposes, only
network clients are tracked, and only by address (not including port
number). A successful authentication clears the counters.

### auth_digest_shm_size

Syntax  
`auth_digest_shm_size` *size-in-bytes*

Default  
`4096k`

Context  
server

Description  
The module maintains a fixed-size cache of active digest sessions to
save state between authenticated requests. Once this cache is full, no
further authentication will be possible until active sessions expire.

As a result, choosing the proper size is a little tricky since it
depends upon the values set in the expiration-related directives. Each
stored challenge takes up `48 + ceil(replays/8)` bytes and will live for
up to `auth_digest_timeout + auth_digest_expires` seconds. When using
the default module settings this translates into allowing around 82k
non-replay requests every 70 seconds.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-auth-digest](https://github.com/atomx/nginx-http-auth-digest){target=_blank}.

# *auth-ldap*: LDAP Authentication module for nginx


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-auth-ldap
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_auth_ldap_module.so;
```


This document describes nginx-module-auth-ldap [v0.3](https://github.com/dvershinin/nginx-auth-ldap/releases/tag/v0.3){target=_blank} 
released on May 28 2020.

<hr />
LDAP module for nginx which supports authentication against multiple LDAP servers.

## Example configuration
Define list of your LDAP servers with required user/group requirements:

```bash
    http {
      ldap_server test1 {
        url ldap://192.168.0.1:3268/DC=test,DC=local?sAMAccountName?sub?(objectClass=person);
        binddn "TEST\\LDAPUSER";
        binddn_passwd LDAPPASSWORD;
        group_attribute uniquemember;
        group_attribute_is_dn on;
        require valid_user;
      }

      ldap_server test2 {
        url ldap://192.168.0.2:3268/DC=test,DC=local?sAMAccountName?sub?(objectClass=person);
        binddn "TEST\\LDAPUSER";
        binddn_passwd LDAPPASSWORD;
        group_attribute uniquemember;
        group_attribute_is_dn on;
        require valid_user;
      }
    }
```

And add required servers in correct order into your location/server directive:
```bash
    server {
        listen       8000;
        server_name  localhost;

        auth_ldap "Forbidden";
        auth_ldap_servers test1;
		auth_ldap_servers test2;

        location / {
            root   html;
            index  index.html index.htm;
        }

    }
```

## Available config parameters

## url
expected value: string

Available URL schemes: ldap://, ldaps://

## binddn
expected value: string

## binddn_passwd
expected value: string

## group_attribute
expected value: string

## group_attribute_is_dn
expected value: on or off, default off

## require
expected value: valid_user, user, group

## satisfy
expected value: all, any

## max_down_retries
expected value: a number, default 0

Retry count for attempting to reconnect to an LDAP server if it is considered
"DOWN".  This may happen if a KEEP-ALIVE connection to an LDAP server times 
out or is terminated by the server end after some amount of time.  

This can usually help with the following error:

```
http_auth_ldap: ldap_result() failed (-1: Can't contact LDAP server)
```

## connections
expected value: a number greater than 0

## ssl_check_cert
expected value: on or off, default off

Verify the remote certificate for LDAPs connections. If disabled, any remote certificate will be
accepted which exposes you to possible man-in-the-middle attacks. Note that the server's
certificate will need to be signed by a proper CA trusted by your system if this is enabled.
See below how to trust CAs without installing them system-wide.

This options needs OpenSSL >= 1.0.2; it is unavailable if compiled with older versions.

## ssl_ca_file
expected value: file path

Trust the CA certificate in this file (see ssl_check_cert above).

## ssl_ca_dir
expected value: directory path

Trust all CA certificates in this directory (see ssl_check_cert above).

Note that you need to provide hash-based symlinks in the directory for this to work;
you'll basically need to run OpenSSL's c_rehash command in this directory.

## referral
expected value: on, off

LDAP library default is on. This option disables usage of referral messages from
LDAP server. Usefull for authenticating against read only AD server without access
to read write.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-auth-ldap](https://github.com/dvershinin/nginx-auth-ldap){target=_blank}.

# *auth-pam*: PAM authentication dynamic module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-auth-pam
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_auth_pam_module.so;
```


This document describes nginx-module-auth-pam [v1.5.5](https://github.com/sto/ngx_http_auth_pam_module/releases/tag/v1.5.5){target=_blank} 
released on Jun 20 2023.

<hr />

## Nginx module to use PAM for simple http authentication

### Configuration

The module only has two directives:

- ``auth_pam``: This is the http authentication realm. If given the value
  ``off`` the module is disabled (needed when we want to override the value
  set on a lower-level directive).

- ``auth_pam_service_name``: this is the PAM service name and by default it is
  set to ``nginx``.

### Examples

To protect everything under ``/secure`` you will add the following to the
``nginx.conf`` file:

	location /secure {
	    auth_pam              "Secure Zone";
	    auth_pam_service_name "nginx";
	}

Note that the module runs as the web server user, so the PAM modules used must
be able to authenticate the users without being root; that means that if you
want to use the ``pam_unix.so`` module to autenticate users you need to let the
web server user to read the ``/etc/shadow`` file if that does not scare you (on
Debian like systems you can add the ``www-data`` user to the ``shadow`` group).

As an example, to authenticate users against an LDAP server (using the
``pam_ldap.so`` module) you will use an ``/etc/pam.d/nginx`` like the
following:

	auth    required     /lib/security/pam_ldap.so
	account required     /lib/security/pam_ldap.so

If you also want to limit the users from LDAP that can authenticate you can
use the ``pam_listfile.so`` module; to limit who can access resources under
``/restricted`` add the following to the ``nginx.conf`` file:

	location /restricted {
	    auth_pam              "Restricted Zone";
	    auth_pam_service_name "nginx_restricted";
	}

Use the following ``/etc/pam.d/nginx_restricted`` file:

	auth    required     /lib/security/pam_listfile.so onerr=fail item=user \
	                     sense=allow file=/etc/nginx/restricted_users
	auth    required     /lib/security/pam_ldap.so
	account required     /lib/security/pam_ldap.so

And add the users allowed to authenticate to the ``/etc/nginx/restricted_users``
(remember that the web server user has to be able to read this file).

### PAM Environment

If you want use the ``pam_exec.so`` plugin for request based authentication the
module can add to the PAM environment the ``HOST`` and ``REQUEST`` variables if
you set the ``auth_pam_set_pam_env`` flag::

	location /pam_exec_protected {
	  auth_pam              "Exec Zone";
	  auth_pam_service_name "nginx_exec";
	  auth_pam_set_pam_env  on;
	}

With this configuration if you access an URL like:

	http://localhost:8000/pam_exec_protected/page?foo=yes&bar=too

the PAM environment will include the following variables:

	HOST=localhost:8000
	REQUEST=GET /pam_exec_protected/page?foo=yes&bar=too HTTP/1.1

You may use this information for request based authentication.
You need a recent pam release (>= version 1.0.90) to expose environment
variables to pam_exec.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-auth-pam](https://github.com/sto/ngx_http_auth_pam_module){target=_blank}.

# *aws-auth*: NGINX module to proxy to authenticated AWS services


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-aws-auth
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_aws_auth_module.so;
```


This document describes nginx-module-aws-auth [v2.1.1](https://github.com/anomalizer/ngx_aws_auth/releases/tag/2.1.1){target=_blank} 
released on Mar 06 2017.

<hr />

 [![Gitter chat](https://badges.gitter.im/anomalizer/ngx_aws_auth.png)](https://gitter.im/ngx_aws_auth/Lobby?utm_source=share-link&utm_medium=link&utm_campaign=share-link)

This nginx module can proxy requests to authenticated S3 backends using Amazon's
V4 authentication API. The first version of this module was written for the V2
authentication protocol and can be found in the *AuthV2* branch.

## Usage example

Implements proxying of authenticated requests to S3.

```nginx
  server {
    listen     8000;

    aws_access_key your_aws_access_key; # Example AKIDEXAMPLE
    aws_key_scope scope_of_generated_signing_key; #Example 20150830/us-east-1/service/aws4_request
    aws_signing_key signing_key_generated_using_script; #Example L4vRLWAO92X5L3Sqk5QydUSdB0nC9+1wfqLMOKLbRp4=
	aws_s3_bucket your_s3_bucket;

    location / {
	  aws_sign;
      proxy_pass http://your_s3_bucket.s3.amazonaws.com;
    }

    # This is an example that does not use the server root for the proxy root
	location /myfiles {
	
      rewrite /myfiles/(.*) /$1 break;
      proxy_pass http://your_s3_bucket.s3.amazonaws.com/$1;


      aws_access_key your_aws_access_key;
      aws_key_scope scope_of_generated_signing_key;
      aws_signing_key signing_key_generated_using_script;
    }

    # This is an example that use specific s3 endpoint, default endpoint is s3.amazonaws.com
	location /s3_beijing {
	
      rewrite /s3_beijing/(.*) /$1 break;
      proxy_pass http://your_s3_bucket.s3.cn-north-1.amazonaws.com.cn/$1;

      aws_sign;
      aws_endpoint "s3.cn-north-1.amazonaws.com.cn";
      aws_access_key your_aws_access_key;
      aws_key_scope scope_of_generated_signing_key;
      aws_signing_key signing_key_generated_using_script;
    }
  }
```

## Security considerations
The V4 protocol does not need access to the actual secret keys that one obtains 
from the IAM service. The correct way to use the IAM key is to actually generate
a scoped signing key and use this signing key to access S3. This nginx module
requires the signing key and not the actual secret key. It is an insecure practise
to let the secret key reside on your nginx server.

Note that signing keys have a validity of just one week. Hence, they need to
be refreshed constantly. Please useyour favourite configuration management
system such as saltstack, puppet, chef, etc. etc. to distribute the signing
keys to your nginx clusters. Do not forget to HUP the server after placing the new
signing key as nginx reads the configuration only at startup time.

A standalone python script has been provided to generate the signing key
```
./generate_signing_key -h
usage: generate_signing_key [-h] -k ACCESS_KEY -r REGION [-s SERVICE]
                            [-d DATE] [--no-base64] [-v]

Generate AWS S3 signing key in it's base64 encoded form

optional arguments:
  -h, --help            show this help message and exit
  -k SECRET_KEY, --secret-key SECRET_KEY
                        The secret key generated using AWS IAM. Do not confuse
                        this with the access key id
  -r REGION, --region REGION
                        The AWS region where this key would be used. Example:
                        us-east-1
  -s SERVICE, --service SERVICE
                        The AWS service for which this key would be used.
                        Example: s3
  -d DATE, --date DATE  The date on which this key is generated in yyyymmdd
                        format
  --no-base64           Disable output as a base64 encoded string. This NOT
                        recommended
  -v, --verbose         Produce verbose output on stderr


./generate_signing_key -k wJalrXUtnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY -r us-east-1
L4vRLWAO92X5L3Sqk5QydUSdB0nC9+1wfqLMOKLbRp4=
20160902/us-east-1/s3/aws4_request

```

## Known limitations
The 2.x version of the module currently only has support for GET and HEAD calls. This is because
signing request body is complex and has not yet been implemented.



## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-aws-auth](https://github.com/anomalizer/ngx_aws_auth){target=_blank}.

# *bot-verifier*: A search index bot verification module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-bot-verifier
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_bot_verifier_module.so;
```


This document describes nginx-module-bot-verifier [v0.0.11](https://github.com/dvershinin/ngx_bot_verifier/releases/tag/v0.0.11){target=_blank} 
released on Jun 26 2022.

<hr />

## Status

[BETA] This module has been tested on a handful of production websites. It has not yet been evaluated at scale. If you would like to consider testing this at scale I would be happy to assist and allocate time to correct any issues.


## Synopsis

```nginx
location / {
    bot_verifier on;
    bot_verifier_redis_host localhost;
    bot_verifier_redis_port 6379;
    bot_verifier_redis_connection_timeout 10;
    bot_verifier_redis_read_timeout 10;
    bot_verifier_redis_expiry 3600;
	bot_verifier_repsheet_enabled on;
}
```


## Description

This is an NGINX module designed to validate actors claiming to be search engine indexers. It is right to disable security mechanisms for valid search engine bots to ensure your controls do not interfere with page rankings on any of the search providers. The issue is that the [User Agent](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent) header cannot be trusted. In order to ensure you are allowing only valid search engine indexers, you must validate according to their published standards. This module performs that validation and caches the results to ensure you do not pay validation penalties on every request.


## Directives

The following directives are used only for module configuration.


## bot_verifier

**syntax:** *bot_verifier* \[on|off\]

**default:** *off*

**context:** *location*

**phase:** *access*

Enables or disables the module. The module will not act unless it is set to *on*.


## bot_verifier_redis_host

**syntax:** *bot_verifier_redis_host* &lt;string&gt;

**default:** *localhost*

**context:** *location*

**phase:** *access*

Sets the Redis host. This setting is used to connect to the Redis database used for caching lookup results.


## bot_verifier_redis_port

**syntax:** *bot_verifier_redis_port* &lt;int&gt;

**default:** *6379*

**context:** *location*

**phase:** *access*

Sets the Redis port. This setting is used to connect to the Redis database used for caching lookup results.


## bot_verifier_redis_connection_timeout

**syntax:** *bot_verifier_redis_connection_timeout* &lt;int&gt;

**default:** *10*

**context:** *location*

**phase:** *access*

Sets the timeout when connecting to Redis. This setting is used to connect to the Redis database used for caching lookup results.


## bot_verifier_redis_read_timeout

**syntax:** *bot_verifier_redis_read_timeout* &lt;int&gt;

**default:** *10*

**context:** *location*

**phase:** *access*

Sets the timeout when querying Redis. This setting is used to connect to the Redis database used for caching lookup results.


## bot_verifier_redis_expiry

**syntax:** *bot_verifier_redis_expiry* &lt;seconds&gt;

**default:** *3600*

**context:** *location*

**phase:** *access*

Sets the timeout when querying Redis. This setting is used to connect to the Redis database used for caching lookup results.


## bot_verifier_repsheet_enabled

**syntax:** *bot_verifier_repsheet_enabled* \[on|off\]

**default:** *off*

**context:** *location*

**phase:** *access*

Enables blacklisting of failed actors in Repsheet. Assumes Repsheet cache lives on already configured redis server.


## Verifying Functionality

In order to ensure the module is working properly you will need to issue a query that will trigger failure and success cases. To trigger a failure case issue the following request:

```
curl -A "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html" localhost:8888
```

This will issue a query that identifies itself as a Google bot. The reverse and forward lookup routine will fail and you will get a `403` response. To ensure the verification works when a bot is identified issue the following request:

```
curl -H "X-Forwarded-For: 66.249.66.1" -A "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html" localhost:8888
```

This will spoof the `X-Forwarded-For` header and pretend to be from a valid google address. The request should succeed and return a normal response.


## Developer Setup

This module contains a full self-contained development environment. This is done to ensure work on the module does not interfere with any other NGINX installations. To setup the environment run the `script/bootstrap` command. This will create the following directories:

```
vendor - The NGINX installation will live here  
build - The NGINX install will live here  
```

The `nginx.conf` file in the root of this repository will be symlinked to `build/nginx/conf/nginx.conf` to make configuration changes easier. You can start NGINX using the following command:

```
build/nginx/sbin/nginx
```

Log files are available at `build/nginx/logs`. You can stop the server by running

```
build/nginx/sbin/nginx -s stop
```

If you are making changes to the module, you can recompile them by running `make compile`. Remember to restart the NGINX after this completes successfully.


## Running the Test Suite

This repository comes with a test suite that uses the `Test::Nginx` library. To run the test you will need to install the following libraries:

```
cpanm -S install Test::Nginx Test::Nginx::Socket
```

Once the libraries are installed just run `make` and the suite will run. If you are submitting a change to this module please make sure to run the test suite before you do. Any changes that break the test suite will not be accepted.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-bot-verifier](https://github.com/dvershinin/ngx_bot_verifier){target=_blank}.

# *brotli*: NGINX Brotli dynamic modules


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-brotli
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_brotli_filter_module.so;
```
```nginx
load_module modules/ngx_http_brotli_static_module.so;
```


This document describes nginx-module-brotli [v0.1.4](https://github.com/GetPageSpeed/ngx_brotli/releases/tag/v0.1.4){target=_blank} 
released on Jul 01 2021.

<hr />

Brotli is a generic-purpose lossless compression algorithm that compresses data
using a combination of a modern variant of the LZ77 algorithm, Huffman coding
and 2nd order context modeling, with a compression ratio comparable to the best
currently available general-purpose compression methods. It is similar in speed
with deflate but offers more dense compression.

ngx_brotli is a set of two nginx modules:

- ngx_brotli filter module - used to compress responses on-the-fly,
- ngx_brotli static module - used to serve pre-compressed files.


## Status

Both Brotli library and nginx module are under active development.

## Configuration directives

### `brotli_static`

- **syntax**: `brotli_static on|off|always`
- **default**: `off`
- **context**: `http`, `server`, `location`

Enables or disables checking of the existence of pre-compressed files with`.br`
extension. With the `always` value, pre-compressed file is used in all cases,
without checking if the client supports it.

### `brotli`

- **syntax**: `brotli on|off`
- **default**: `off`
- **context**: `http`, `server`, `location`, `if`

Enables or disables on-the-fly compression of responses.

### `brotli_types`

- **syntax**: `brotli_types <mime_type> [..]`
- **default**: `text/html`
- **context**: `http`, `server`, `location`

Enables on-the-fly compression of responses for the specified MIME types
in addition to `text/html`. The special value `*` matches any MIME type.
Responses with the `text/html` MIME type are always compressed.

### `brotli_buffers`

- **syntax**: `brotli_buffers <number> <size>`
- **default**: `32 4k|16 8k`
- **context**: `http`, `server`, `location`

**Deprecated**, ignored.

### `brotli_comp_level`

- **syntax**: `brotli_comp_level <level>`
- **default**: `6`
- **context**: `http`, `server`, `location`

Sets on-the-fly compression Brotli quality (compression) `level`.
Acceptable values are in the range from `0` to `11`.

### `brotli_window`

- **syntax**: `brotli_window <size>`
- **default**: `512k`
- **context**: `http`, `server`, `location`

Sets Brotli window `size`. Acceptable values are `1k`, `2k`, `4k`, `8k`, `16k`,
`32k`, `64k`, `128k`, `256k`, `512k`, `1m`, `2m`, `4m`, `8m` and `16m`.

### `brotli_min_length`

- **syntax**: `brotli_min_length <length>`
- **default**: `20`
- **context**: `http`, `server`, `location`

Sets the minimum `length` of a response that will be compressed.
The length is determined only from the `Content-Length` response header field.

## Variables

### `$brotli_ratio`

Achieved compression ratio, computed as the ratio between the original
and compressed response sizes.

## Sample configuration

```
brotli on;
brotli_comp_level 6;
brotli_static on;
brotli_types application/atom+xml application/javascript application/json application/rss+xml
             application/vnd.ms-fontobject application/x-font-opentype application/x-font-truetype
             application/x-font-ttf application/x-javascript application/xhtml+xml application/xml
             font/eot font/opentype font/otf font/truetype image/svg+xml image/vnd.microsoft.icon
             image/x-icon image/x-win-bitmap text/css text/javascript text/plain text/xml;
```

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-brotli](https://github.com/GetPageSpeed/ngx_brotli){target=_blank}.

# *cache-purge*: NGINX Cache Purge module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-cache-purge
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_cache_purge_module.so;
```


This document describes nginx-module-cache-purge [v2.5.3](https://github.com/nginx-modules/ngx_cache_purge/releases/tag/2.5.3){target=_blank} 
released on Feb 22 2023.

<hr />
`ngx_cache_purge` is `nginx` module which adds ability to purge content from
`FastCGI`, `proxy`, `SCGI` and `uWSGI` caches. A purge operation removes the 
content with the same cache key as the purge request has.


## Sponsors
Work on the original patch was fully funded by [yo.se](http://yo.se).


## Status
This module is production-ready.


## Configuration directives (same location syntax)
## fastcgi_cache_purge
* **syntax**: `fastcgi_cache_purge on|off|<method> [purge_all] [from all|<ip> [.. <ip>]]`
* **default**: `none`
* **context**: `http`, `server`, `location`

Allow purging of selected pages from `FastCGI`'s cache.


## proxy_cache_purge
* **syntax**: `proxy_cache_purge on|off|<method> [purge_all] [from all|<ip> [.. <ip>]]`
* **default**: `none`
* **context**: `http`, `server`, `location`

Allow purging of selected pages from `proxy`'s cache.


## scgi_cache_purge
* **syntax**: `scgi_cache_purge on|off|<method> [purge_all] [from all|<ip> [.. <ip>]]`
* **default**: `none`
* **context**: `http`, `server`, `location`

Allow purging of selected pages from `SCGI`'s cache.


## uwsgi_cache_purge
* **syntax**: `uwsgi_cache_purge on|off|<method> [purge_all] [from all|<ip> [.. <ip>]]`
* **default**: `none`
* **context**: `http`, `server`, `location`

Allow purging of selected pages from `uWSGI`'s cache.


## Configuration directives (separate location syntax)
## fastcgi_cache_purge
* **syntax**: `fastcgi_cache_purge zone_name key`
* **default**: `none`
* **context**: `location`

Sets area and key used for purging selected pages from `FastCGI`'s cache.


## proxy_cache_purge
* **syntax**: `proxy_cache_purge zone_name key`
* **default**: `none`
* **context**: `location`

Sets area and key used for purging selected pages from `proxy`'s cache.


## scgi_cache_purge
* **syntax**: `scgi_cache_purge zone_name key`
* **default**: `none`
* **context**: `location`

Sets area and key used for purging selected pages from `SCGI`'s cache.


## uwsgi_cache_purge
* **syntax**: `uwsgi_cache_purge zone_name key`
* **default**: `none`
* **context**: `location`

Sets area and key used for purging selected pages from `uWSGI`'s cache.

## Configuration directives (Optional)

## cache_purge_response_type
* **syntax**: `cache_purge_response_type html|json|xml|text`
* **default**: `html`
* **context**: `http`, `server`, `location`

Sets a response type of purging result.



## Partial Keys
Sometimes it's not possible to pass the exact key cache to purge a page. For example; when the content of a cookie or the params are part of the key.
You can specify a partial key adding an asterisk at the end of the URL.

    curl -X PURGE /page*

The asterisk must be the last character of the key, so you **must** put the $uri variable at the end.



## Sample configuration (same location syntax)
    http {
        proxy_cache_path  /tmp/cache  keys_zone=tmpcache:10m;

        server {
            location / {
                proxy_pass         http://127.0.0.1:8000;
                proxy_cache        tmpcache;
                proxy_cache_key    "$uri$is_args$args";
                proxy_cache_purge  PURGE from 127.0.0.1;
            }
        }
    }


## Sample configuration (same location syntax - purge all cached files)
    http {
        proxy_cache_path  /tmp/cache  keys_zone=tmpcache:10m;

        server {
            location / {
                proxy_pass         http://127.0.0.1:8000;
                proxy_cache        tmpcache;
                proxy_cache_key    "$uri$is_args$args";
                proxy_cache_purge  PURGE purge_all from 127.0.0.1 192.168.0.0/8;
            }
        }
    }


## Sample configuration (separate location syntax)
    http {
        proxy_cache_path  /tmp/cache  keys_zone=tmpcache:10m;

        server {
            location / {
                proxy_pass         http://127.0.0.1:8000;
                proxy_cache        tmpcache;
                proxy_cache_key    "$uri$is_args$args";
            }

            location ~ /purge(/.*) {
                allow              127.0.0.1;
                deny               all;
                proxy_cache        tmpcache;
                proxy_cache_key    "$1$is_args$args";
            }
        }
    }

## Sample configuration (Optional)
    http {
        proxy_cache_path  /tmp/cache  keys_zone=tmpcache:10m;

        cache_purge_response_type text;

        server {

            cache_purge_response_type json;

            location / { #json
                proxy_pass         http://127.0.0.1:8000;
                proxy_cache        tmpcache;
                proxy_cache_key    "$uri$is_args$args";
            }

            location ~ /purge(/.*) { #xml
                allow              127.0.0.1;
                deny               all;
                proxy_cache        tmpcache;
                proxy_cache_key    "$1$is_args$args";
                cache_purge_response_type xml;
            }

            location ~ /purge2(/.*) { # json
                allow              127.0.0.1;
                deny               all;
                proxy_cache        tmpcache;
                proxy_cache_key    "$1$is_args$args";
            }
        }

        server {

            location / { #text
                proxy_pass         http://127.0.0.1:8000;
                proxy_cache        tmpcache;
                proxy_cache_key    "$uri$is_args$args";
            }

            location ~ /purge(/.*) { #text
                allow              127.0.0.1;
                deny               all;
                proxy_cache        tmpcache;
                proxy_cache_key    "$1$is_args$args";
            }

            location ~ /purge2(/.*) { #html
                allow              127.0.0.1;
                deny               all;
                proxy_cache        tmpcache;
                proxy_cache_key    "$1$is_args$args";
                cache_purge_response_type html;
            }
        }
    }



## Solve problems
* Enabling [`gzip_vary`](https://nginx.org/r/gzip_vary) can lead to different results when clearing, when enabling it, you may have problems clearing the cache. For reliable operation, you can disable [`gzip_vary`](https://nginx.org/r/gzip_vary) inside the location [#20](https://github.com/nginx-modules/ngx_cache_purge/issues/20).


## Testing
`ngx_cache_purge` comes with complete test suite based on [Test::Nginx](http://github.com/agentzh/test-nginx).

You can test it by running:

`$ prove`


## See also
- [ngx_slowfs_cache](http://github.com/FRiCKLE/ngx_slowfs_cache).
- http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#purger
- http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_cache_purge
- https://github.com/wandenberg/nginx-selective-cache-purge-module
- https://github.com/wandenberg/nginx-sorted-querystring-module
- https://github.com/ledgetech/ledge
- [Faking Surrogate Cache-Keys for Nginx Plus](https://www.innoq.com/en/blog/faking-surrogate-cache-keys-for-nginx-plus/) ([gist](https://gist.github.com/titpetric/2f142e89eaa0f36ba4e4383b16d61474))
- [Delete NGINX cached md5 items with a PURGE with wildcard support](https://gist.github.com/nosun/0cfb58d3164f829e2f027fd37b338ede)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-cache-purge](https://github.com/nginx-modules/ngx_cache_purge){target=_blank}.

# *captcha*: NGINX Captcha Module

## Installation

CentOS/RHEL/RockyLinux/etc. and Amazon Linux are supported and require a [subscription](https://www.getpagespeed.com/repo-subscribe).

Fedora Linux is supported free of charge and doesn't require a subscription.

### OS-specific complete installation and configuration guides available:

*   [CentOS/RHEL 7](https://bit.ly/nginx-captcha-el)
*   [CentOS/RHEL 8](https://bit.ly/nginx-captcha-el)
*   [Amazon Linux 2](https://bit.ly/nginx-captcha-el)

### Other supported operating systems
        
```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-captcha
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_captcha_module.so;
```


This document describes nginx-module-captcha [v0.0.1](https://github.com/RekGRpth/ngx_http_captcha_module/releases/tag/0.0.1){target=_blank} 
released on Apr 14 2023.

<hr />

### Example Configuration:
```nginx
location =/captcha {
    captcha;
}
location =/login {
    set_form_input $csrf_form csrf;
    set_unescape_uri $csrf_unescape $csrf_form;
    set_form_input $captcha_form captcha;
    set_unescape_uri $captcha_unescape $captcha_form;
    set_md5 $captcha_md5 "secret${captcha_unescape}${csrf_unescape}";
    if ($captcha_md5 != $cookie_captcha) {
        # captcha invalid code
    }
}
```
### Directives:

    Syntax:	 captcha;
    Default: ——
    Context: location

Enables generation of captcha image.<hr>

    Syntax:	 captcha_case on | off;
    Default: off
    Context: http, server, location

Enables/disables ignoring captcha case.<hr>

    Syntax:	 captcha_expire seconds;
    Default: 3600
    Context: http, server, location

Sets seconds before expiring captcha.<hr>

    Syntax:	 captcha_height pixels;
    Default: 30
    Context: http, server, location

Sets height of captcha image.<hr>

    Syntax:	 captcha_length characters;
    Default: 4
    Context: http, server, location

Sets length of captcha text.<hr>

    Syntax:	 captcha_size pixels;
    Default: 20
    Context: http, server, location

Sets size of captcha font.<hr>

    Syntax:	 captcha_width pixels;
    Default: 130
    Context: http, server, location

Sets width of captcha image.<hr>

    Syntax:	 captcha_charset string;
    Default: abcdefghkmnprstuvwxyzABCDEFGHKMNPRSTUVWXYZ23456789
    Context: http, server, location

Sets characters used in captcha text.<hr>

    Syntax:	 captcha_csrf string;
    Default: csrf
    Context: http, server, location

Sets name of csrf var of captcha.<hr>

    Syntax:	 captcha_font string;
    Default: /usr/share/fonts/ttf-liberation/LiberationSans-Regular.ttf
    Context: http, server, location

Sets font of captcha text.<hr>

    Syntax:	 captcha_name string;
    Default: Captcha
    Context: http, server, location

Sets name of captcha cookie.<hr>

    Syntax:	 captcha_secret string;
    Default: secret
    Context: http, server, location

Sets secret of captcha.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-captcha](https://github.com/RekGRpth/ngx_http_captcha_module){target=_blank}.

# *combined-upstreams*: NGINX Combined Upstreams module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-combined-upstreams
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_combined_upstreams_module.so;
```


This document describes nginx-module-combined-upstreams [v2.2](https://github.com/lyokha/nginx-combined-upstreams-module/releases/tag/2.2){target=_blank} 
released on Nov 07 2023.

<hr />

<!--[![Build Status](https://travis-ci.com/lyokha/nginx-combined-upstreams-module.svg?branch=master)](https://travis-ci.com/lyokha/nginx-combined-upstreams-module)-->

The module introduces three directives *add_upstream*,
*combine_server_singlets*, and *extend_single_peers* available inside upstream
configuration blocks, and a new configuration block *upstrand* for building
super-layers of upstreams. Additionally, directive *dynamic_upstrand* is
introduced for choosing upstrands in run-time.

## Directive add_upstream

Populates the host upstream with servers listed in an already defined upstream
specified by the mandatory 1st parameter of the directive. The server attributes
such as weights, max_fails and others are kept in the host upstream. Optional
parameters may include values *backup* to mark all servers of the sourced
upstream as backup servers and *weight=N* to calibrate weights of servers of the
sourced upstream by multiplying them by factor *N*.

### An example

```nginx
upstream  combined {
    add_upstream    upstream1;            # src upstream 1
    add_upstream    upstream2 weight=2;   # src upstream 2
    server          some_another_server;  # if needed
    add_upstream    upstream3 backup;     # src upstream 3
}
```

## Directive combine_server_singlets

Produces multiple *singlet upstreams* from servers so far defined in the host
upstream. A singlet upstream contains only one active server whereas other
servers are marked as backup or down. If no parameters were passed then the
singlet upstreams will have names of the host upstream appended by the ordering
number of the active server in the host upstream. Optional 2 parameters can be
used to adjust their names. The 1st parameter is a suffix added after the name
of the host upstream and before the ordering number. The 2nd parameter must be
an integer value which defines *zero-alignment* of the ordering number. For
example, if it has value 2 then the ordering numbers could be
``'01', '02', ..., '10', ... '100' ...``.

To mark secondary servers as down rather than backup, use another optional
parameter *nobackup*. This parameter must be put in the end, after all other
parameters.

### An example

```nginx
upstream  uhost {
    server                   s1;
    server                   s2;
    server                   s3 backup;
    server                   s4;
    # build singlet upstreams uhost_single_01,
    # uhost_single_02, uhost_single_03 and uhost_single_04
    combine_server_singlets  _single_ 2;
    server                   s5;
}
```

### Why numbers, not names?

In the example above, singlet upstreams will have names like *uhost_single_01*,
but names that contain server names like *uhost_single_s1* would look better and
more convenient. Why not use them instead ordering numbers? Unfortunately, Nginx
does not remember server names after a server has been added into an upstream,
therefore we cannot simply fetch them.

*Update.* There is a good news! Since version *1.7.2*, Nginx remembers server
names in upstream data and now we can use them when referring to a special
keyword *byname*. For example,

```nginx
    combine_server_singlets  byname;
    # or
    combine_server_singlets  _single_ byname;
```

All colons (*:*) in the server names get replaced with underscores (*_*).

### Where this can be useful

Hmm, I do not know. Anyway, a singlet upstream is a prominent category because
it declares a single server with fallback mode. We can use them to provide
robust HTTP session management when backend servers identify themselves using a
known mechanism like HTTP cookies.

```nginx
upstream  uhost {
    server  s1;
    server  s2;
    combine_server_singlets;
}

server {
    listen       8010;
    server_name  main;
    location / {
        proxy_pass http://uhost$cookie_rt;
    }
}
server {
    listen       8020;
    server_name  server1;
    location / {
        add_header Set-Cookie "rt=1";
        echo "Passed to $server_name";
    }
}
server {
    listen       8030;
    server_name  server2;
    location / {
        add_header Set-Cookie "rt=2";
        echo "Passed to $server_name";
    }
}
```

In this configuration, the first client request will choose backend server
randomly, the chosen server will set cookie *rt* to a predefined value (*1* or
*2*), and all further requests from this client will be proxied to the chosen
server automatically until it goes down. Say, it was *server1*, then when it
goes down, the cookie *rt* on the client side will still be *1*. Directive
*proxy_pass* will route the next client request to a singlet upstream *uhost1*
where *server1* is declared active and *server2* is backed up. As soon as
*server1* is not reachable any longer, Nginx will route the request to *server2*
which will rewrite the cookie *rt* and all further client requests will be
proxied to *server2* until it goes down.

## Directive extend_single_peers

Peers in upstreams fail according to the rules listed in directive
*proxy_next_upstream*. If an upstream has only one peer in its main or backup
part then this peer will never fail. This can be a serious problem when writing
a custom algorithm for active health checks of upstream peers. Directive
*extend_single_peers*, being declared in an upstream block, adds a fake peer
marked as *down* in the main or the backup part of the upstream if the part
originally contains only one peer. This makes Nginx mark the original single
peer as failed when it fails to pass the rules of *proxy_next_upstream* just
like in the general case of multiple peers.

### An example

```nginx
upstream  upstream1 {
    server  s1;
    extend_single_peers;
}

upstream  upstream2 {
    server  s1;
    server  s2;
    server  s3 backup;
    extend_single_peers;
}
```

Notice that if a part (the main or the backup) of an upstream contains more than
one peer (like the main part in *upstream2* from the example) then the directive
has no effect: particularly, in the *upstream2* it only affects the backup part
of the upstream.

## Block upstrand

Is aimed to configure a super-layer of upstreams that do not lose their
identities. Accepts a number of directives including *upstream*, *order*,
*next_upstream_statuses* and others. Upstreams with names starting with tilde
(*~*) match a regular expression. Only upstreams that already have been declared
before the upstrand block definition are regarded as candidates.

### An example

```nginx
upstrand us1 {
    upstream ~^u0 blacklist_interval=60s;
    upstream b01 backup;
    order start_random;
    next_upstream_statuses error timeout non_idempotent 204 5xx;
    next_upstream_timeout 60s;
    intercept_statuses 5xx /Internal/failover;
}
```

Upstrand *us1* will combine all upstreams whose names start with *u0* and
upstream *b01* as backup. Backup upstreams are checked if all normal upstreams
fail. The *failure* means that all upstreams in normal or backup cycles have
responded with statuses listed in directive *next_upstream_statuses* or been
*blacklisted*. Here, the *upstream's response* means the status returned by the
last server of the upstream, which is strongly affected by value of directive
*proxy_next_upstream*. An upstream is set as blacklisted when it has parameter
*blacklist_interval* and responds with a status listed in the
*next_upstream_statuses*. Blacklisting state is not shared between Nginx worker
processes.

The next four upstrand directives are akin to those from the Nginx proxy module.

Directive *next_upstream_statuses* accepts *4xx* and *5xx* statuses notation and
values *error* and *timeout* to distinguish between cases when errors happen
with the upstream's peer connections from those when backends send statuses
*502* or *504* (plain values *502* and *504* as well as *5xx* refer to both
cases). It also accepts value *non_idempotent* to allow further processing of
*non-idempotent* requests when they were responded by the last server from an
upstream but failed according to other statuses listed in the directive.
Requests are considered to be non-idempotent when their methods are *POST*,
*LOCK* or *PATCH* just like in directive *proxy_next_upstream*.

Directive *next_upstream_timeout* limits the overall duration time the upstrand
cycles through all of its upstreams. If the time elapses while the upstrand is
ready to pass to a next upstream, the last upstream cycle result is returned.

Directive *intercept_statuses* allows *upstrand failover* by intercepting the
final response in location that matches the given URI. Interceptions must happen
even when the upstrand times out. Notice also that walking through upstreams in
an upstrand and the upstrand failover URI are not interceptable. Speaking more
generally, any internal redirection (by *error_page*, *proxy_intercept_errors*,
*X-Accel-Redirect* etc.) will break nested subrequests on which the upstrand's
implementation is based which leads to returning empty responses. These are
extremely bad cases, and this is why walking through upstreams was protected
against interceptions. The upstrand failover URI is more affected by this as
the implementation has less control over its location. Particularly, the
upstrand failover has only protection against interceptions by *error_page* and
*proxy_intercept_errors*. This means that the upstrand failover URI location
must be as simple as possible (e.g. using simple directives like *return* or
*echo*).

Directive *order* currently accepts only one value *start_random* which means
that starting upstreams in normal and backup cycles after worker fired up will
be chosen randomly. Starting upstreams in further requests will be cycled in
round-robin manner. Additionally, a modifier *per_request* is also accepted in
the *order* directive: it turns off the global per-worker round-robin cycle.
The combination of *per_request* and *start_random* makes the starting upstream
in every new request be chosen randomly.

Such a failover between *failure* statuses can be reached during a single
request by feeding a special variable that starts with *upstrand_* to the
*proxy_pass* directive like so:

```nginx
location /us1 {
    proxy_pass http://$upstrand_us1;
}
```

Be careful when accessing this variable from other directives! It starts up the
subrequests machinery which may be not desirable in many cases.

### Upstrand status variables

There are a number of upstrand status variables available: *upstrand_addr*,
*upstrand_cache_status*, *upstrand_connect_time*, *upstrand_header_time*,
*upstrand_response_length*, *upstrand_response_time* and *upstrand_status*. They
all are counterparts of corresponding *upstream* variables and contain the
values of the latter for all upstreams passed through a request and all
subrequests chronologically. Variable *upstrand_path* contains path of all
upstreams visited during request.

### Where this can be useful

The *upstrand* looks very similar to a simple combined upstream but it also has
a crucial difference: the upstreams inside of an upstrand do not get flattened
and keep holding their identities. This gives a possibility to configure a
*failover* status for a group of servers associated with a single upstream
without need to check them all by turn. In the above example, upstrand *us1* may
hold a list of upstreams like *u01*, *u02* etc. Imagine that upstream *u01*
holds 10 servers inside and represents a part of a geographically distributed
backend system. Let upstrand *us1* combine all such parts in a whole, and let us
run a client application that polls the parts for doing some tasks. Let the
backends send HTTP status *204* if they do not have new tasks. In a flat
combined upstream, all 10 servers may have been polled before the application
will finally receive a new task from another upstream. The upstrand *us1* allows
skipping to the next upstream after checking the first server in an upstream
that does not have tasks. This machinery is apparently suitable for *upstream
broadcasting*, when messages are being sent to all upstreams in an upstrand.

The examples above show that an upstrand can be regarded as a *2-dimensional*
upstream that comprises a number of clusters representing natural upstreams and
allows short-cycling over them.

To illustrate this, let's emulate an upstream without round-robin balancing.
Every new client request will start by proxying to the first server in the
upstream list and then failing over to the next server.

```nginx
    upstream u1 {
        server localhost:8020;
        server localhost:8030;
        combine_server_singlets _single_ nobackup;
    }

    upstrand us1 {
        upstream ~^u1_single_ blacklist_interval=60s;
        order per_request;
        next_upstream_statuses error timeout non_idempotent 5xx;
        intercept_statuses 5xx /Internal/failover;
    }
```

Directive *combine_server_singlets* in upstream *u1* generates two singlet
upstreams *u1_single_1* and *u1_single_2* to inhabit upstrand *us1*. Due to
*per_request* ordering inside the upstrand, the two upstreams will be traversed
in order *u1_single_1 → u1_single_2* in each client request.

## Directive dynamic_upstrand

Allows choosing an upstrand from passed variables in run-time. The directive can
be set in server, location and location-if clauses.

In the following configuration

```nginx
    upstrand us1 {
        upstream ~^u0;
        upstream b01 backup;
        order start_random;
        next_upstream_statuses 5xx;
    }
    upstrand us2 {
        upstream ~^u0;
        upstream b02 backup;
        order start_random;
        next_upstream_statuses 5xx;
    }

    server {
        listen       8010;
        server_name  main;

        dynamic_upstrand $dus1 $arg_a us2;

        location / {
            dynamic_upstrand $dus2 $arg_b;
            if ($arg_b) {
                proxy_pass http://$dus2;
                break;
            }
            proxy_pass http://$dus1;
        }
    }
```

upstrands returned in variables *dus1* and *dus2* are to be chosen from values
of variables *arg_a* and *arg_b*. If *arg_b* is set then the client request will
be sent to an upstrand with name equal to the value of *arg_b*. If there is not
an upstrand with this name then *dus2* will be empty and *proxy_pass* will
return HTTP status *500*. To prevent initialization of a dynamic upstrand
variable with empty value, its declaration must be terminated with a literal
name that corresponds to an existing upstrand. In this example, dynamic upstrand
variable *dus1* will be initialized by the upstrand *us2* if *arg_a* is empty or
not set. Altogether, if *arg_b* is not set or empty and *arg_a* is set and has a
value equal to an existing upstrand, the request will be sent to this upstrand,
otherwise (if *arg_b* is not set or empty and *arg_a* is set but does not refer
to an existing upstrand) *proxy_pass* will most likely return HTTP status *500*
(except there is a variable composed from literal string *upstrand_* and the
value of *arg_a* that points to a valid destination), otherwise (both *arg_b*
and *arg_a* are not set or empty) the request will be sent to the upstrand
*us2*.

## See also

There are several articles about the module in my blog, in chronological order:

1. [*Простой модуль nginx для создания комбинированных
апстримов*](http://lin-techdet.blogspot.com/2011/10/nginx.html) (in Russian). A
comprehensive article discovering details of implementation of directive
*add_upstream* which can also be regarded as a small tutorial for Nginx modules
development.
2. [*nginx upstrand to configure super-layers of
upstreams*](http://lin-techdet.blogspot.com/2015/09/nginx-upstrand-to-configure-super.html).
An overview of block *upstrand* usage and some details on its implementation.
3. [*Не такой уж простой модуль nginx для создания комбинированных
апстримов*](http://lin-techdet.blogspot.com/2015/12/nginx.html) (in Russian). An
overview of all features of the module with configuration examples and testing
session samples.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-combined-upstreams](https://github.com/lyokha/nginx-combined-upstreams-module){target=_blank}.

# *concat*: HTTP Concatenation module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-concat
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_concat_module.so;
```


This document describes nginx-module-concat [v1.2.3](https://github.com/dvershinin/nginx-http-concat/releases/tag/1.2.3){target=_blank} 
released on Jan 15 2020.

<hr />

## Introduction 

This is a module that is distributed with
[tengine](http://tengine.taobao.org) which is a distribution of
[Nginx](http://nginx.org) that is used by the e-commerce/auction site
[Taobao.com](http://en.wikipedia.org/wiki/Taobao). This distribution
contains some modules that are new on the Nginx scene. The
`ngx_http_concat` module is one of them.

The module is inspired by Apache's
[`modconcat`](http://code.google.com/p/modconcat). It follows the same
pattern for enabling the concatenation. It uses two `?`, like this: 

    http://example.com/??style1.css,style2.css,foo/style3.css
    
If a **third** `?` is present it's treated as **version string**. Like
this:

    http://example.com/??style1.css,style2.css,foo/style3.css?v=102234

## Configuration example

    location /static/css/ {
        concat on;
        concat_max_files 20;
    }
        
    location /static/js/ {
        concat on;
        concat_max_files 30;
    }

## Module directives

**concat** `on` | `off`

**default:** `concat off`

**context:** `http, server, location`

It enables the concatenation in a given context.

<br/>
<br/>

**concat_types** `MIME types`

**default:** `concat_types: text/css application/x-javascript`

**context:** `http, server, location`

Defines the [MIME types](http://en.wikipedia.org/wiki/MIME_type) which
can be concatenated in a given context.

<br/>
<br/>

**concat_unique** `on` | `off`

**default:** `concat_unique on`

**context:** `http, server, location`

Defines if only files of a given MIME type can concatenated or if
several MIME types can be concatenated. For example if set to `off`
then in a given context you can concatenate Javascript and CSS files.

Note that the default value is `on`, meaning that only files with same
MIME type are concatenated in a given context. So if you have CSS and
JS you cannot do something like this:

    http://example.com/static/??foo.css,bar/foobaz.js
    
In order to do that you **must** set `concat_unique off`. This applies
to any other type of files that you decide to concatenate by adding
the respective MIME type via `concat_types`,

<br/>
<br/>

**concat\_max\_files** `number`p

**default:** `concat_max_files 10`

**context:** `http, server, location`

Defines the **maximum** number of files that can be concatenated in a
given context. Note that a given URI cannot be bigger than the page
size of your platform. On Linux you can get the page size issuing:

    getconf PAGESIZE
    
Usually is 4k. So if you try to concatenate a lot of files together in
a given context you might hit this barrier. To overcome that OS
defined limitation you must use
the [`large_client_header_buffers`](http://wiki.nginx.org/NginxHttpCoreModule#large_client_header_buffers)
directive. Set it to the value you need.

<br/>
<br/>

**concat_delimiter**: string

**default**: NONE

**context**: `http, server, locatione`

Defines the **delimiter** between two files.
If the config is **concat_delimiter "\n"**,a '\n' would be inserted betwen 1.js and 2.js when
visted http://example.com/??1.js,2.js

<br/>
<br/>

**concat_ignore_file_error**: `on` | `off`

**default**: off

**context**: `http, server, location`

Whether to ignore 404 and 403 or not.

<br/>
<br/>

## Tagging releases 

Perusio is maintaing a tagged release
at http://github.com/alibaba/nginx-http-concat
in synch with the [Tengine](http://tengine.taobao.org)
releases. Refer there for the latest uncommitted tags.
 
## Other tengine modules on Github

 + [footer filter](https://github.com/alibaba/nginx-http-footer-filter):
   allows to add some extra data (markup or not) at the end of a
   request body. It's pratical for things like adding time stamps or
   other miscellaneous stuff without having to tweak your application.
   
 + [http slice](https://github.com/alibaba/nginx-http-slice): allows
   to serve a file by slices. A sort of reverse byte-range. Useful for
   serving large files while not hogging the network.

## Other builds

 1. As referred at the outset this module is part of the
    [`tengine`](http://tengine.taobao.org) Nginx distribution. So you
    might want to save yourself some work and just build it from
    scratch using `tengine` in lieu if the official Nginx source.

 2. If you fancy a bleeding edge Nginx package (from the dev releases)
    for Debian made to measure then you might be interested in Perusio's HA/HP
    [debian](http://debian.perusio.net/unstable) Nginx
    package with built-in support for nginx-http-concat.
    Instructions for using the repository and making the
    package live happily inside a stable distribution installation are
    [provided](http://debian.perusio.net).
        

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-concat](https://github.com/dvershinin/nginx-http-concat){target=_blank}.

# *cookie-flag*: NGINX cookie flag module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-cookie-flag
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_cookie_flag_filter_module.so;
```


This document describes nginx-module-cookie-flag [v1.1.0](https://github.com/AirisX/nginx_cookie_flag_module/releases/tag/v1.1.0){target=_blank} 
released on Dec 15 2017.

<hr />

[![License](http://img.shields.io/badge/license-BSD-brightgreen.svg)](https://github.com/Airis777/nginx_cookie_flag_module/blob/master/LICENSE)

The Nginx module for adding cookie flag

## Synopsis

```Nginx
location / {
    set_cookie_flag Secret HttpOnly secure SameSite;
    set_cookie_flag * HttpOnly;
    set_cookie_flag SessionID SameSite=Lax secure;
    set_cookie_flag SiteToken SameSite=Strict;
}
```

## Description
This module for Nginx allows to set the flags "**HttpOnly**", "**secure**" and "**SameSite**" for cookies in the "*Set-Cookie*" response headers.
The register of letters for the flags doesn't matter as it will be converted to the correct value. The order of cookie declaration among multiple directives doesn't matter too.
It is possible to set a default value using symbol "*". In this case flags will be added to the all cookies if no other value for them is overriden.

## Directives

### set_cookie_flag

-| -
--- | ---
**Syntax**  | **set_cookie_flag** \<cookie_name\|*\> [HttpOnly] [secure] [SameSite\|SameSite=[Lax\|Strict]];
**Default** | -
**Context** | server, location

Description: Add flag to desired cookie.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-cookie-flag](https://github.com/AirisX/nginx_cookie_flag_module){target=_blank}.

# *cookie-limit*: NGINX module to limit the number of malicious ip forged cookies


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-cookie-limit
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_cookie_limit_req_module.so;
```


This document describes nginx-module-cookie-limit [v1.2](https://github.com/limithit/ngx_cookie_limit_req_module/releases/tag/1.2){target=_blank} 
released on Jun 23 2022.

<hr />
 
## Introduction

The *ngx_cookie_limit_req_module* module not only limits the access rate of cookies but also limits the number of malicious ip forged cookies.

## Donate
The developers work tirelessly to improve and develop ngx_cookie_limit_req_module. Many hours have been put in to provide the software as it is today, but this is an extremely time-consuming process with no financial reward. If you enjoy using the software, please consider donating to the devs, so they can spend more time implementing improvements.

 ### Alipay:
![Alipay](https://github.com/limithit/shellcode/blob/master/alipay.png)

Author
Gandalf zhibu1991@gmail.com

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-cookie-limit](https://github.com/limithit/ngx_cookie_limit_req_module){target=_blank}.

# *coolkit*: NGINX CoolKit Module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-coolkit
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_coolkit_module.so;
```


This document describes nginx-module-coolkit [v0.2](https://github.com/dvershinin/ngx_coolkit/releases/tag/0.2){target=_blank} 
released on Aug 23 2020.

<hr />
ngx_coolkit is collection of small and useful nginx add-ons.


## CONFIGURATION DIRECTIVES:

##   override_method off | [methods] source (context: http, server, location)
  Override HTTP method.

  default: none


## CONFIGURATION VARIABLES:

##   $remote_passwd
  Decoded password from "Authorization" header (Basic HTTP Authentication).


##   $location
  Name of the matched location block.


## EXAMPLE CONFIGURATION #1:
http {
    server {
        location / {
            override_method  $arg_method;
            proxy_pass       http://127.0.0.1:8100;
        }
    }
}

Pass request with changed HTTP method (based on "?method=XXX") to the backend.


## EXAMPLE CONFIGURATION #2:
http {
    upstream database {
        postgres_server        127.0.0.1 dbname=test
                               user=monty password=some_pass;
    }

    server {
        location = /auth {
            internal;

            set_quote_sql_str  $user $remote_user;
            set_quote_sql_str  $pass $remote_passwd;

            postgres_pass      database;
            postgres_query     "SELECT login FROM users WHERE login=$user AND pass=$pass";
            postgres_rewrite   no_rows 403;
            postgres_output    none;
        }

        location / {
            auth_request       /auth;
            root               /files;
        }
    }
}

Restrict access to local files by authenticating against SQL database.

Required modules (other than ngx_coolkit):
- ngx_http_auth_request_module,
- ngx_postgres (PostgreSQL) or ngx_drizzle (MySQL, Drizzle, SQLite),
- ngx_set_misc.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-coolkit](https://github.com/dvershinin/ngx_coolkit){target=_blank}.

# *dav-ext*: NGINX WebDAV PROPFIND,OPTIONS,LOCK,UNLOCK support


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-dav-ext
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_dav_ext_module.so;
```


This document describes nginx-module-dav-ext [v3.0.0](https://github.com/arut/nginx-dav-ext-module/releases/tag/v3.0.0){target=_blank} 
released on Dec 17 2018.

<hr />

[nginx](http://nginx.org) [WebDAV](https://tools.ietf.org/html/rfc4918)
PROPFIND,OPTIONS,LOCK,UNLOCK support.

## About

The standard
[ngx_http_dav_module](http://nginx.org/en/docs/http/ngx_http_dav_module.html)
provides partial [WebDAV](https://tools.ietf.org/html/rfc4918)
implementation and only supports GET,HEAD,PUT,DELETE,MKCOL,COPY,MOVE
methods.

For full [WebDAV](https://tools.ietf.org/html/rfc4918) support in
[nginx](http://nginx.org) you need to enable the standard
[ngx_http_dav_module](http://nginx.org/en/docs/http/ngx_http_dav_module.html)
as well as this module for the missing methods.

## Testing

The module tests require standard
[nginx-tests](http://hg.nginx.org/nginx-tests) and Perl `HTTP::DAV`
library.

``` bash
$ export PERL5LIB=/path/to/nginx-tests/lib
$ export TEST_NGINX_BINARY=/path/to/nginx
$ prove t
```

## Locking

-   Only the exclusive write locks are supported, which is the only type
    of locks described in the
    [WebDAV](https://tools.ietf.org/html/rfc4918) specification.
-   All currently held locks are kept in a list. Checking if an object
    is constrained by a lock requires O(n) operations. A huge number of
    simultaneously held locks may degrade performance. Thus it is not
    recommended to have a large lock timeout which would increase the
    number of locks.

## Directives

### dav_ext_methods

|            |                                                        |
|------------|--------------------------------------------------------|
| *Syntax:*  | `dav_ext_methods [PROPFIND] [OPTIONS] [LOCK] [UNLOCK]` |
| *Context:* | http, server, location                                 |

Enables support for the specified WebDAV methods in the current scope.

### dav_ext_lock_zone

|            |                                                      |
|------------|------------------------------------------------------|
| *Syntax:*  | `dav_ext_lock_zone zone=NAME:SIZE [timeout=TIMEOUT]` |
| *Context:* | http                                                 |

Defines a shared zone for WebDAV locks with specified NAME and SIZE.
Also, defines a lock expiration TIMEOUT. Default lock timeout value is 1
minute.

### dav_ext_lock

|            |                          |
|------------|--------------------------|
| *Syntax:*  | `dav_ext_lock zone=NAME` |
| *Context:* | http, server, location   |

Enables WebDAV locking in the specified scope. Locks are stored in the
shared zone specified by NAME. This zone must be defined with the
`dav_ext_lock_zone` directive.

Note that even though this directive enables locking capabilities in the
current scope, HTTP methods LOCK and UNLOCK should also be explicitly
specified in the `dav_ext_methods`.

## Example 1

Simple lockless example:

    location / {
        root /data/www;

        dav_methods PUT DELETE MKCOL COPY MOVE;
        dav_ext_methods PROPFIND OPTIONS;
    }

## Example 2

WebDAV with locking:

    http {
        dav_ext_lock_zone zone=foo:10m;

        ...

        server {
            ...

            location / {
                root /data/www;

                dav_methods PUT DELETE MKCOL COPY MOVE;
                dav_ext_methods PROPFIND OPTIONS LOCK UNLOCK;
                dav_ext_lock zone=foo;
            }
        }
    }

## Example 3

WebDAV with locking which works with MacOS client:

    http {
        dav_ext_lock_zone zone=foo:10m;

        ...

        server {
            ...

            location / {
                root /data/www;

                # enable creating directories without trailing slash
                set $x $uri$request_method;
                if ($x ~ [^/]MKCOL$) {
                    rewrite ^(.*)$ $1/;
                }

                dav_methods PUT DELETE MKCOL COPY MOVE;
                dav_ext_methods PROPFIND OPTIONS LOCK UNLOCK;
                dav_ext_lock zone=foo;
            }
        }
    }

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-dav-ext](https://github.com/arut/nginx-dav-ext-module){target=_blank}.

# *doh*: NGINX module for serving DNS-over-HTTPS (DOH) requests


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-doh
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_doh_module.so;
```


This document describes nginx-module-doh [v0.1](https://github.com/dvershinin/Nginx-DOH-Module/releases/tag/0.1){target=_blank} 
released on Jan 15 2020.

<hr />
Simple Nginx module for serving DNS-over-HTTPS (DOH) requests.

CAVEAT EMPTOR: This module is experimental, even though I have been using it successfully with both Firefox and Curl, there may be undiscovered bugs. Zone transfer is currently not officially supported.

Tested with Nginx versions:
1.16.1 (stable)
1.17.6
1.17.7 (mainline).

Instructions for building installing and using Nginx modules can be found at the links below.

dynamic: https://www.nginx.com/resources/wiki/extending/converting/#compiling-dynamic

static: https://www.nginx.com/resources/wiki/extending/compiling/

I have included a config file for both building as both a dynamic and static module.

This module is only allowed to be used in an http location block.

MODULE DIRECTIVES

doh: (takes no arguments) enable DOH at this location block, default upstream DNS server address is 127.0.0.1, default port is 53, and default timeout is 5 seconds.

doh_address: (takes 1 argument) sets the address of the upstream DNS server, can be either IPv4 or IPv6.

doh_port: (takes 1 argument) sets the port to contact the upstream DNS server on (appies to both TCP and UDP connections).

doh_timeout: (takes 1 argument) sets the timeout in seconds.

EXAMPLES

simplest use case with upstream DNS server listening on 127.0.0.1 on port 53:

```nginx
location /dns-query { 
	doh;
}
```

set an upstream address of 127.0.2.1, a port of 5353, and a timeout of 2 seconds:

```nginx
location /dns-query { 
	doh;
	doh_address 127.0.2.1;
	doh_port 5353;
	doh_timeout 2;
}
```

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-doh](https://github.com/dvershinin/Nginx-DOH-Module){target=_blank}.

# *dynamic-etag*: NGINX module for adding ETag to dynamic content


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-dynamic-etag
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_dynamic_etag_module.so;
```


This document describes nginx-module-dynamic-etag [v0.2.1](https://github.com/dvershinin/ngx_dynamic_etag/releases/tag/0.2.1){target=_blank} 
released on Aug 11 2019.

<hr />


This NGINX module empowers your dynamic content with automatic [`ETag`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag)
header. It allows client browsers to issue conditional `GET` requests to 
dynamic pages. And thus saves bandwidth and ensures better performance! 

## Caveats first!

This module is a real hack: it calls a header filter from a body filter, etc. 

The original author abandoned it, [having to say](https://github.com/kali/nginx-dynamic-etags/issues/2):
 
 > It never really worked.

I largely rewrote it to deal with existing obvious faults, but the key part with buffers, 
which, myself being old, I probably will never understand, is untouched.

To be reliable, the module has to read entire response and take a hash of it. 
Reading entire response is against NGINX lightweight design.
I am not sure whether the buffer part waits for the entire response.

Having said that, the tests which I added showcase that this whole stuff works!

Note that the `HEAD` requests will not have any `ETag` returned, because we have no data to play with, 
since NGINX rightfully discards body for this request method.

Consider this as a feature or a bug :-) If we remove this, then all `HEAD` requests end up having same `ETag` (hash on emptiness),
which is definitely worse.

Thus, be sure you check headers like this:

    curl -IL -X GET https://www.example.com/
    
 And not like this:
 
     curl -IL https://www.example.com/
     
Another worthy thing to mention is that it makes little to no sense applying dynamic `ETag` on a page that changes on 
each reload. E.g. I found I wasn't using the dynamic `ETag` with benefits, because of `<?= antispambot(get_option('admin_email')) ?>`,
in my Wordpress theme's `header.php`, since in this function:

> the selection is random and changes each time the function is called 

To quickly check if your page is changing on reload, use:

    diff <(curl http://www.example.com") <(curl http://www.example.com")

Now that we're done with the "now you know" yada-yada, you can proceed with trying out this stuff :)    


## Synopsis

```nginx
http {
    server {
        location ~ \.php$ {
            dynamic_etag on;
            fastcgi_pass ...;
        }
    }
}
```

## Configuration directives

### `dynamic_etag`

- **syntax**: `dynamic_etag on|off|$var`
- **default**: `off`
- **context**: `http`, `server`, `location`, `if`

Enables or disables applying ETag automatically.

### `dynamic_etag_types`

- **syntax**: `dynamic_etag_types <mime_type> [..]`
- **default**: `text/html`
- **context**: `http`, `server`, `location`

Enables applying ETag automatically for the specified MIME types
in addition to `text/html`. The special value `*` matches any MIME type.
Responses with the `text/html` MIME type are always included.

## Tips

You can use `map` directive for conditionally enabling dynamic `ETag` based on URLs, e.g.:

    map $request_uri $dyn_etag {
        default "off";
        /foo "on";
        /bar "on";
    }
    server { 
       ...
       location / {
           dynamic_etag $dyn_etag;
           fastcgi_pass ...
       }
    }       
        

## Original author's README

Attempt at handling ETag / If-None-Match on proxied content.

I plan on using this to front a Varnish server using a lot of ESI.

It does kind of work, but... be aware, this is my first attempt at developing
a nginx plugin, and dealing with headers after having read the body was not
exactly in the how-to.

Any comment and/or improvement and/or fork is welcome.

Thanks to http://github.com/kkung/nginx-static-etags/ for... inspiration.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-dynamic-etag](https://github.com/dvershinin/ngx_dynamic_etag){target=_blank}.

# *dynamic-limit-req*: NGINX module to dynamically lock IP and release it periodically


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-dynamic-limit-req
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_dynamic_limit_req_module.so;
```


This document describes nginx-module-dynamic-limit-req [v1.9.3](https://github.com/limithit/ngx_dynamic_limit_req_module/releases/tag/1.9.3){target=_blank} 
released on Jan 29 2021.

<hr />
 
## Introduction

The *ngx_dynamic_limit_req_module* module is used to dynamically lock IP and release it periodically.

## principle
The ngx_dynamic_limit_req_module module is used to limit the request processing rate per a defined key, in particular, the processing rate of requests coming from a single IP address. The limitation is done using the “leaky bucket” method.

## About
This module is an extension based on [ngx_http_limit_req_module](http://nginx.org/en/docs/http/ngx_http_limit_req_module.html).

## Donate
The developers work tirelessly to improve and develop ngx_dynamic_limit_req_module. Many hours have been put in to provide the software as it is today, but this is an extremely time-consuming process with no financial reward. If you enjoy using the software, please consider donating to the devs, so they can spend more time implementing improvements.

 ### Alipay:
![Alipay](https://github.com/limithit/shellcode/blob/master/alipay.png)

## Extend
This module can be works with [RedisPushIptables](https://github.com/limithit/RedisPushIptables),  the application layer matches then the network layer to intercept. Although network layer interception will save resources, there are also deficiencies. Assuming that only one specific interface is filtered and no other interfaces are filtered, those that do not need to be filtered will also be inaccessible. Although precise control is not possible at the network layer or the transport layer, it can be precisely controlled at the application layer. Users need to weigh which solution is more suitable for the event at the time.

## Api-count
### If you want to use the api counting function, please use [limithit-API_alerts](https://github.com/limithit/ngx_dynamic_limit_req_module/tree/limithit-API_alerts). Because not everyone needs this feature, so it doesn't merge into the trunk. Users who do not need this feature can skip this paragraph description.

``` 
git clone https://github.com/limithit/ngx_dynamic_limit_req_module.git
cd ngx_dynamic_limit_req_module
git checkout limithit-API_alerts
```
``` 
root@debian:~# redis-cli 
127.0.0.1:6379> SELECT 3
127.0.0.1:6379[3]> scan 0 match *12/Dec/2018* count 10000 
127.0.0.1:6379[3]> scan 0 match *PV count 10000
1) "0"
2) 1) "[13/Dec/2018]PV"
   2) "[12/Dec/2018]PV"
127.0.0.1:6379[3]> get [12/Dec/2018]PV
"9144"
127.0.0.1:6379[3]> get [13/Dec/2018]PV
"8066"
127.0.0.1:6379[3]> get [13/Dec/2018]UV
"214"

```

This module is compatible with following nginx releases:

Author
Gandalf zhibu1991@gmail.com

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-dynamic-limit-req](https://github.com/limithit/ngx_dynamic_limit_req_module){target=_blank}.

# *echo*: nginx Echo module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-echo
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_echo_module.so;
```


This document describes nginx-module-echo [v0.63](https://github.com/openresty/echo-nginx-module/releases/tag/v0.63){target=_blank} 
released on Jul 31 2022.

<hr />

**ngx_echo** - Brings "echo", "sleep", "time", "exec" and more shell-style goodies to Nginx config file.


## Status

This module is production ready.

## Synopsis

```nginx

   location /hello {
     echo "hello, world!";
   }
```

```nginx

   location /hello {
     echo -n "hello, ";
     echo "world!";
   }
```

```nginx

   location /timed_hello {
     echo_reset_timer;
     echo hello world;
     echo "'hello world' takes about $echo_timer_elapsed sec.";
     echo hiya igor;
     echo "'hiya igor' takes about $echo_timer_elapsed sec.";
   }
```

```nginx

   location /echo_with_sleep {
     echo hello;
     echo_flush;  # ensure the client can see previous output immediately
     echo_sleep   2.5;  # in sec
     echo world;
   }
```

```nginx

   # in the following example, accessing /echo yields
   #   hello
   #   world
   #   blah
   #   hiya
   #   igor
   location /echo {
       echo_before_body hello;
       echo_before_body world;
       proxy_pass $scheme://127.0.0.1:$server_port$request_uri/more;
       echo_after_body hiya;
       echo_after_body igor;
   }
   location /echo/more {
       echo blah;
   }
```

```nginx

   # the output of /main might be
   #   hello
   #   world
   #   took 0.000 sec for total.
   # and the whole request would take about 2 sec to complete.
   location /main {
       echo_reset_timer;

       # subrequests in parallel
       echo_location_async /sub1;
       echo_location_async /sub2;

       echo "took $echo_timer_elapsed sec for total.";
   }
   location /sub1 {
       echo_sleep 2;
       echo hello;
   }
   location /sub2 {
       echo_sleep 1;
       echo world;
   }
```

```nginx

   # the output of /main might be
   #   hello
   #   world
   #   took 3.003 sec for total.
   # and the whole request would take about 3 sec to complete.
   location /main {
       echo_reset_timer;

       # subrequests in series (chained by CPS)
       echo_location /sub1;
       echo_location /sub2;

       echo "took $echo_timer_elapsed sec for total.";
   }
   location /sub1 {
       echo_sleep 2;
       echo hello;
   }
   location /sub2 {
       echo_sleep 1;
       echo world;
   }
```

```nginx

   # Accessing /dup gives
   #   ------ END ------
   location /dup {
     echo_duplicate 3 "--";
     echo_duplicate 1 " END ";
     echo_duplicate 3 "--";
     echo;
   }
```

```nginx

   # /bighello will generate 1000,000,000 hello's.
   location /bighello {
     echo_duplicate 1000_000_000 'hello';
   }
```

```nginx

   # echo back the client request
   location /echoback {
     echo_duplicate 1 $echo_client_request_headers;
     echo "\r";

     echo_read_request_body;

     echo_request_body;
   }
```

```nginx

   # GET /multi will yields
   #   querystring: foo=Foo
   #   method: POST
   #   body: hi
   #   content length: 2
   #   ///
   #   querystring: bar=Bar
   #   method: PUT
   #   body: hello
   #   content length: 5
   #   ///
   location /multi {
       echo_subrequest_async POST '/sub' -q 'foo=Foo' -b 'hi';
       echo_subrequest_async PUT '/sub' -q 'bar=Bar' -b 'hello';
   }
   location /sub {
       echo "querystring: $query_string";
       echo "method: $echo_request_method";
       echo "body: $echo_request_body";
       echo "content length: $http_content_length";
       echo '///';
   }
```

```nginx

   # GET /merge?/foo.js&/bar/blah.js&/yui/baz.js will merge the .js resources together
   location /merge {
       default_type 'text/javascript';
       echo_foreach_split '&' $query_string;
           echo "/* JS File $echo_it */";
           echo_location_async $echo_it;
           echo;
       echo_end;
   }
```

```nginx

   # accessing /if?val=abc yields the "hit" output
   # while /if?val=bcd yields "miss":
   location ^~ /if {
       set $res miss;
       if ($arg_val ~* '^a') {
           set $res hit;
           echo $res;
       }
       echo $res;
   }
```


## Description

This module wraps lots of Nginx internal APIs for streaming input and output, parallel/sequential subrequests, timers and sleeping, as well as various meta data accessing.

Basically it provides various utilities that help testing and debugging of other modules by trivially emulating different kinds of faked subrequest locations.

People will also find it useful in real-world applications that need to

1. serve static contents directly from memory (loading from the Nginx config file).
1. wrap the upstream response with custom header and footer (kinda like the [addition module](http://nginx.org/en/docs/http/ngx_http_addition_module.html) but with contents read directly from the config file and Nginx variables).
1. merge contents of various "Nginx locations" (i.e., subrequests) together in a single main request (using [echo_location](#echo_location) and its friends).

This is a special dual-role module that can *lazily* serve as a content handler or register itself as an output filter only upon demand. By default, this module does not do anything at all.

Technically, this module has also demonstrated the following techniques that might be helpful for module writers:

1. Issue parallel subrequests directly from content handler.
1. Issue chained subrequests directly from content handler, by passing continuation along the subrequest chain.
1. Issue subrequests with all HTTP 1.1 methods and even an optional faked HTTP request body.
1. Interact with the Nginx event model directly from content handler using custom events and timers, and resume the content handler back if necessary.
1. Dual-role module that can (lazily) serve as a content handler or an output filter or both.
1. Nginx config file variable creation and interpolation.
1. Streaming output control using output_chain, flush and its friends.
1. Read client request body from the content handler, and returns back (asynchronously) to the content handler after completion.
1. Use Perl-based declarative [test suite](#test-suite) to drive the development of Nginx C modules.


## Content Handler Directives

Use of the following directives register this module to the current Nginx location as a content handler. If you want to use another module, like the [standard proxy module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html), as the content handler, use the [filter directives](#filter-directives) provided by this module.

All the content handler directives can be mixed together in a single Nginx location and they're supposed to run sequentially just as in the Bash scripting language.

Every content handler directive supports variable interpolation in its arguments (if any).

The MIME type set by the [standard default_type directive](http://nginx.org/en/docs/http/ngx_http_core_module.html#default_type) is respected by this module, as in:

```nginx

   location /hello {
     default_type text/plain;
     echo hello;
   }
```

Then on the client side:

```bash

   $ curl -I 'http://localhost/echo'
   HTTP/1.1 200 OK
   Server: nginx/0.8.20
   Date: Sat, 17 Oct 2009 03:40:19 GMT
   Content-Type: text/plain
   Connection: keep-alive
```

Since the [v0.22](#v022) release, all of the directives are allowed in the [rewrite module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html)'s [if](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#if) directive block, for instance:

```nginx

 location ^~ /if {
     set $res miss;
     if ($arg_val ~* '^a') {
         set $res hit;
         echo $res;
     }
     echo $res;
 }
```


## echo
**syntax:** *echo \[options\] &lt;string&gt;...*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Sends arguments joined by spaces, along with a trailing newline, out to the client.

Note that the data might be buffered by Nginx's underlying buffer. To force the output data flushed immediately, use the [echo_flush](#echo_flush) command just after `echo`, as in

```nginx

    echo hello world;
    echo_flush;
```

When no argument is specified, *echo* emits the trailing newline alone, just like the *echo* command in shell.

Variables may appear in the arguments. An example is

```nginx

    echo The current request uri is $request_uri;
```

where [$request_uri](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_uri) is a variable exposed by the [ngx_http_core_module](http://nginx.org/en/docs/http/ngx_http_core_module.html).

This command can be used multiple times in a single location configuration, as in

```nginx

 location /echo {
     echo hello;
     echo world;
 }
```

The output on the client side looks like this

```bash

 $ curl 'http://localhost/echo'
 hello
 world
```

Special characters like newlines (`\n`) and tabs (`\t`) can be escaped using C-style escaping sequences. But a notable exception is the dollar sign (`$`). As of Nginx 0.8.20, there's still no clean way to escape this character. (A work-around might be to use a `$echo_dollor` variable that is always evaluated to the constant `$` character. This feature will possibly be introduced in a future version of this module.)

As of the echo [v0.28](#v028) release, one can suppress the trailing newline character in the output by using the `-n` option, as in

```nginx

 location /echo {
     echo -n "hello, ";
     echo "world";
 }
```

Accessing `/echo` gives

```bash

 $ curl 'http://localhost/echo'
 hello, world
```

Leading `-n` in variable values won't take effect and will be emitted literally, as in

```nginx

 location /echo {
     set $opt -n;
     echo $opt "hello,";
     echo "world";
 }
```

This gives the following output

```bash

 $ curl 'http://localhost/echo'
 -n hello,
 world
```

One can output leading `-n` literals and other options using the special `--` option like this

```nginx

 location /echo {
     echo -- -n is an option;
 }
```

which yields

```bash

 $ curl 'http://localhost/echo'
 -n is an option
```

Use this form when you want to output anything leading with a dash (`-`).


## echo_duplicate
**syntax:** *echo_duplicate &lt;count&gt; &lt;string&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Outputs duplication of a string indicated by the second argument, using the count specified in the first argument.

For instance,

```nginx

   location /dup {
       echo_duplicate 3 "abc";
   }
```

will lead to the output of `"abcabcabc"`.

Underscores are allowed in the count number, just like in Perl. For example, to emit 1000,000,000 instances of `"hello, world"`:

```nginx

   location /many_hellos {
       echo_duplicate 1000_000_000 "hello, world";
   }
```

The `count` argument could be zero, but not negative. The second `string` argument could be an empty string ("") likewise.

Unlike the [echo](#echo) directive, no trailing newline is appended to the result. So it's possible to "abuse" this directive as a no-trailing-newline version of [echo](#echo) by using "count" 1, as in

```nginx

   location /echo_art {
       echo_duplicate 2 '---';
       echo_duplicate 1 ' END ';  # we don't want a trailing newline here
       echo_duplicate 2 '---';
       echo;  # we want a trailing newline here...
   }
```

You get

```bash
   ------ END ------
```

But use of the `-n` option in [echo](#echo) is more appropriate for this purpose.

This directive was first introduced in [version 0.11](#v011).


## echo_flush
**syntax:** *echo_flush*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Forces the data potentially buffered by underlying Nginx output filters to send immediately to the client side via socket.

Note that techically the command just emits a ngx_buf_t object with `flush` slot set to 1, so certain weird third-party output filter module could still block it before it reaches Nginx's (last) write filter.

This directive does not take any argument.

Consider the following example:

```nginx

   location /flush {
      echo hello;

      echo_flush;

      echo_sleep 1;
      echo world;
   }
```

Then on the client side, using curl to access `/flush`, you'll see the "hello" line immediately, but only after 1 second, the last "world" line. Without calling `echo_flush` in the example above, you'll most likely see no output until 1 second is elapsed due to the internal buffering of Nginx.

This directive will fail to flush the output buffer in case of subrequests get involved. Consider the following example:

```nginx

   location /main {
       echo_location_async /sub;
       echo hello;
       echo_flush;
   }
   location /sub {
       echo_sleep 1;
   }
```

Then the client won't see "hello" appear even if `echo_flush` has been executed before the subrequest to `/sub` has actually started executing. The outputs of `/main` that are sent *after* [echo_location_async](#echo_location_async) will be postponed and buffered firmly.

This does *not* apply to outputs sent before the subrequest initiated. For a modified version of the example given above:

```nginx

   location /main {
       echo hello;
       echo_flush;
       echo_location_async /sub;
   }
   location /sub {
       echo_sleep 1;
   }
```

The client will immediately see "hello" before `/sub` enters sleeping.

See also [echo](#echo), [echo_sleep](#echo_sleep), and [echo_location_async](#echo_location_async).


## echo_sleep
**syntax:** *echo_sleep &lt;seconds&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Sleeps for the time period specified by the argument, which is in seconds.

This operation is non-blocking on server side, so unlike the [echo_blocking_sleep](#echo_blocking_sleep) directive, it won't block the whole Nginx worker process.

The period might takes three digits after the decimal point and must be greater than 0.001.

An example is

```nginx

    location /echo_after_sleep {
        echo_sleep 1.234;
        echo resumed!;
    }
```

Behind the scene, it sets up a per-request "sleep" ngx_event_t object, and adds a timer using that custom event to the Nginx event model and just waits for a timeout on that event. Because the "sleep" event is per-request, this directive can work in parallel subrequests.


## echo_blocking_sleep
**syntax:** *echo_blocking_sleep &lt;seconds&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

This is a blocking version of the [echo_sleep](#echo_sleep) directive.

See the documentation of [echo_sleep](#echo_sleep) for more detail.

Behind the curtain, it calls the ngx_msleep macro provided by the Nginx core which maps to usleep on POSIX-compliant systems.

Note that this directive will block the current Nginx worker process completely while being executed, so never use it in production environment.


## echo_reset_timer
**syntax:** *echo_reset_timer*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Reset the timer begin time to *now*, i.e., the time when this command is executed during request.

The timer begin time is default to the starting time of the current request and can be overridden by this directive, potentially multiple times in a single location. For example:

```nginx

   location /timed_sleep {
       echo_sleep 0.03;
       echo "$echo_timer_elapsed sec elapsed.";

       echo_reset_timer;

       echo_sleep 0.02;
       echo "$echo_timer_elapsed sec elapsed.";
   }
```

The output on the client side might be

```bash

 $ curl 'http://localhost/timed_sleep'
 0.032 sec elapsed.
 0.020 sec elapsed.
```

The actual figures you get on your side may vary a bit due to your system's current activities.

Invocation of this directive will force the underlying Nginx timer to get updated to the current system time (regardless the timer resolution specified elsewhere in the config file). Furthermore, references of the [$echo_timer_elapsed](#echo_timer_elapsed) variable will also trigger timer update forcibly.

See also [echo_sleep](#echo_sleep) and [$echo_timer_elapsed](#echo_timer_elapsed).


## echo_read_request_body
**syntax:** *echo_read_request_body*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Explicitly reads request body so that the [$request_body](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_body) variable will always have non-empty values (unless the body is so big that it has been saved by Nginx to a local temporary file).

Note that this might not be the original client request body because the current request might be a subrequest with a "artificial" body specified by its parent.

This directive does not generate any output itself, just like [echo_sleep](#echo_sleep).

Here's an example for echo'ing back the original HTTP client request (both headers and body are included):

```nginx

   location /echoback {
     echo_duplicate 1 $echo_client_request_headers;
     echo "\r";
     echo_read_request_body;
     echo $request_body;
   }
```

The content of `/echoback` looks like this on my side (I was using Perl's LWP utility to access this location on the server):

```bash

   $ (echo hello; echo world) | lwp-request -m POST 'http://localhost/echoback'
   POST /echoback HTTP/1.1
   TE: deflate,gzip;q=0.3
   Connection: TE, close
   Host: localhost
   User-Agent: lwp-request/5.818 libwww-perl/5.820
   Content-Length: 12
   Content-Type: application/x-www-form-urlencoded

   hello
   world
```

Because `/echoback` is the main request, [$request_body](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_body) holds the original client request body.

Before Nginx 0.7.56, it makes no sense to use this directive because [$request_body](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_body) was first introduced in Nginx 0.7.58.

This directive itself was first introduced in the echo module's [v0.14 release](#v014).


## echo_location_async
**syntax:** *echo_location_async &lt;location&gt; [&lt;url_args&gt;]*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Issue GET subrequest to the location specified (first argument) with optional url arguments specified in the second argument.

As of Nginx 0.8.20, the `location` argument does *not* support named location, due to a limitation in the `ngx_http_subrequest` function. The same is true for its brother, the [echo_location](#echo_location) directive.

A very simple example is

```nginx

 location /main {
     echo_location_async /sub;
     echo world;
 }
 location /sub {
     echo hello;
 }
```

Accessing `/main` gets

```bash

   hello
   world
```

Calling multiple locations in parallel is also possible:

```nginx

 location /main {
     echo_reset_timer;
     echo_location_async /sub1;
     echo_location_async /sub2;
     echo "took $echo_timer_elapsed sec for total.";
 }
 location /sub1 {
     echo_sleep 2; # sleeps 2 sec
     echo hello;
 }
 location /sub2 {
     echo_sleep 1; # sleeps 1 sec
     echo world;
 }
```

Accessing `/main` yields

```bash

   $ time curl 'http://localhost/main'
   hello
   world
   took 0.000 sec for total.

   real  0m2.006s
   user  0m0.000s
   sys   0m0.004s
```

You can see that the main handler `/main` does *not* wait the subrequests `/sub1` and `/sub2` to complete and quickly goes on, hence the "0.000 sec" timing result. The whole request, however takes approximately 2 sec in total to complete because `/sub1` and `/sub2` run in parallel (or "concurrently" to be more accurate).

If you use [echo_blocking_sleep](#echo_blocking_sleep) in the previous example instead, then you'll get the same output, but with 3 sec total response time, because "blocking sleep" blocks the whole Nginx worker process.

Locations can also take an optional querystring argument, for instance

```nginx

 location /main {
     echo_location_async /sub 'foo=Foo&bar=Bar';
 }
 location /sub {
     echo $arg_foo $arg_bar;
 }
```

Accessing `/main` yields

```bash

   $ curl 'http://localhost/main'
   Foo Bar
```

Querystrings is *not* allowed to be concatenated onto the `location` argument with "?" directly, for example, `/sub?foo=Foo&bar=Bar` is an invalid location, and shouldn't be fed as the first argument to this directive.

Technically speaking, this directive is an example that Nginx content handler issues one or more subrequests directly. AFAIK, the [fancyindex module](https://connectical.com/projects/ngx-fancyindex/wiki) also does such kind of things ;)

Nginx named locations like `@foo` is *not* supported here.

This directive is logically equivalent to the GET version of [echo_subrequest_async](#echo_subrequest_async). For example,

```nginx

   echo_location_async /foo 'bar=Bar';
```

is logically equivalent to

```nginx

   echo_subrequest_async GET /foo -q 'bar=Bar';
```

But calling this directive is slightly faster than calling [echo_subrequest_async](#echo_subrequest_async) using `GET` because we don't have to parse the HTTP method names like `GET` and options like `-q`.

This directive is first introduced in [version 0.09](#v009) of this module and requires at least Nginx 0.7.46.


## echo_location
**syntax:** *echo_location &lt;location&gt; [&lt;url_args&gt;]*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Just like the [echo_location_async](#echo_location_async) directive, but `echo_location` issues subrequests *in series* rather than in parallel. That is, the content handler directives following this directive won't be executed until the subrequest issued by this directive completes.

The final response body is almost always equivalent to the case when [echo_location_async](#echo_location_async) is used instead, only if timing variables is used in the outputs.

Consider the following example:

```nginx

 location /main {
     echo_reset_timer;
     echo_location /sub1;
     echo_location /sub2;
     echo "took $echo_timer_elapsed sec for total.";
 }
 location /sub1 {
     echo_sleep 2;
     echo hello;
 }
 location /sub2 {
     echo_sleep 1;
     echo world;
 }
```

The location `/main` above will take for total 3 sec to complete (compared to 2 sec if [echo_location_async](#echo_location_async) is used instead here). Here's the result in action on my machine:

```bash

   $ curl 'http://localhost/main'
   hello
   world
   took 3.003 sec for total.

   real  0m3.027s
   user  0m0.020s
   sys   0m0.004s
```

This directive is logically equivalent to the GET version of [echo_subrequest](#echo_subrequest). For example,

```nginx

   echo_location /foo 'bar=Bar';
```

is logically equivalent to

```nginx

   echo_subrequest GET /foo -q 'bar=Bar';
```

But calling this directive is slightly faster than calling [echo_subrequest](#echo_subrequest) using `GET` because we don't have to parse the HTTP method names like `GET` and options like `-q`.

Behind the scene, it creates an `ngx_http_post_subrequest_t` object as a *continuation* and passes it into the `ngx_http_subrequest` function call. Nginx will later reopen this "continuation" in the subrequest's `ngx_http_finalize_request` function call. We resumes the execution of the parent-request's content handler and starts to run the next directive (command) if any.

Nginx named locations like `@foo` is *not* supported here.

This directive was first introduced in the [release v0.12](#v012).

See also [echo_location_async](#echo_location_async) for more details about the meaning of the arguments.


## echo_subrequest_async
**syntax:** *echo_subrequest_async &lt;HTTP_method&gt; &lt;location&gt; [-q &lt;url_args&gt;] [-b &lt;request_body&gt;] [-f &lt;request_body_path&gt;]*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Initiate an asynchronous subrequest using HTTP method, an optional url arguments (or querystring) and an optional request body which can be defined as a string or as a path to a file which contains the body.

This directive is very much like a generalized version of the [echo_location_async](#echo_location_async) directive.

Here's a small example demonstrating its usage:

```nginx

 location /multi {
     # body defined as string
     echo_subrequest_async POST '/sub' -q 'foo=Foo' -b 'hi';
     # body defined as path to a file, relative to nginx prefix path if not absolute
     echo_subrequest_async PUT '/sub' -q 'bar=Bar' -f '/tmp/hello.txt';
 }
 location /sub {
     echo "querystring: $query_string";
     echo "method: $echo_request_method";
     echo "body: $echo_request_body";
     echo "content length: $http_content_length";
     echo '///';
 }
```

Then on the client side:

```bash

   $ echo -n hello > /tmp/hello.txt
   $ curl 'http://localhost/multi'
   querystring: foo=Foo
   method: POST
   body: hi
   content length: 2
   ///
   querystring: bar=Bar
   method: PUT
   body: hello
   content length: 5
   ///
```

Here's more funny example using the standard [proxy module](#httpproxymodule) to handle the subrequest:

```nginx

 location /main {
     echo_subrequest_async POST /sub -b 'hello, world';
 }
 location /sub {
     proxy_pass $scheme://127.0.0.1:$server_port/proxied;
 }
 location /proxied {
     echo "method: $echo_request_method.";

     # we need to read body explicitly here...or $echo_request_body
     #   will evaluate to empty ("")
     echo_read_request_body;

     echo "body: $echo_request_body.";
 }
```

Then on the client side, we can see that

```bash

   $ curl 'http://localhost/main'
   method: POST.
   body: hello, world.
```

Nginx named locations like `@foo` is *not* supported here.

This directive takes several options:


    -q <url_args>        Specify the URL arguments (or URL querystring) for the subrequest.

    -f <path>            Specify the path for the file whose content will be serve as the
                         subrequest's request body.

    -b <data>            Specify the request body data


This directive was first introduced in the [release v0.15](#v015).

The `-f` option to define a file path for the body was introduced in the [release v0.35](#v035).

See also the [echo_subrequest](#echo_subrequest) and [echo_location_async](#echo_location_async) directives.


## echo_subrequest
**syntax:** *echo_subrequest &lt;HTTP_method&gt; &lt;location&gt; [-q &lt;url_args&gt;] [-b &lt;request_body&gt;] [-f &lt;request_body_path&gt;]*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

This is the synchronous version of the [echo_subrequest_async](#echo_subrequest_async) directive. And just like [echo_location](#echo_location), it does not block the Nginx worker process (while [echo_blocking_sleep](#echo_blocking_sleep) does), rather, it uses continuation to pass control along the subrequest chain.

See [echo_subrequest_async](#echo_subrequest_async) for more details.

Nginx named locations like `@foo` is *not* supported here.

This directive was first introduced in the [release v0.15](#v015).


## echo_foreach_split
**syntax:** *echo_foreach_split &lt;delimiter&gt; &lt;string&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Split the second argument `string` using the delimiter specified in the first argument, and then iterate through the resulting items. For instance:

```nginx

   location /loop {
     echo_foreach_split ',' $arg_list;
       echo "item: $echo_it";
     echo_end;
   }
```

Accessing /main yields

```bash

   $ curl 'http://localhost/loop?list=cat,dog,mouse'
   item: cat
   item: dog
   item: mouse
```

As seen in the previous example, this directive should always be accompanied by an [echo_end](#echo_end) directive.

Parallel `echo_foreach_split` loops are allowed, but nested ones are currently forbidden.

The `delimiter` argument could contain *multiple* arbitrary characters, like

```nginx

   # this outputs "cat\ndog\nmouse\n"
   echo_foreach_split -- '-a-' 'cat-a-dog-a-mouse';
     echo $echo_it;
   echo_end;
```

Logically speaking, this looping structure is just the `foreach` loop combined with a `split` function call in Perl (using the previous example):

```perl

    foreach (split ',', $arg_list) {
        print "item $_\n";
    }
```

People will also find it useful in merging multiple `.js` or `.css` resources into a whole. Here's an example:

```nginx

   location /merge {
       default_type 'text/javascript';

       echo_foreach_split '&' $query_string;
           echo "/* JS File $echo_it */";
           echo_location_async $echo_it;
           echo;
       echo_end;
   }
```

Then accessing /merge to merge the `.js` resources specified in the query string:

```bash

   $ curl 'http://localhost/merge?/foo/bar.js&/yui/blah.js&/baz.js'
```

One can also use third-party Nginx cache module to cache the merged response generated by the `/merge` location in the previous example.

This directive was first introduced in the [release v0.17](#v017).


## echo_end
**syntax:** *echo_end*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

This directive is used to terminate the body of looping and conditional control structures like [echo_foreach_split](#echo_foreach_split).

This directive was first introduced in the [release v0.17](#v017).


## echo_request_body
**syntax:** *echo_request_body*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Outputs the contents of the request body previous read.

Behind the scene, it's implemented roughly like this:

```C

   if (r->request_body && r->request_body->bufs) {
       return ngx_http_output_filter(r, r->request_body->bufs);
   }
```

Unlike the [$echo_request_body](#echo_request_body) and $request_body variables, this directive will show the whole request body even if some parts or all parts of it are saved in temporary files on the disk.

It is a "no-op" if no request body has been read yet.

This directive was first introduced in the [release v0.18](#v018).

See also [echo_read_request_body](#echo_read_request_body) and the [chunkin module](http://github.com/agentzh/chunkin-nginx-module).


## echo_exec
**syntax:** *echo_exec &lt;location&gt; [&lt;query_string&gt;]*

**syntax:** *echo_exec &lt;named_location&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Does an internal redirect to the location specified. An optional query string can be specified for normal locations, as in

```nginx

   location /foo {
       echo_exec /bar weight=5;
   }
   location /bar {
       echo $arg_weight;
   }
```

Or equivalently

```nginx

   location /foo {
       echo_exec /bar?weight=5;
   }
   location /bar {
       echo $arg_weight;
   }
```

Named locations are also supported. Here's an example:

```nginx

   location /foo {
       echo_exec @bar;
   }
   location @bar {
       # you'll get /foo rather than @bar
       #  due to a potential bug in nginx.
       echo $echo_request_uri;
   }
```

But query string (if any) will always be ignored for named location redirects due to a limitation in the `ngx_http_named_location` function.

Never try to echo things before the `echo_exec` directive or you won't see the proper response of the location you want to redirect to. Because any echoing will cause the original location handler to send HTTP headers before the redirection happens.

Technically speaking, this directive exposes the Nginx internal API functions `ngx_http_internal_redirect` and `ngx_http_named_location`.

This directive was first introduced in the [v0.21 release](#v021).


## echo_status
**syntax:** *echo_status &lt;status-num&gt;*

**default:** *echo_status 200*

**context:** *location, location if*

**phase:** *content*

Specify the default response status code. Default to `200`. This directive is declarative and the relative order with other echo-like directives is not important.

Here is an example,

```nginx

 location = /bad {
     echo_status 404;
     echo "Something is missing...";
 }
```

then we get a response like this:


    HTTP/1.1 404 Not Found
    Server: nginx/1.2.1
    Date: Sun, 24 Jun 2012 03:58:18 GMT
    Content-Type: text/plain
    Transfer-Encoding: chunked
    Connection: keep-alive

    Something is missing...


This directive was first introduced in the `v0.40` release.


## Filter Directives

Use of the following directives trigger the filter registration of this module. By default, no filter will be registered by this module.

Every filter directive supports variable interpolation in its arguments (if any).


## echo_before_body
**syntax:** *echo_before_body \[options\] \[argument\]...*

**default:** *no*

**context:** *location, location if*

**phase:** *output filter*

It's the filter version of the [echo](#echo) directive, and prepends its output to the beginning of the original outputs generated by the underlying content handler.

An example is

```nginx

 location /echo {
     echo_before_body hello;
     proxy_pass $scheme://127.0.0.1:$server_port$request_uri/more;
 }
 location /echo/more {
     echo world
 }
```

Accessing `/echo` from the client side yields

```bash

   hello
   world
```

In the previous sample, we borrow the [standard proxy module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html) to serve as the underlying content handler that generates the "main contents".

Multiple instances of this filter directive are also allowed, as in:

```nginx

 location /echo {
     echo_before_body hello;
     echo_before_body world;
     echo !;
 }
```

On the client side, the output is like

```bash

   $ curl 'http://localhost/echo'
   hello
   world
   !
```

In this example, we also use the [content handler directives](#content-handler-directives) provided by this module as the underlying content handler.

This directive also supports the `-n` and `--` options like the [echo](#echo) directive.

This directive can be mixed with its brother directive [echo_after_body](#echo_after_body).


## echo_after_body
**syntax:** *echo_after_body \[argument\]...*

**default:** *no*

**context:** *location, location if*

**phase:** *output filter*

It's very much like the [echo_before_body](#echo_before_body) directive, but *appends* its output to the end of the original outputs generated by the underlying content handler.

Here's a simple example:

```nginx

 location /echo {
     echo_after_body hello;
     proxy_pass http://127.0.0.1:$server_port$request_uri/more;
 }
 location /echo/more {
     echo world
 }
```

Accessing `/echo` from the client side yields


      world
      hello


Multiple instances are allowed, as in:

```nginx

 location /echo {
     echo_after_body hello;
     echo_after_body world;
     echo i;
     echo say;
 }
```

The output on the client side while accessing the `/echo` location looks like


      i
      say
      hello
      world


This directive also supports the `-n` and `--` options like the [echo](#echo) directive.

This directive can be mixed with its brother directive [echo_before_body](#echo_before_body).


## Variables


## $echo_it

This is a "topic variable" used by [echo_foreach_split](#echo_foreach_split), just like the `$_` variable in Perl.


## $echo_timer_elapsed

This variable holds the seconds elapsed since the start of the current request (might be a subrequest though) or the last invocation of the [echo_reset_timer](#echo_reset_timer) command.

The timing result takes three digits after the decimal point.

References of this variable will force the underlying Nginx timer to update to the current system time, regardless the timer resolution settings elsewhere in the config file, just like the [echo_reset_timer](#echo_reset_timer) directive.


## $echo_request_body

Evaluates to the current (sub)request's request body previously read if no part of the body has been saved to a temporary file. To always show the request body even if it's very large, use the [echo_request_body](#echo_request_body) directive.


## $echo_request_method

Evaluates to the HTTP request method of the current request (it can be a subrequest).

Behind the scene, it just takes the string data stored in `r->method_name`.

Compare it to the [$echo_client_request_method](#echo_client_request_method) variable.

At least for Nginx 0.8.20 and older, the [$request_method](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_method) variable provided by the [http core module](http://nginx.org/en/docs/http/ngx_http_core_module.html) is actually doing what our [$echo_client_request_method](#echo_client_request_method) is doing.

This variable was first introduced in our [v0.15 release](#v015).


## $echo_client_request_method

Always evaluates to the main request's HTTP method even if the current request is a subrequest.

Behind the scene, it just takes the string data stored in `r->main->method_name`.

Compare it to the [$echo_request_method](#echo_request_method) variable.

This variable was first introduced in our [v0.15 release](#v015).


## $echo_client_request_headers

Evaluates to the original client request's headers.

Just as the name suggests, it will always take the main request (or the client request) even if it's currently executed in a subrequest.

A simple example is below:

```nginx

   location /echoback {
      echo "headers are:"
      echo $echo_client_request_headers;
   }
```

Accessing `/echoback` yields

```bash

   $ curl 'http://localhost/echoback'
   headers are
   GET /echoback HTTP/1.1
   User-Agent: curl/7.18.2 (i486-pc-linux-gnu) libcurl/7.18.2 OpenSSL/0.9.8g
   Host: localhost:1984
   Accept: */*
```

Behind the scene, it recovers `r->main->header_in` (or the large header buffers, if any) on the C level and does not construct the headers itself by traversing parsed results in the request object.

This varible is always evaluated to an empty value in HTTP/2 requests for now due to the current implementation.

This variable was first introduced in [version 0.15](#v015).


## $echo_cacheable_request_uri

Evaluates to the parsed form of the URI (usually led by `/`) of the current (sub-)request. Unlike the [$echo_request_uri](#echo_request_uri) variable, it is cacheable.

See [$echo_request_uri](#echo_request_uri) for more details.

This variable was first introduced in [version 0.17](#v017).


## $echo_request_uri

Evaluates to the parsed form of the URI (usually led by `/`) of the current (sub-)request. Unlike the [$echo_cacheable_request_uri](#echo_cacheable_request_uri) variable, it is *not* cacheable.

This is quite different from the [$request_uri](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_uri) variable exported by the [ngx_http_core_module](http://nginx.org/en/docs/http/ngx_http_core_module.html), because `$request_uri` is the *unparsed* form of the current request's URI.

This variable was first introduced in [version 0.17](#v017).


## $echo_incr

It is a counter that always generate the current counting number, starting from 1. The counter is always associated with the main request even if it is accessed within a subrequest.

Consider the following example

```Nginx

 location /main {
     echo "main pre: $echo_incr";
     echo_location_async /sub;
     echo_location_async /sub;
     echo "main post: $echo_incr";
 }
 location /sub {
     echo "sub: $echo_incr";
 }
```

Accessing `/main` yields

    main pre: 1
    sub: 3
    sub: 4
    main post: 2

This directive was first introduced in the [v0.18 release](#v018).


## $echo_response_status

Evaluates to the status code of the current (sub)request, null if not any.

Behind the scene, it's just the textual representation of `r->headers_out->status`.

This directive was first introduced in the [v0.23 release](#v023).


## Modules that use this module for testing

The following modules take advantage of this `echo` module in their test suite:

* The [memc](http://github.com/openresty/memc-nginx-module) module that supports almost the whole memcached TCP protocol.
* The [chunkin](http://github.com/agentzh/chunkin-nginx-module) module that adds HTTP 1.1 chunked input support to Nginx.
* The [headers_more](http://github.com/openresty/headers-more-nginx-module) module that allows you to add, set, and clear input and output headers under the conditions that you specify.
* The `echo` module itself.

Please mail me other modules that use `echo` in any form and I'll add them to the list above :)


## Changes

The changes of every release of this module can be obtained from the OpenResty bundle's change logs:

<http://openresty.org/#Changes>


## Test Suite

This module comes with a Perl-driven test suite. The [test cases](https://github.com/openresty/echo-nginx-module/tree/master/t/) are
[declarative](https://github.com/openresty/echo-nginx-module/blob/master/t/echo.t) too. Thanks to the [Test::Nginx](http://search.cpan.org/perldoc?Test::Nginx) module in the Perl world.

To run it on your side:

```bash

 $ PATH=/path/to/your/nginx-with-echo-module:$PATH prove -r t
```

You need to terminate any Nginx processes before running the test suite if you have changed the Nginx server binary.

Because a single nginx server (by default, `localhost:1984`) is used across all the test scripts (`.t` files), it's meaningless to run the test suite in parallel by specifying `-jN` when invoking the `prove` utility.

Some parts of the test suite requires standard modules [proxy](http://nginx.org/en/docs/http/ngx_http_proxy_module.html), [rewrite](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html) and [SSI](http://nginx.org/en/docs/http/ngx_http_ssi_module.html) to be enabled as well when building Nginx.


## See Also

* The original [blog post](http://agentzh.blogspot.com/2009/10/hacking-on-nginx-echo-module.html) about this module's initial development.
* The standard [addition filter module](http://nginx.org/en/docs/http/ngx_http_addition_module.html).
* The standard [proxy module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html).
* The [OpenResty](http://openresty.org) bundle.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-echo](https://github.com/openresty/echo-nginx-module){target=_blank}.

# *encrypted-session*: Encrypt and decrypt NGINX variable values


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-encrypted-session
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_encrypted_session_module.so;
```


This document describes nginx-module-encrypted-session [v0.9](https://github.com/openresty/encrypted-session-nginx-module/releases/tag/v0.09){target=_blank} 
released on Nov 18 2021.

<hr />

encrypted-session-nginx-module - encrypt and decrypt nginx variable values

installation instructions.

## Status

This module is production ready.

## Synopsis

```nginx
## key must be of 32 bytes long
encrypted_session_key "abcdefghijklmnopqrstuvwxyz123456";

## iv must not be longer than 16 bytes
## default: "deadbeefdeadbeef" (w/o quotes)
encrypted_session_iv "1234567812345678";

## default: 1d (1 day)
encrypted_session_expires 3600; # in sec

location /encrypt {
    set $raw 'text to encrypted'; # from the ngx_rewrite module
    set_encrypt_session $session $raw;
    set_encode_base32 $session; # from the ngx_set_misc module

    add_header Set-Cookie 'my_login=$session';  # from the ngx_headers module

    # your content handler goes here...
}

location /decrypt {
    set_decode_base32 $session $cookie_my_login; # from the ngx_set_misc module
    set_decrypt_session $raw $session;

    if ($raw = '') {
        # bad session
    }

    # your content handler goes here...
}
```

## Description

This module provides encryption and decryption support for
nginx variables based on AES-256 with Mac.

This module is usually used with the [ngx_set_misc module](http://github.com/agentzh/set-misc-nginx-module)
and the standard rewrite module's directives.

This module can be used to implement simple user login and ACL.

Usually, you just decrypt data in nginx level, and pass the unencrypted
data to your FastCGI/HTTP backend, as in

```nginx
location /blah {
    set_decrypt_session $raw_text $encrypted;

    # this directive is from the ngx_set_misc module
    set_escape_uri $escaped_raw_text $raw_text;

    fastcgi_param QUERY_STRING "uid=$uid";
    fastcgi_pass unix:/path/to/my/php/or/python/fastcgi.sock;
}
```

Lua web applications running directly on [ngx_lua](https://github.com/openresty/lua-nginx-module) can call
this module's directives directly from within Lua code:

```lua
local raw_text = ndk.set_var.set_decrypt_session(encrypted_text)
```


## Directives


## encrypted_session_key
**syntax:** *encrypted_session_key &lt;key&gt;*

**default:** *no*

**context:** *http, server, server if, location, location if*

Sets the key for the cipher (must be 32 bytes long). For example,

```nginx
encrypted_session_key "abcdefghijklmnopqrstuvwxyz123456";
```


## encrypted_session_iv
**syntax:** *encrypted_session_iv &lt;iv&gt;*

**default:** *encrypted_session_iv "deadbeefdeadbeef";*

**context:** *http, server, server if, location, location if*

Sets the initial vector used for the cipher (must be *no longer* than 16 bytes).

For example,

```nginx
encrypted_session_iv "12345678";
```


## encrypted_session_expires
**syntax:** *encrypted_session_expires &lt;time&gt;*

**default:** *encrypted_session_expires 1d;*

**context:** *http, server, server if, location, location if*

Sets expiration time difference (in seconds by default).

For example, consider the following configuration:

```nginx
encypted_session_expires 1d;
```

When your session is being generated, ngx_encrypted_session will plant
an expiration time (1 day in the future in this example) into the
encrypted session string, such that when the session is being decrypted
later, the server can pull the expiration time out of the session and
compare it with the server's current system time. No matter how you
transfer and store your session, like using cookies, or URI query arguments,
or whatever.

People may confuse this setting with the expiration date of HTTP
cookies. This directive simply controls when the session gets expired;
it knows nothing about HTTP cookies. Even if the end user intercepted
this session from cookie by himself and uses it later manually, the
server will still reject it when the expiration time gets passed.


## set_encrypt_session
**syntax:** *set_encrypt_session $target &lt;value&gt;*

**default:** *no*

**context:** *http, server, server if, location, location if*

Encrypts the string value specified by the `value` argument and saves the result into
the variable specified by `$target`.

For example,

```nginx
set_encrypt_session $res $value;
```

will encrypts the value in the variable $value into the target variable `$res`.

The `value` argument can also be an nginx string value, for example,

```nginx
set_encrypt_session $res "my value = $value";
```

The resulting data can later be decrypted via the [set_decrypt_session](#set_decrypt_session) directive.


## set_decrypt_session
**syntax:** *set_decrypt_session $target &lt;value&gt;*

**default:** *no*

**context:** *http, server, server if, location, location if*

Similar to [set_encrypt_session](#set_encrypt_session), but performs the inverse operation, that is,
to decrypt things.


## See Also
* [NDK](http://github.com/simpl-it/ngx_devel_kit)
* [ngx_set_misc module](http://github.com/agentzh/set-misc-nginx-module)


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-encrypted-session](https://github.com/openresty/encrypted-session-nginx-module){target=_blank}.

# *execute*: NGINX Execute module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-execute
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_execute_module.so;
```


This document describes nginx-module-execute [v1.6.1](https://github.com/limithit/NginxExecute/releases/tag/1.6.1){target=_blank} 
released on May 21 2018.

<hr />

## Introduction

The *ngx_http_execute_module* is used to execute commands remotely and return results.

Configuration example：


    worker_processes  1;
    events {
        worker_connections  1024;
    }
    http {
        include       mime.types;
        default_type  application/octet-stream;
        sendfile        on;
        keepalive_timeout  65;
        server {
            listen       80;
            server_name  localhost;
            location / {
                root   html;
                index  index.html index.htm;
                command on;
            }
            error_page   500 502 503 504  /50x.html;
            location = /50x.html {
                root   html;
            }
        }
    }

Usage:  ```view-source:http://192.168.18.22/?system.run[command]```
The ```command``` can be any system command. The command you will want to use depends on the permissions that nginx runs with.

    view-source:http://192.168.18.22/?system.run[ifconfig]

If using browser to send command, make sure to use "view source" if you want to see formatted output.
Alternatively, you can also use some tools such as Postman, Fiddler.

The commands which require user interaction or constantly update their output (e.g. ```top```) will not run properly, so do not file a bug for this.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-execute](https://github.com/limithit/NginxExecute){target=_blank}.

# *f4fhds*: NGINX module for Adobe f4f format


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-f4fhds
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_f4fhds_module.so;
```


This document describes nginx-module-f4fhds [v0.0.1](https://github.com/GetPageSpeed/f4fhds/releases/tag/v0.0.1){target=_blank} 
released on Oct 24 2020.

<hr />

Nginx module for Adobe f4f format.

This module implements handling of HTTP Dynamic Streaming requests in the “/videoSeg1-Frag1” form — extracting the 
needed fragment from the videoSeg1.f4f file using the videoSeg1.f4x index file. This module is an alternative to the 
Adobe’s f4f module (HTTP Origin Module) for Apache.

It is open-source equivalent for commercial [ngx_http_f4f_module](http://nginx.org/en/docs/http/ngx_http_f4f_module.html#f4f_buffer_size)
module.

## Synopsis

```nginx
location /video/ {
    f4fhds;
    ...
}
```

## Limitations

* The assumption is that all files contain a single (first) segment, e.g. Seg1
* The files should reside in a local non-networked filesystem, due to use of `mmap(2)`.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-f4fhds](https://github.com/GetPageSpeed/f4fhds){target=_blank}.

# *fancyindex*: NGINX Fancy Index module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-fancyindex
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_fancyindex_module.so;
```


This document describes nginx-module-fancyindex [v0.5.2](https://github.com/aperezdc/ngx-fancyindex/releases/tag/v0.5.2){target=_blank} 
released on Oct 28 2021.

<hr />


The Fancy Index module makes possible the generation of file listings,
like the built-in
[autoindex](http://wiki.nginx.org/NginxHttpAutoindexModule) module does,
but adding a touch of style. This is possible because the module allows
a certain degree of customization of the generated content:

-   Custom headers. Either local or stored remotely.
-   Custom footers. Either local or stored remotely.
-   Add you own CSS style rules.
-   Allow choosing to sort elements by name (default), modification
    time, or size; both ascending (default), or descending.

This module is designed to work with [Nginx](https://nginx.org), a high
performance open source web server written by [Igor
Sysoev](http://sysoev.ru).

## Example

You can test the default built-in style by adding the following lines
into a `server` section in your [Nginx](https://nginx.org) configuration
file:

    location / {
      fancyindex on;              # Enable fancy indexes.
      fancyindex_exact_size off;  # Output human-readable file sizes.
    }

### Themes

The following themes demonstrate the level of customization which can be
achieved using the module:

-   [Theme](https://github.com/TheInsomniac/Nginx-Fancyindex-Theme) by
    [@TheInsomniac](https://github.com/TheInsomniac). Uses custom header
    and footer.
-   [Theme](https://github.com/Naereen/Nginx-Fancyindex-Theme) by
    [@Naereen](https://github.com/Naereen/). Uses custom header and
    footer, the header includes search field to filter by filename using
    JavaScript.
-   [Theme](https://github.com/fraoustin/Nginx-Fancyindex-Theme) by
    [@fraoustin](https://github.com/fraoustin). Responsive theme using
    Material Design elements.
-   [Theme](https://github.com/alehaa/nginx-fancyindex-flat-theme) by
    [@alehaa](https://github.com/alehaa). Simple, flat theme based on
    Bootstrap 4 and FontAwesome.

## Directives

### fancyindex

Syntax  
*fancyindex* \[*on* \| *off*\]

Default  
fancyindex off

Context  
http, server, location

Description  
Enables or disables fancy directory indexes.

### fancyindex_default_sort

Syntax  
*fancyindex_default_sort* \[*name* \| *size* \| *date* \| *name_desc* \|
*size_desc* \| *date_desc*\]

Default  
fancyindex_default_sort name

Context  
http, server, location

Description  
Defines sorting criterion by default.

### fancyindex_directories_first

Syntax  
*fancyindex_directories_first* \[*on* \| *off*\]

Default  
fancyindex_directories_first on

Context  
http, server, location

Description  
If enabled (default setting), groups directories together and sorts them
before all regular files. If disabled, directories are sorted together
with files.

### fancyindex_css_href

Syntax  
*fancyindex_css_href uri*

Default  
fancyindex_css_href ""

Context  
http, server, location

Description  
Allows inserting a link to a CSS style sheet in generated listings. The
provided *uri* parameter will be inserted as-is in a `<link>` HTML tag.
The link is inserted after the built-in CSS rules, so you can override
the default styles.

### fancyindex_exact_size

Syntax  
*fancyindex_exact_size* \[*on* \| *off*\]

Default  
fancyindex_exact_size on

Context  
http, server, location

Description  
Defines how to represent file sizes in the directory listing; either
accurately, or rounding off to the kilobyte, the megabyte and the
gigabyte.

### fancyindex_name_length

Syntax  
*fancyindex_name_length length*

Default  
fancyindex_name_length 50

Context  
http, server, location

Description  
Defines the maximum file name length limit in bytes.

### fancyindex_footer

Syntax  
*fancyindex_footer path* \[*subrequest* \| *local*\]

Default  
fancyindex_footer ""

Context  
http, server, location

Description  
Specifies which file should be inserted at the foot of directory
listings. If set to an empty string, the default footer supplied by the
module will be sent. The optional parameter indicates whether the *path*
is to be treated as an URI to load using a *subrequest* (the default),
or whether it refers to a *local* file.

Note

Using this directive needs the [ngx_http_addition_module]() built into
Nginx.

Warning

When inserting custom header/footer a subrequest will be issued so
potentially any URL can be used as source for them. Although it will
work with external URLs, only using internal ones is supported. External
URLs are totally untested and using them will make
[Nginx](https://nginx.org) block while waiting for the subrequest to
complete. If you feel like external header/footer is a must-have for
you, please [let me know](mailto:aperez@igalia.com).

### fancyindex_header

Syntax  
*fancyindex_header path* \[*subrequest* \| *local*\]

Default  
fancyindex_header ""

Context  
http, server, location

Description  
Specifies which file should be inserted at the head of directory
listings. If set to an empty string, the default header supplied by the
module will be sent. The optional parameter indicates whether the *path*
is to be treated as an URI to load using a *subrequest* (the default),
or whether it refers to a *local* file.

Note

Using this directive needs the [ngx_http_addition_module]() built into
Nginx.

### fancyindex_show_path

Syntax  
*fancyindex_show_path* \[*on* \| *off*\]

Default  
fancyindex_show_path on

Context  
http, server, location

Description  
Whether to output or not the path and the closing \</h1\> tag after the
header. This is useful when you want to handle the path displaying with
a PHP script for example.

Warning

This directive can be turned off only if a custom header is provided
using fancyindex_header.

fancyindex_show_dotfiles \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~~
:Syntax: *fancyindex_show_dotfiles* \[*on* \| *off*\] :Default:
fancyindex_show_dotfiles off :Context: http, server, location
:Description: Whether to list files that are proceeded with a dot.
Normal convention is to hide these.

### fancyindex_ignore

Syntax  
*fancyindex_ignore string1 \[string2 \[... stringN\]\]*

Default  
No default.

Context  
http, server, location

Description  
Specifies a list of file names which will be not be shown in generated
listings. If Nginx was built with PCRE support strings are interpreted
as regular expressions.

### fancyindex_hide_symlinks

Syntax  
*fancyindex_hide_symlinks* \[*on* \| *off*\]

Default  
fancyindex_hide_symlinks off

Context  
http, server, location

Description  
When enabled, generated listings will not contain symbolic links.

fancyindex_hide_parent_dir
\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~~ :Syntax:
*fancyindex_hide_parent_dir* \[*on* \| *off*\] :Default:
fancyindex_hide_parent_dir off :Context: http, server, location
:Description: When enabled, it will not show parent directory.

### fancyindex_localtime

Syntax  
*fancyindex_localtime* \[*on* \| *off*\]

Default  
fancyindex_localtime off

Context  
http, server, location

Description  
Enables showing file times as local time. Default is “off” (GMT time).

### fancyindex_time_format

Syntax  
*fancyindex_time_format* string

Default  
fancyindex_time_format "%Y-%b-%d %H:%M"

Context  
http, server, location

Description  
Format string used for timestamps. The format specifiers are a subset of
those supported by the [strftime](https://linux.die.net/man/3/strftime)
function, and the behavior is locale-independent (for example, day and
month names are always in English). The supported formats are:

-   `%a`: Abbreviated name of the day of the week.
-   `%A`: Full name of the day of the week.
-   `%b`: Abbreviated month name.
-   `%B`: Full month name.
-   `%d`: Day of the month as a decimal number (range 01 to 31).
-   `%e`: Like `%d`, the day of the month as a decimal number, but a
    leading zero is replaced by a space.
-   `%F`: Equivalent to `%Y-%m-%d` (the ISO 8601 date format).
-   `%H`: Hour as a decimal number using a 24-hour clock (range 00 to
    23).
-   `%I`: Hour as a decimal number using a 12-hour clock (range 01 to
    12).
-   `%k`: Hour (24-hour clock) as a decimal number (range 0 to 23);
    single digits are preceded by a blank.
-   `%l`: Hour (12-hour clock) as a decimal number (range 1 to 12);
    single digits are preceded by a blank.
-   `%m`: Month as a decimal number (range 01 to 12).
-   `%M`: Minute as a decimal number (range 00 to 59).
-   `%p`: Either "AM" or "PM" according to the given time value.
-   `%P`: Like `%p` but in lowercase: "am" or "pm".
-   `%r`: Time in a.m. or p.m. notation. Equivalent to `%I:%M:%S %p`.
-   `%R`: Time in 24-hour notation (`%H:%M`).
-   `%S`: Second as a decimal number (range 00 to 60).
-   `%T`: Time in 24-hour notation (`%H:%M:%S`).
-   `%u`: Day of the week as a decimal, range 1 to 7, Monday being 1.
-   `%w`: Day of the week as a decimal, range 0 to 6, Monday being 0.
-   `%y`: Year as a decimal number without a century (range 00 to 99).
-   `%Y`: Year as a decimal number including the century.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-fancyindex](https://github.com/aperezdc/ngx-fancyindex){target=_blank}.

# *fips-check*: FIPS status check module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 6, 7, 8, 9
* CentOS 6, 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-fips-check
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_fips_check_module.so;
```


This document describes nginx-module-fips-check [v0.1](https://github.com/ogarrett/nginx-fips-check-module/releases/tag/v0.1){target=_blank} 
released on Jan 11 2021.

<hr />

## Introduction

This module applies to NGINX builds that use OpenSSL for SSL/TLS crypto.  It runs after 
NGINX startup and queries the OpenSSL library, reporting if the library is in FIPS mode or not.

```sh
sudo tail /var/log/nginx/error.log
2020/04/03 07:45:54 [notice] 11250#11250: using the "epoll" event method
2020/04/03 07:45:54 [notice] 11250#11250: OpenSSL FIPS Mode is enabled
2020/04/03 07:45:54 [notice] 11250#11250: nginx/1.17.6 (nginx-plus-r20)
2020/04/03 07:45:54 [notice] 11250#11250: built by gcc 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC)
2020/04/03 07:45:54 [notice] 11250#11250: OS: Linux 3.10.0-1062.el7.x86_64
```

For more information on using NGINX in FIPS mode, see the [NGINX Plus FIPS documentation], which applies to both NGINX open source builds and NGINX Plus. To determine which TLS ciphers NGINX offers, the [nmap ssl-enum-ciphers] script is useful.

  [NGINX Plus FIPS documentation]:https://docs.nginx.com/nginx/fips-compliance-nginx-plus/
  [nmap ssl-enum-ciphers]:https://nmap.org/nsedoc/scripts/ssl-enum-ciphers.html

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-fips-check](https://github.com/ogarrett/nginx-fips-check-module){target=_blank}.

# *flv*: Media streaming server based on nginx-module-rtmp


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-flv
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_flv_live_module.so;
```


This document describes nginx-module-flv [v1.2.11](https://github.com/winshining/nginx-http-flv-module/releases/tag/v1.2.11){target=_blank} 
released on Mar 19 2023.

<hr />

![nginx-http-flv-module workflow](https://github.com/winshining/nginx-http-flv-module/actions/workflows/nginx-http-flv-module.yml/badge.svg?branch=master)

A media streaming server based on [nginx-rtmp-module](https://github.com/arut/nginx-rtmp-module).

[中文说明](https://github.com/winshining/nginx-http-flv-module/blob/master/README.CN.md).

Donate if you like this module. Many thanks to you!

<a href="https://www.paypal.me/ShingWong" target="_blank"><img src="https://www.paypalobjects.com/digitalassets/c/website/marketing/apac/C2/logos-buttons/optimize/44_Grey_PayPal_Pill_Button.png" alt="PayPal" /></a>

## Features

* All features [nginx-rtmp-module](https://github.com/arut/nginx-rtmp-module) provides.

* Other features provided by nginx-http-flv-module vs [nginx-rtmp-module](https://github.com/arut/nginx-rtmp-module):

|         Features                   | nginx-http-flv-module | nginx-rtmp-module |                    Remarks                     |
| :--------------------------------: | :-------------------: | :---------------: | :--------------------------------------------: |
|        HTTP-FLV (for play)         |           √           |         x         |    HTTPS-FLV and chunked response supported    |
|             GOP cache              |           √           |         x         |                                                |
|            Virtual Host            |           √           |         x         |                                                |
|      Omit `listen` directive       |           √           |    See remarks    | There MUST be at least one `listen` directive  |
|Audio-only support for RTMP/HTTP-FLV|           √           |    See remarks    | Won't work if `wait_video` or `wait_key` is on |
|    Single-track support for HLS    |           √           |         x         |                                                |
|        `reuseport` support         |           √           |         x         |                                                |
|        Timer for access log        |           √           |         x         |                                                |
|       JSON style statistics        |           √           |         x         |                                                |
|     Statistics for recordings      |           √           |         x         |                                                |
|     Independent of endianness      |           √           |    See remarks    |   Partially supported in branch `big-endian`   |

## Systems supported

* Linux (recommended) / FreeBSD / MacOS / Windows (limited).

## Players supported

* [VLC](http://www.videolan.org) (RTMP & HTTP-FLV) / [OBS](https://obsproject.com) (RTMP & HTTP-FLV) / [JW Player](https://www.jwplayer.com) (RTMP) / [flv.js](https://github.com/Bilibili/flv.js) (HTTP-FLV).

### Note

* [Flash player](https://www.adobe.com/products/flashplayer.html) will be no longer supported officially by Adobe after December 31, 2020, refer to [Adobe Flash Player EOL General Information Page](https://www.adobe.com/products/flashplayer/end-of-life.html) for details. Plugins that use flash player won't work after the major browsers subsequently remove flash player.

* [flv.js](https://github.com/Bilibili/flv.js) can only run with browsers that support [Media Source Extensions](https://www.w3.org/TR/media-source).

## Prerequisites

* GNU make for activating compiler on Unix-like systems to compile software.

* GCC for compilation on Unix-like systems or MSVC for compilation on Windows.

* GDB for debug on Unix-like systems.

* [FFmpeg](http://ffmpeg.org) or [OBS](https://obsproject.com) for publishing media streams.

* [VLC](http://www.videolan.org) (recommended) or [flv.js](https://github.com/Bilibili/flv.js) (recommended) for playing media streams.

* [PCRE](http://www.pcre.org) for NGINX if regular expressions needed.

* [OpenSSL](https://www.openssl.org) for NGINX if encrypted access needed.

* [zlib](http://www.zlib.net) for NGINX if compression needed.

## Usage

For details of usages of [nginx-rtmp-module](https://github.com/arut/nginx-rtmp-module), please refer to [README.md](https://github.com/arut/nginx-rtmp-module/blob/master/README.md).

### Publish

For simplicity, transcoding is not used (so **-c copy** is used):

    ffmpeg -re -i MEDIA_FILE_NAME -c copy -f flv rtmp://example.com[:port]/appname/streamname

#### Note

Some legacy versions of [FFmpeg](http://ffmpeg.org) don't support the option `-c copy`, the options `-vcodec copy -acodec copy` can be used instead.

The `appname` is used to match an application block in rtmp block (see below for details).

The `streamname` can be specified at will but can **NOT** be omitted.

The **default port for RTMP** is **1935**, if some other ports were used, `:port` must be specified.

### Play

#### via HTTP-FLV

    http://example.com[:port]/dir?[port=xxx&]app=appname&stream=streamname

#### Note

* If [ffplay](http://www.ffmpeg.org/ffplay.html) is used in command line to play the stream, the url above **MUST** be enclosed by quotation marks, or arguments in url will be discarded (some shells not so smart will interpret "&" as "run in background").

* If [flv.js](https://github.com/Bilibili/flv.js) is used to play the stream, make sure that the published stream is encoded properly, for [flv.js](https://github.com/Bilibili/flv.js) supports **ONLY H.264 encoded video and AAC/MP3 encoded audio**.

The `dir` is used to match location blocks in http block (see below for details).

The **default port for HTTP** is **80**, if some other ports were used, `:port` must be specified.

The **default port for RTMP** is **1935**, if some other ports were used, `port=xxx` must be specified.

The value of `app` (appname) is used to match an application block, but if the requested `app` appears in several server blocks and those blocks have the same address and port configuration, host name matches `server_name` directive will be additionally used to identify the requested application block, otherwise the first one is matched.

The value of `stream` (streamname) is used to match the name of published stream.

#### Example

Assume that `listen` directive specified in `http` block is:

    http {
        ...
        server {
            listen 8080; #not default port 80
            ...

            location /live {
                flv_live on;
            }
        }
    }

And `listen` directive specified in `rtmp` block is:

    rtmp {
        ...
        server {
            listen 1985; #not default port 1935
            ...

            application myapp {
                live on;
            }
        }
    }

And the name of published stream is `mystream`, then the url of playback based on HTTP is:

    http://example.com:8080/live?port=1985&app=myapp&stream=mystream

#### Note

Since some players don't support HTTP chunked transmission, it's better to specify `chunked_transfer_encoding off;` in location where `flv_live on;` is specified in this case, or play will fail.

#### via RTMP

    rtmp://example.com[:port]/appname/streamname

#### via HLS

    http://example.com[:port]/dir/streamname.m3u8

#### via DASH

    http://example.com[:port]/dir/streamname.mpd

## Sample Pictures

### RTMP ([JW Player](https://www.jwplayer.com)) & HTTP-FLV ([VLC](http://www.videolan.org))

![RTMP & HTTP-FLV](samples/jwplayer_vlc.png)

### HTTP-FLV ([flv.js](https://github.com/Bilibili/flv.js))

![HTTP-FLV](samples/flv.js.png)

## Example nginx.conf

### Note

The directives `rtmp_auto_push`, `rtmp_auto_push_reconnect` and `rtmp_socket_dir` will not function on Windows except on Windows 10 17063 and later versions, because `relay` in multiple processes mode needs help of Unix domain socket, please refer to [Unix domain socket on Windows 10](https://blogs.msdn.microsoft.com/commandline/2017/12/19/af_unix-comes-to-windows) for details.

It's better to specify the directive `worker_processes` as 1, because `ngx_rtmp_stat_module` may not get statistics from a specified worker process in multi-processes mode, for HTTP requests are randomly distributed to worker processes. `ngx_rtmp_control_module` has the same problem. The problem can be optimized by this patch [per-worker-listener](https://github.com/arut/nginx-patches/blob/master/per-worker-listener).

In addtion, `vhost` feature is OK in single process mode but not perfect in multi-processes mode yet, waiting to be fixed. For example, the following configuration is OK in multi-processes mode:

    rtmp {
        ...
        server {
            listen 1935;
            server_name domain_name;

            application myapp {
                ...
            }
        }
    }

While the following configuration doesn't work properly for play requests distinated to the second `server` (whether port is 1935 or not) of non-publisher worker processes:

    rtmp {
        ...
        server {
            listen 1935;
            server_name 1st_domain_name;

            application myapp {
                ...
            }
        }

        server {
            listen 1945;
            server_name 2nd_domain_name;

            application myapp {
                ...
            }
        }
    }

If [NGINX](http://nginx.org) is running in muti-processes mode and socket option `SO_REUSEPORT` is supported by platform, adding option `reuseport` for the directive `listen` will resolve the thundering herd problem.

    rtmp {
        ...

        server {
            listen 1935 reuseport;
            ...
        }
    }

### Example configuration

    worker_processes  1; #should be 1 for Windows, for it doesn't support Unix domain socket
    #worker_processes  auto; #from versions 1.3.8 and 1.2.5

    #worker_cpu_affinity  0001 0010 0100 1000; #only available on FreeBSD and Linux
    #worker_cpu_affinity  auto; #from version 1.9.10

    error_log logs/error.log error;

    #if the module is compiled as a dynamic module and features relevant
    #to RTMP are needed, the command below MUST be specified and MUST be
    #located before events directive, otherwise the module won't be loaded
    #or will be loaded unsuccessfully when NGINX is started

    #load_module modules/ngx_http_flv_live_module.so;

    events {
        worker_connections  4096;
    }

    http {
        include       mime.types;
        default_type  application/octet-stream;

        keepalive_timeout  65;

        server {
            listen       80;

            location / {
                root   /var/www;
                index  index.html index.htm;
            }

            error_page   500 502 503 504  /50x.html;
            location = /50x.html {
                root   html;
            }

            location /live {
                flv_live on; #open flv live streaming (subscribe)
                chunked_transfer_encoding  on; #open 'Transfer-Encoding: chunked' response

                add_header 'Access-Control-Allow-Origin' '*'; #add additional HTTP header
                add_header 'Access-Control-Allow-Credentials' 'true'; #add additional HTTP header
            }

            location /hls {
                types {
                    application/vnd.apple.mpegurl m3u8;
                    video/mp2t ts;
                }

                root /tmp;
                add_header 'Cache-Control' 'no-cache';
            }

            location /dash {
                root /tmp;
                add_header 'Cache-Control' 'no-cache';
            }

            location /stat {
                #configuration of streaming & recording statistics

                rtmp_stat all;
                rtmp_stat_stylesheet stat.xsl;
            }

            location /stat.xsl {
                root /var/www/rtmp; #specify in where stat.xsl located
            }

            #if JSON style stat needed, no need to specify
            #stat.xsl but a new directive rtmp_stat_format

            #location /stat {
            #    rtmp_stat all;
            #    rtmp_stat_format json;
            #}

            location /control {
                rtmp_control all; #configuration of control module of rtmp
            }
        }
    }

    rtmp_auto_push on;
    rtmp_auto_push_reconnect 1s;
    rtmp_socket_dir /tmp;

    rtmp {
        out_queue           4096;
        out_cork            8;
        max_streams         128;
        timeout             15s;
        drop_idle_publisher 15s;

        log_interval 5s; #interval used by log module to log in access.log, it is very useful for debug
        log_size     1m; #buffer size used by log module to log in access.log

        server {
            listen 1935;
            server_name www.test.*; #for suffix wildcard matching of virtual host name

            application myapp {
                live on;
                gop_cache on; #open GOP cache for reducing the wating time for the first picture of video
            }

            application hls {
                live on;
                hls on;
                hls_path /tmp/hls;
            }

            application dash {
                live on;
                dash on;
                dash_path /tmp/dash;
            }
        }

        server {
            listen 1935;
            server_name *.test.com; #for prefix wildcard matching of virtual host name

            application myapp {
                live on;
                gop_cache on; #open GOP cache for reducing the wating time for the first picture of video
            }
        }

        server {
            listen 1935;
            server_name www.test.com; #for completely matching of virtual host name

            application myapp {
                live on;
                gop_cache on; #open GOP cache for reducing the wating time for the first picture of video
            }
        }
    }

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-flv](https://github.com/winshining/nginx-http-flv-module){target=_blank}.

# *form-input*: NGINX form input module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-form-input
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_form_input_module.so;
```


This document describes nginx-module-form-input [v0.12](https://github.com/calio/form-input-nginx-module/releases/tag/v0.12){target=_blank} 
released on May 16 2016.

<hr />

form-input-nginx-module - NGINX module that reads HTTP POST and PUT request body encoded in "application/x-www-form-urlencoded" and parses the arguments into nginx variables.

## Description

This is a nginx module that reads HTTP POST and PUT request body encoded
in "application/x-www-form-urlencoded", and parse the arguments in
request body into nginx variables.

This module depends on the ngx_devel_kit (NDK) module.

## Usage

```nginx
set_form_input $variable;
set_form_input $variable argument;

set_form_input_multi $variable;
set_form_input_multi $variable argument;
```

example:

```nginx
#nginx.conf

location /foo {
    # ensure client_max_body_size == client_body_buffer_size
    client_max_body_size 100k;
    client_body_buffer_size 100k;

    set_form_input $data;    # read "data" field into $data
    set_form_input $foo foo; # read "foo" field into $foo
}

location /bar {
    # ensure client_max_body_size == client_body_buffer_size
    client_max_body_size 1m;
    client_body_buffer_size 1m;

    set_form_input_multi $data; # read all "data" field into $data
    set_form_input_multi $foo data; # read all "data" field into $foo

    array_join ' ' $data; # now $data is an string
    array_join ' ' $foo;  # now $foo is an string
}
```


## Limitations

* ngx_form_input will discard request bodies that are buffered
to disk files. When the client_max_body_size setting is larger than
client_body_buffer_size, request bodies that are larger
than client_body_buffer_size (but no larger than
client_max_body_size) will be buffered to disk files.
So it's important to ensure these two config settings take
the same values to avoid confustion.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-form-input](https://github.com/calio/form-input-nginx-module){target=_blank}.

# *geoip*: NGINX GeoIP dynamic modules


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-geoip
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_geoip_module.so;
```
```nginx
load_module modules/ngx_stream_geoip_module.so;
```

<hr />


## Directives

You may find information about configuration directives for this module at the following links:        

*   http://nginx.org/en/docs/http/ngx_http_geoip_module.html#directives

# *geoip2*: NGINX GeoIP2 module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-geoip2
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_geoip2_module.so;
```
```nginx
load_module modules/ngx_stream_geoip2_module.so;
```


This document describes nginx-module-geoip2 [v3.4](https://github.com/leev/ngx_http_geoip2_module/releases/tag/3.4){target=_blank} 
released on Jun 22 2022.

<hr />

**ngx_http_geoip2_module** - creates variables with values from the maxmind geoip2 databases based on the client IP (default) or from a specific variable (supports both IPv4 and IPv6)

The module now supports nginx streams and can be used in the same way the http module can be used.

## Download Maxmind GeoLite2 Database (optional)
The free GeoLite2 databases are available from [Maxminds website](http://dev.maxmind.com/geoip/geoip2/geolite2/) (requires signing up)

## Example Usage:
```nginx
http {
    ...
    geoip2 /etc/maxmind-country.mmdb {
        auto_reload 5m;
        $geoip2_metadata_country_build metadata build_epoch;
        $geoip2_data_country_code default=US source=$variable_with_ip country iso_code;
        $geoip2_data_country_name country names en;
    }

    geoip2 /etc/maxmind-city.mmdb {
        $geoip2_data_city_name default=London city names en;
    }
    ....

    fastcgi_param COUNTRY_CODE $geoip2_data_country_code;
    fastcgi_param COUNTRY_NAME $geoip2_data_country_name;
    fastcgi_param CITY_NAME    $geoip2_data_city_name;
    ....
}

stream {
    ...
    geoip2 /etc/maxmind-country.mmdb {
        $geoip2_data_country_code default=US source=$remote_addr country iso_code;
    }
    ...
}
```

##### Metadata:
Retrieve metadata regarding the geoip database.
```
$variable_name metadata <field>
```
Available fields:
  - build_epoch: the build timestamp of the maxmind database.
  - last_check: the last time the database was checked for changes (when using auto_reload)
  - last_change: the last time the database was reloaded (when using auto_reload)

##### Autoreload (default: disabled):
Enabling auto reload will have nginx check the modification time of the database at the specified
interval and reload it if it has changed.
```
auto_reload <interval>
```

##### GeoIP:
```
$variable_name [default=<value] [source=$variable_with_ip] path ...
```
If default is not specified, the variable will be empty if not found.

If source is not specified, $remote_addr will be used to perform the lookup.

To find the path of the data you want (eg: country names en), use the [mmdblookup tool](https://maxmind.github.io/libmaxminddb/mmdblookup.html):

```
$ mmdblookup --file /usr/share/GeoIP/GeoIP2-Country.mmdb --ip 8.8.8.8

  {
    "country":
      {
        "geoname_id":
          6252001 <uint32>
        "iso_code":
          "US" <utf8_string>
        "names":
          {
            "de":
              "USA" <utf8_string>
            "en":
              "United States" <utf8_string>
          }
      }
  }

$ mmdblookup --file /usr/share/GeoIP/GeoIP2-Country.mmdb --ip 8.8.8.8 country names en

  "United States" <utf8_string>
```

This translates to:

```
$country_name "default=United States" source=$remote_addr country names en
```

##### Additional Commands:
These commands works the same as the original ngx_http_geoip_module documented here: http://nginx.org/en/docs/http/ngx_http_geoip_module.html#geoip_proxy.

However, if you provide the `source=$variable_with_ip` option on a variable, these settings will be ignored for that particular variable.

```
geoip2_proxy < cidr >
```
Defines trusted addresses.  When a request comes from a trusted address, an address from the "X-Forwarded-For" request header field will be used instead.

```
geoip2_proxy_recursive < on | off >
```
If recursive search is disabled then instead of the original client address that matches one of the trusted addresses, the last address sent in "X-Forwarded-For" will be used. If recursive search is enabled then instead of the original client address that matches one of the trusted addresses, the last non-trusted address sent in "X-Forwarded-For" will be used.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-geoip2](https://github.com/leev/ngx_http_geoip2_module){target=_blank}.

# *google*: NGINX Module for Google Mirror creation


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-google
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_google_filter_module.so;
```


This document describes nginx-module-google [v0.2.4](https://github.com/GetPageSpeed/ngx_http_google_filter_module/releases/tag/0.2.4){target=_blank} 
released on Jun 17 2023.

<hr />

[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/cuber/ngx_http_google_filter_module?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

#### Description ####
`ngx_http_google_filter_module` is a filter module which makes google mirror much easier to deploy.    
Regular expressions, uri locations and other complex configurations have been built-in already.    
The native nginx module ensure the efficiency of handling cookies, gstatic scoures and redirections.   
Let's see how `easy` it is to setup a google mirror.
```nginx
location / {
  google on;
}
```
> _What? Are you kidding me?_   
> _Yes, it's just that simple!_
  
#### Demo site [https://g2.wen.lu](https://g2.wen.lu) ####
![Demo Site](http://ww2.sinaimg.cn/large/68bd1777gw1f13naidonmj212i0najsy.jpg)
  
#### Dependency ####
  1. [`pcre`](http://www.pcre.org/) *regular expression support*
  1. [`ngx_http_proxy_module`](http://nginx.org/en/docs/http/ngx_http_proxy_module.html) *backend proxy support*
  1. [`ngx_http_substitutions_filter_module`](https://github.com/yaoweibin/ngx_http_substitutions_filter_module) *mutiple substitutions support*

#
## download the newest source
## @see http://nginx.org/en/download.html
#
wget http://nginx.org/download/nginx-1.7.8.tar.gz

#
## clone ngx_http_google_filter_module
## @see https://github.com/cuber/ngx_http_google_filter_module
#
git clone https://github.com/cuber/ngx_http_google_filter_module

#
## clone ngx_http_substitutions_filter_module
## @see https://github.com/yaoweibin/ngx_http_substitutions_filter_module
#
git clone https://github.com/yaoweibin/ngx_http_substitutions_filter_module
```
##### Brand new installation #####
``` bash
#
## configure nginx customly
## replace </path/to/> with your real path
#
./configure \
  <your configuration> \
  --add-module=</path/to/>ngx_http_google_filter_module \
  --add-module=</path/to/>ngx_http_substitutions_filter_module
```

##### Migrate from existed distribution #####
```bash
#
## get the configuration of existed nginx
## replace </path/to/> with your real path
#
</path/to/>nginx -V
> nginx version: nginx/ <version>
> built by gcc 4.x.x
> configure arguments: <configuration>

#
## download the same version of nginx source
## @see http://nginx.org/en/download.html
## replace <version> with your nginx version
#
wget http://nginx.org/download/nginx-<version>.tar.gz
  
#
## configure nginx
## replace <configuration> with your nginx configuration
## replace </path/to/> with your real path
#
./configure \
  <configuration> \
  --add-module=</path/to/>ngx_http_google_filter_module \
  --add-module=</path/to/>ngx_http_substitutions_filter_module
#
## if some libraries were missing, you should install them with the package manager
## eg. apt-get, pacman, yum ...
#
```

#### Usage ####
##### Basic Configuration #####
  `resolver` is needed to resolve domains.
```nginx
server {
  # ... part of server configuration
  resolver 8.8.8.8;
  location / {
    google on;
  }
  # ...
}
```

##### Google Scholar #####
`google_scholar` depends on `google`, so `google_scholar` cannot be used independently.    
Nowadays google scholar has migrate from `http` to `https`, and `ncr` is supported, so the `tld` of google scholar is no more needed.     
``` nginx
location / {
  google on;
  google_scholar on;
}
```

##### Google Language #####
The default language can be set through `google_language`, if it is not setup, `zh-CN` will be the default language.
```nginx
location / {
  google on;
  google_scholar on;
  # set language to German
  google_language de; 
}
```

Supported languages are listed below.
```txt
ar    -> Arabic
bg    -> Bulgarian
ca    -> Catalan
zh-CN -> Chinese (Simplified)
zh-TW -> Chinese (Traditional)
hr    -> Croatian
cs    -> Czech
da    -> Danish
nl    -> Dutch
en    -> English
tl    -> Filipino
fi    -> Finnish
fr    -> French
de    -> German
el    -> Greek
iw    -> Hebrew
hi    -> Hindi
hu    -> Hungarian
id    -> Indonesian
it    -> Italian
ja    -> Japanese
ko    -> Korean
lv    -> Latvian
lt    -> Lithuanian
no    -> Norwegian
fa    -> Persian
pl    -> Polish
pt-BR -> Portuguese (Brazil)
pt-PT -> Portuguese (Portugal)
ro    -> Romanian
ru    -> Russian
sr    -> Serbian
sk    -> Slovak
sl    -> Slovenian
es    -> Spanish
sv    -> Swedish
th    -> Thai
tr    -> Turkish
uk    -> Ukrainian
vi    -> Vietnamese
```

##### Spider Exclusion #####
The spiders of any search engines are not allowed to crawl google mirror.    
Default `robots.txt` listed below was build-in aleady.
```txt
User-agent: *
Disallow: /
```     
If `google_robots_allow` set to `on`, the `robots.txt` will be replaced with the version of google itself.   
```nginx
  #...
  location / {
    google on;
    google_robots_allow on;
  }
  #...
```

##### Upstreaming #####
`upstream` can help you to avoid name resolving cost, decrease the possibility of google robot detection and proxy through some specific servers.   
``` nginx
upstream www.google.com {
  server 173.194.38.1:443;
  server 173.194.38.2:443;
  server 173.194.38.3:443;
  server 173.194.38.4:443;
}
```

##### Proxy Protocol #####
By default, the proxy will use `https` to communicate with backend servers.      
You can use `google_ssl_off` to force some domains to fall back to `http` protocol.      
It is useful, if you want to proxy some domains through another gateway without ssl certificate.
```nginx
#
## eg. 
## i want to proxy the domain 'www.google.com' like this
## vps(hk) -> vps(us) -> google
#

#
## configuration of vps(hk)
#
server {
  # ...
  location / {
    google on;
    google_ssl_off "www.google.com";
  }
  # ...
}

upstream www.google.com {
  server < ip of vps(us) >:80;
}

#
## configuration of vps(us)
#
server {
  listen 80;
  server_name www.google.com;
  # ...
  location / {
    proxy_pass https://www.google.com;
  }
  # ...
}
```


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-google](https://github.com/GetPageSpeed/ngx_http_google_filter_module){target=_blank}.

# *graphite*: An NGINX module for collecting stats into Graphite


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-graphite
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_graphite_module.so;
```


This document describes nginx-module-graphite [v4.3](https://github.com/mailru/graphite-nginx-module/releases/tag/v4.3){target=_blank} 
released on Jan 20 2023.

<hr />

An nginx module for collecting location stats into Graphite.


## Features

* Aggregation of location, server or http metrics
* Calculation of percentiles
* Sending data to Graphite over UDP or TCP in non-blocking way
* Sending custom metrics from lua

## Synopsis

```nginx

http {
    graphite_config prefix=playground server=127.0.0.1;
    server {
        location /foo/ {
            graphite_data nginx.foo;
        }
    }
}
```

## Description

This module use shared memory segment to collect aggregated stats from all workers and send calculated values for last minute to Graphite every 60s (default) over UDP or TCP in non-blocking way.
Stats aggegation made on the fly in fixed size buffer allocated on server start and does't affect server performance.

This module is in active use on [Mail.Ru Sites](http://mail.ru/) (one of largest web-services in Russia) for about a year and considered stable and well-tested.

To collect metrics from nginx core modules (ssl, gzip, upstream) little patch must be applied on nginx source tree. See [the installation instructions](#installation).
You can build this module as a dynamic one, but then you won't be able to collect metrics from nginx core modules (ssl, gzip, upstream) and lua functions.


## Directives

### graphite_config

**syntax:** *graphite_config key1=&lt;value1&gt; key2=&lt;value2&gt; ... keyN=&lt;valueN&gt;*

**context:** *http*

Specify global settings for a whole server instance.

Param     | Required | Default       | Description
--------- | -------- | ------------- | -----------
prefix    |          |               | path prefix for all graphs
host      |          | gethostname() | host name for all graphs
server    | Yes      |               | carbon-cache server IP address
protocol  |          | udp           | carbon-cache server protocol (udp or tcp)
port      |          | 2003          | carbon-cache server port
frequency |          | 60            | how often send values to Graphite (seconds)
intervals |          | 1m            | aggregation intervals, time interval list, vertical bar separator (`m` - minutes)
params    |          | *             | limit metrics list to track, vertical bar separator
shared    |          | 2m            | shared memory size, increase in case of `too small shared memory` error
buffer    |          | 64k           | network buffer size, increase in case of `too small buffer size` error
package   |          | 1400          | maximum UDP packet size
template  |          |               | template for graph name (default is $prefix.$host.$split.$param_$interval) 
error\_log|          |               | path suffix for error logs graphs (\*)

(\*): works only when nginx_error\_log\_limiting\*.patch is applied to the nginx source code

Example (standard):

```nginx
http {
    graphite_config prefix=playground server=127.0.0.1;
}
```

Example (custom):

```nginx
http {
    graphite_config prefix=playground server=127.0.0.1 intervals=1m|5m|15m params=rps|request_time|upstream_time template=$prefix.$host.$split.$param_$interval;
}
```

Example (error_log):

```nginx
http {
    graphite_config prefix=playground server=127.0.0.1 error_log=log;
}
```

### graphite_default_data

**syntax:** *graphite_default_data &lt;path prefix&gt; [params=&lt;params&gt;] [if=&lt;condition&gt;]*

**context:** *http, server*

Create measurement point in all nested locations.
You can use "$location" or "$server" variables which represent the name of the current location and the name of current server with all non-alphanumeric characters replaced with "\_." Leading and trailing "\_" are deleted.

Example:

```nginx

   graphite_default_data nginx.$location;

   location /foo/ {
   }

   location /bar/ {
   }
```

Data for `/foo/` will be sent to `nginx.foo`, data for `/bar/` - to `nginx.bar`.
The `<params>` parameter (1.3.0) specifies list of params to be collected for all nested locations. To add all default params, use \*.
The `<if>` parameter (1.1.0) enables conditional logging. A request will not be logged if the condition evaluates to "0" or an empty string.

Example(with $server):
```nginx

    graphite_default_data nginx.$server.$location

    server {
        server_name foo_host;

        location /foo/ {
        }
    }

    server {
        server_name bar_host;

        location /bar/ {
        }
    }
```

Data for `/foo/` will be sent to `nginx.foo_host.foo`, data for `/bar/` - to `nginx.bar_host.bar`.

### graphite_data

**syntax:** *graphite_data &lt;path prefix&gt; [params=&lt;params&gt;] [if=&lt;condition&gt;]*

**context:** *http, server, location, if*

Create measurement point in specific location.

Example:

```nginx

    location /foo/ {
        graphite_data nginx.foo;
    }
```

The `<params>` parameter (1.3.0) specifies list of params to be collected for this location. To add all default params, use \*.
The `<if>` parameter (1.1.0) enables conditional logging. A request will not be logged if the condition evaluates to "0" or an empty string.

Example:

```nginx

    map $scheme $is_http { http 1; }
    map $scheme $is_https { https 1; }

    ...

    location /bar/ {
        graphite_data nginx.all.bar;
        graphite_data nginx.http.bar if=$is_http;
        graphite_data nginx.https.bar if=$is_https;
        graphite_data nginx.arg params=rps|request_time;
        graphite_data nginx.ext params=*|rps|request_time;
    }
```

### graphite_param

**syntax:** *graphite_param name=&lt;path&gt; interval=&lt;time value&gt; aggregate=&lt;func&gt;*

**context:** *location*

Param      | Required | Description
---------- | -------- | -----------
name       | Yes      | path prefix for all graphs
interval   | Yes\*    | aggregation interval, time intrval value format (`m` - minutes)
aggregate  | Yes\*    | aggregation function on values
percentile | Yes\*    | percentile level

#### aggregate functions
func   | Description
------ | -----------
sum    | sum of values per interval
persec | sum of values per second  (`sum` divided on seconds in `interval`)
avg    | average value on interval
gauge  | gauge value

Example: see below.

## Nginx API for Lua

**syntax:** *ngx.graphite.param(&lt;name&gt;)*

Get a link on a graphite parameter name, to use it in place of the name for the functions below.
The link is valid up to nginx reload. After getting the link of a parameter, you can still pass
the parameter name to the functions below. You can get the link of a parameter multiple times,
you'll always get the same object by the same name (a lightuserdata). The function returns false
if the parameter specified by name doesn't exist. The function returns nil on link getting errors.
Functions access parameters information by link faster than by name.

*Available after applying patch to lua-nginx-module.* The feature is present in the patch for lua
module v0.10.12. See [the installation instructions](#build-nginx-with-lua-and-graphite-modules).

**syntax:** *ngx.graphite(&lt;name_or_link&gt;,&lt;value&gt;[,&lt;config&gt;])*

Write stat value into aggregator function. Floating point numbers accepted in `value`.

*Available after applying patch to lua-nginx-module.* See [the installation instructions](#build-nginx-with-lua-and-graphite-modules).

```lua
ngx.graphite(name, value, config)
```

Example:

```nginx

location /foo/ {
    graphite_param name=lua.foo_sum aggregate=sum interval=1m;
    graphite_param name=lua.foo_rps aggregate=persec interval=1m;
    graphite_param name=lua.foo_avg aggregate=avg interval=1m;
    graphite_param name=lua.foo_gauge aggregate=gauge;

    content_by_lua '
        ngx.graphite("lua.foo_sum", 0.01)
        ngx.graphite("lua.foo_rps", 1)
        ngx.graphite("lua.foo_avg", ngx.var.request_uri:len())
        local foo_gauge_link = ngx.graphite.param("lua.foo_gauge")
        ngx.graphite(foo_gauge_link, 10)
        ngx.graphite(foo_gauge_link, -2)
        ngx.graphite("lua.auto_rps", 1, "aggregate=persec interval=1m percentile=50|90|99")
        ngx.say("hello")
    ';
}
```

You must either specify the `graphite_param` command or pass the `config` argument.
If you choose the second option, the data for this graph will not be sent until the first call to ngx.graphite.

**Warning:**
If you do not declare graph using `graphite_param` command then memory for the graph will be allocated dynamically in module's shared memory.
If module's shared memory is exhausted while nginx is running, no new graphs will be created and an error message will be logged.

**syntax:** *ngx.graphite.get(&lt;name_or_link&gt;)*

Get value of the gauge param with specified `name_or_link`.

**syntax:** *ngx.graphite.set(&lt;name&gt;,&lt;value&gt;)*

Set `value` to the gauge param with specified `name_or_link`.

## Params

Param                   | Units | Func | Description
----------------------- | ----- | ---- | ------------------------------------------
request\_time           | ms    | avg  | total time spent on serving request
bytes\_sent             | bytes | avg  | http response length
body\_bytes\_sent       | bytes | avg  | http response body length
request\_length         | bytes | avg  | http request length
ssl\_handshake\_time    | ms    | avg  | time spent on ssl handsake
ssl\_cache\_usage       | %     | last | how much SSL cache used
content\_time           | ms    | avg  | time spent generating content inside nginx
gzip\_time              | ms    | avg  | time spent gzipping content ob-the-fly
upstream\_time          | ms    | avg  | time spent tailking with upstream
upstream\_connect\_time | ms    | avg  | time spent on upstream connect (nginx >= 1.9.1)
upstream\_header\_time  | ms    | avg  | time spent on upstream header (nginx >= 1.9.1)
rps                     | rps   | sum  | total requests number per second
keepalive\_rps          | rps   | sum  | requests number sent over previously opened keepalive connection
response\_2xx\_rps      | rps   | sum  | total responses number with 2xx code
response\_3xx\_rps      | rps   | sum  | total responses number with 3xx code
response\_4xx\_rps      | rps   | sum  | total responses number with 4xx code
response\_5xx\_rps      | rps   | sum  | total responses number with 5xx code
response\_[0-9]{3}\_rps | rps   | sum  | total responses number with given code
upstream\_cache\_(miss\|bypass\|expired\|stale\|updating\|revalidated\|hit)\_rps | rps   | sum  | totar responses with a given upstream cache status
lua\_time               | ms    | avg  | time spent on lua code

## Percentiles

To calculate percentile value for any parameter, set percentile level via `/`. E.g. `request_time/50|request_time/90|request_time/99`.

#### Build nginx with graphite module
```bash

wget 'http://nginx.org/download/nginx-1.9.2.tar.gz'
tar -xzf nginx-1.9.2.tar.gz
cd nginx-1.9.2/

## patch to add api for sending metrics from lua code (optional)
patch -p1 < /path/to/graphite-nginx-module/lua_module_v0_9_11.patch
cd ..

wget 'http://nginx.org/download/nginx-1.9.2.tar.gz'
tar -xzf nginx-1.9.2.tar.gz
cd nginx-1.9.2/

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-graphite](https://github.com/mailru/graphite-nginx-module){target=_blank}.

# *headers-more*: NGINX Headers More dynamic module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-headers-more
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_headers_more_filter_module.so;
```


This document describes nginx-module-headers-more [v0.33](https://github.com/dvershinin/headers-more-nginx-module/releases/tag/v0.33){target=_blank} 
released on Jun 28 2022.

<hr />

**ngx_headers_more** - Set and clear input and output headers...more than "add"!


## Synopsis

```nginx

 # set the Server output header
 more_set_headers 'Server: my-server';

 # set and clear output headers
 location /bar {
     more_set_headers 'X-MyHeader: blah' 'X-MyHeader2: foo';
     more_set_headers -t 'text/plain text/css' 'Content-Type: text/foo';
     more_set_headers -s '400 404 500 503' -s 413 'Foo: Bar';
     more_clear_headers 'Content-Type';

     # your proxy_pass/memcached_pass/or any other config goes here...
 }

 # set output headers
 location /type {
     more_set_headers 'Content-Type: text/plain';
     # ...
 }

 # set input headers
 location /foo {
     set $my_host 'my dog';
     more_set_input_headers 'Host: $my_host';
     more_set_input_headers -t 'text/plain' 'X-Foo: bah';

     # now $host and $http_host have their new values...
     # ...
 }

 # replace input header X-Foo *only* if it already exists
 more_set_input_headers -r 'X-Foo: howdy';
```

## Description

This module allows you to add, set, or clear any output
or input header that you specify.

This is an enhanced version of the standard
[headers](http://nginx.org/en/docs/http/ngx_http_headers_module.html) module because it provides more utilities like
resetting or clearing "builtin headers" like `Content-Type`,
`Content-Length`, and `Server`.

It also allows you to specify an optional HTTP status code
criteria using the `-s` option and an optional content
type criteria using the `-t` option while modifying the
output headers with the [more_set_headers](#more_set_headers) and
[more_clear_headers](#more_clear_headers) directives. For example,

```nginx
 more_set_headers -s 404 -t 'text/html' 'X-Foo: Bar';
```

You can also specify multiple MIME types to filter out in a single `-t` option.
For example,

```nginx
more_set_headers -t 'text/html text/plain' 'X-Foo: Bar';
```

Never use other paramemters like `charset=utf-8` in the `-t` option values; they will not
work as you would expect.

Input headers can be modified as well. For example

```nginx
 location /foo {
     more_set_input_headers 'Host: foo' 'User-Agent: faked';
     # now $host, $http_host, $user_agent, and
     #   $http_user_agent all have their new values.
 }
```

The option `-t` is also available in the
[more_set_input_headers](#more_set_input_headers) and
[more_clear_input_headers](#more_clear_input_headers) directives (for request header filtering) while the `-s` option
is not allowed.

Unlike the standard [headers](http://nginx.org/en/docs/http/ngx_http_headers_module.html) module, this module's directives will by
default apply to all the status codes, including `4xx` and `5xx`.


## Directives


## more_set_headers
**syntax:** *more_set_headers [-t &lt;content-type list&gt;]... [-s &lt;status-code list&gt;]... &lt;new-header&gt;...*

**default:** *no*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

Replaces (if any) or adds (if not any) the specified output headers when the response status code matches the codes specified by the `-s` option *AND* the response content type matches the types specified by the `-t` option.

If either `-s` or `-t` is not specified or has an empty list value, then no match is required. Therefore, the following directive set the `Server` output header to the custom value for *any* status code and *any* content type:

```nginx

   more_set_headers    "Server: my_server";
```

Existing response headers with the same name are always overridden. If you want to add headers incrementally, use the standard [add_header](http://nginx.org/en/docs/http/ngx_http_headers_module.html#add_header) directive instead.

A single directive can set/add multiple output headers. For example

```nginx

   more_set_headers 'Foo: bar' 'Baz: bah';
```

Multiple occurrences of the options are allowed in a single directive. Their values will be merged together. For instance

```nginx

   more_set_headers -s 404 -s '500 503' 'Foo: bar';
```

is equivalent to

```nginx

   more_set_headers -s '404 500 503' 'Foo: bar';
```

The new header should be the one of the forms:

1. `Name: Value`
1. `Name: `
1. `Name`

The last two effectively clear the value of the header `Name`.

Nginx variables are allowed in header values. For example:

```nginx

    set $my_var "dog";
    more_set_headers "Server: $my_var";
```

But variables won't work in header keys due to performance considerations.

Multiple set/clear header directives are allowed in a single location, and they're executed sequentially.

Directives inherited from an upper level scope (say, http block or server blocks) are executed before the directives in the location block.

Note that although `more_set_headers` is allowed in *location* if blocks, it is *not* allowed in the *server* if blocks, as in

```nginx

   ?  # This is NOT allowed!
   ?  server {
   ?      if ($args ~ 'download') {
   ?          more_set_headers 'Foo: Bar';
   ?      }
   ?      ...
   ?  }
```

Behind the scene, use of this directive and its friend [more_clear_headers](#more_clear_headers) will (lazily) register an ouput header filter that modifies `r->headers_out` the way you specify.


## more_clear_headers
**syntax:** *more_clear_headers [-t &lt;content-type list&gt;]... [-s &lt;status-code list&gt;]... &lt;new-header&gt;...*

**default:** *no*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

Clears the specified output headers.

In fact,

```nginx

    more_clear_headers -s 404 -t 'text/plain' Foo Baz;
```

is exactly equivalent to

```nginx

    more_set_headers -s 404 -t 'text/plain' "Foo: " "Baz: ";
```

or

```nginx

    more_set_headers -s 404 -t 'text/plain' Foo Baz
```

See [more_set_headers](#more_set_headers) for more details.

The wildcard character, `*`, can also be used at the end of the header name to specify a pattern. For example, the following directive
effectively clears *any* output headers starting by "`X-Hidden-`":

```nginx

 more_clear_headers 'X-Hidden-*';
```

The `*` wildcard support was first introduced in [v0.09](#v009).


## more_set_input_headers
**syntax:** *more_set_input_headers [-r] [-t &lt;content-type list&gt;]... &lt;new-header&gt;...*

**default:** *no*

**context:** *http, server, location, location if*

**phase:** *rewrite tail*

Very much like [more_set_headers](#more_set_headers) except that it operates on input headers (or request headers) and it only supports the `-t` option.

Note that using the `-t` option in this directive means filtering by the `Content-Type` *request* header, rather than the response header.

Behind the scene, use of this directive and its friend [more_clear_input_headers](#more_clear_input_headers) will (lazily)
register a `rewrite phase` handler that modifies `r->headers_in` the way you specify. Note that it always run at the *end* of
the `rewrite` phase so that it runs *after* the standard [rewrite module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html)
and works in subrequests as well.

If the `-r` option is specified, then the headers will be replaced to the new values *only if* they already exist.


## more_clear_input_headers
**syntax:** *more_clear_input_headers [-t &lt;content-type list&gt;]... &lt;new-header&gt;...*

**default:** *no*

**context:** *http, server, location, location if*

**phase:** *rewrite tail*

Clears the specified input headers.

In fact,

```nginx

    more_clear_input_headers -t 'text/plain' Foo Baz;
```

is exactly equivalent to

```nginx

    more_set_input_headers -t 'text/plain' "Foo: " "Baz: ";
```

or

```nginx

    more_set_input_headers -t 'text/plain' Foo Baz
```

To remove request headers "Foo" and "Baz" for all incoming requests regardless of the content type, we can write

```nginx

    more_clear_input_headers "Foo" "Baz";
```

See [more_set_input_headers](#more_set_input_headers) for more details.

The wildcard character, `*`, can also be used at the end of the header name to specify a pattern. For example, the following directive
effectively clears *any* input headers starting by "`X-Hidden-`":

```nginx

     more_clear_input_headers 'X-Hidden-*';
```


## Limitations

* Unlike the standard [headers](http://nginx.org/en/docs/http/ngx_http_headers_module.html) module, this module does not automatically take care of the constraint among the `Expires`, `Cache-Control`, and `Last-Modified` headers. You have to get them right yourself or use the [headers](http://nginx.org/en/docs/http/ngx_http_headers_module.html) module together with this module.
* You cannot remove the `Connection` response header using this module because the `Connection` response header is generated by the standard `ngx_http_header_filter_module` in the Nginx core, whose output header filter runs always *after* the filter of this module. The only way to actually remove the `Connection` header is to patch the Nginx core, that is, editing the C function `ngx_http_header_filter` in the `src/http/ngx_http_header_filter_module.c` file.


## Changes

The changes of every release of this module can be obtained from the OpenResty bundle's change logs:

<http://openresty.org/#Changes>


## Test Suite

This module comes with a Perl-driven test suite. The [test cases](https://github.com/openresty/headers-more-nginx-module/tree/master/t/) are
[declarative](https://github.com/openresty/headers-more-nginx-module/blob/master/t/sanity.t) too. Thanks to the [Test::Nginx](http://search.cpan.org/perldoc?Test::Nginx) module in the Perl world.

To run it on your side:

```bash

 $ PATH=/path/to/your/nginx-with-headers-more-module:$PATH prove -r t
```

To run the test suite with valgrind's memcheck, use the following commands:

```bash

 $ export PATH=/path/to/your/nginx-with-headers-more-module:$PATH
 $ TEST_NGINX_USE_VALGRIND=1 prove -r t
```

You need to terminate any Nginx processes before running the test suite if you have changed the Nginx server binary.

Because a single nginx server (by default, `localhost:1984`) is used across all the test scripts (`.t` files), it's meaningless to run the test suite in parallel by specifying `-jN` when invoking the `prove` utility.

Some parts of the test suite requires modules [proxy](http://nginx.org/en/docs/http/ngx_http_proxy_module.html), [rewrite](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html), and [echo](https://github.com/openresty/echo-nginx-module) to be enabled as well when building Nginx.


## See Also

* The original thread on the Nginx mailing list that inspires this module's development: ["A question about add_header replication"](http://forum.nginx.org/read.php?2,11206,11738).
* The orginal announcement thread on the Nginx mailing list: ["The "headers_more" module: Set and clear output headers...more than 'add'!"](http://forum.nginx.org/read.php?2,23460).
* The original [blog post](http://agentzh.blogspot.com/2009/11/headers-more-module-scripting-input-and.html) about this module's initial development.
* The [echo module](https://github.com/openresty/echo-nginx-module) for Nginx module's automated testing.
* The standard [headers](http://nginx.org/en/docs/http/ngx_http_headers_module.html) module.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-headers-more](https://github.com/dvershinin/headers-more-nginx-module){target=_blank}.

# *hmac-secure-link*: Alternative NGINX HMAC Secure Link module with support for OpenSSL hashes


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-hmac-secure-link
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_hmac_secure_link_module.so;
```


This document describes nginx-module-hmac-secure-link [v0.3](https://github.com/nginx-modules/ngx_http_hmac_secure_link_module/releases/tag/0.3){target=_blank} 
released on Mar 06 2019.

<hr />

## Description:

The Nginx HMAC secure link module enhances the security and functionality of the standard secure link module.  
Secure token is created using secure HMAC construction with an arbitrary hash algorithm supported by OpenSSL, e.g.:
`blake2b512`, `blake2s256`, `gost`, `md4`, `md5`, `rmd160`, `sha1`, `sha224`, `sha256`,
`sha3-224`, `sha3-256`, `sha3-384`, `sha3-512`, `sha384`, `sha512`, `sha512-224`, `sha512-256`, `shake128`, `shake256`, `sm3`.

Furthermore, secure token is created as described in RFC2104, that is,
`H(secret_key XOR opad,H(secret_key XOR ipad, message))` instead of a simple `MD5(secret_key,message, expire)`.

## Usage:

Message to be hashed is defined by `secure_link_hmac_message`, `secret_key` is given by `secure_link_hmac_secret`, and hashing algorithm H is defined by `secure_link_hmac_algorithm`.

For improved security the timestamp in ISO 8601 the format `2017-12-08T07:54:59+00:00` (one possibility according to ISO 8601) or as `Unix Timestamp` should be appended to the message to be hashed.

It is possible to create links with limited lifetime. This is defined by an optional parameter. If the expiration period is zero or it is not specified, a link has the unlimited lifetime.

Configuration example for server side.

```nginx
location ^~ /files/ {
    # Variable to be passed are secure token, timestamp, expiration period (optional)
    secure_link_hmac  $arg_st,$arg_ts,$arg_e;

    # Secret key
    secure_link_hmac_secret my_secret_key;

    # Message to be verified
    secure_link_hmac_message $uri$arg_ts$arg_e;

    # Cryptographic hash function to be used
    secure_link_hmac_algorithm sha256;

    # If the hash is incorrect then $secure_link_hmac is a null string.
    # If the hash is correct but the link has already expired then $secure_link_hmac is zero.
    # If the hash is correct and the link has not expired then $secure_link_hmac is one.

    # In production environment, we should not reveal to potential attacker
    # why hmac authentication has failed
    if ($secure_link_hmac != "1") {
        return 404;
    }

    rewrite ^/files/(.*)$ /files/$1 break;
}
```

Application side should use a standard hash_hmac function to generate hash, which then needs to be base64url encoded. Example in Perl below.

#### Variable $data contains secure token, timestamp in ISO 8601 format, and expiration period in seconds

```nginx
perl_set $secure_token '
    sub {
        use Digest::SHA qw(hmac_sha256_base64);
        use POSIX qw(strftime);

        my $now = time();
        my $key = "my_very_secret_key";
        my $expire = 60;
        my $tz = strftime("%z", localtime($now));
        $tz =~ s/(\d{2})(\d{2})/$1:$2/;
        my $timestamp = strftime("%Y-%m-%dT%H:%M:%S", localtime($now)) . $tz;
        my $r = shift;
        my $data = $r->uri;
        my $digest = hmac_sha256_base64($data . $timestamp . $expire,  $key);
        $digest =~ tr(+/)(-_);
        $data = "st=" . $digest . "&ts=" . $timestamp . "&e=" . $expire;
        return $data;
    }
';
```

A similar function in PHP

```php
$secret = 'my_very_secret_key';
$expire = 60;
$algo = 'sha256';
$timestamp = date('c');
$stringtosign = "/files/top_secret.pdf{$timestamp}{$expire}";
$hashmac = base64_encode(hash_hmac($algo, $stringtosign, $secret, true));
$hashmac = strtr($hashmac, '+/', '-_'));
$hashmac = str_replace('=', '', $hashmac);
$host = $_SERVER['HTTP_HOST'];
$loc = "https://{$host}/files/top_secret.pdf?st={$hashmac}&ts={$timestamp}&e={$expire}";
```

Using Unix timestamp in Node.js

```javascript
const crypto = require("crypto");
const secret = 'my_very_secret_key';
const expire = 60;
const unixTimestamp = Math.round(Date.now() / 1000.);
const stringToSign = `/files/top_secret.pdf${unixTimestamp}${expire}`;
const hashmac = crypto.createHmac('sha256', secret).update(stringToSign).digest('base64')
      .replace(/=/g, '')
      .replace(/\+/g, '-')
      .replace(/\//g, '_');
const loc = `https://host/files/top_secret.pdf?st=${hashmac}&ts=${unixTimestamp}&e=${expire}`;
```

It is also possible to use this module with a Nginx acting as proxy server.

The string to be signed is defined in `secure_link_hmac_message`, the `secure_link_hmac_token` variable contains then a secure token to be passed to backend server.

```nginx
location ^~ /backend_location/ {
    set $expire 60;

    secure_link_hmac_message "$uri$time_iso8601$expire";
    secure_link_hmac_secret "my_very_secret_key";
    secure_link_hmac_algorithm sha256;

    proxy_pass "http://backend_server$uri?st=$secure_link_hmac_token&ts=$time_iso8601&e=$expire";
}
```


## Embedded Variables
* `$secure_link_hmac` - 
* `$secure_link_hmac_token` - 
* `$secure_link_hmac_expires` - The lifetime of a link passed in a request.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-hmac-secure-link](https://github.com/nginx-modules/ngx_http_hmac_secure_link_module){target=_blank}.

# *html-sanitize*: NGINX module to sanitize HTML 5 with whitelisted elements, attributes and CSS


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-html-sanitize
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_html_sanitize_module.so;
```


This document describes nginx-module-html-sanitize [v0.2.4](https://github.com/dvershinin/ngx_http_html_sanitize_module/releases/tag/0.2.4){target=_blank} 
released on Aug 08 2019.

<hr />
[gumbo-parser]: https://github.com/google/gumbo-parser
[katana-parser]: https://github.com/hackers-painters/katana-parser
[google/gumbo-parser]: https://github.com/google/gumbo-parser
[hackers-painters/katana-parser]: https://github.com/hackers-painters/katana-parser
[nginx]: https://nginx.org/
[license]: /license
[expression]: https://msdn.microsoft.com/en-us/library/ms537634(v=vs.85).aspx
[ngx_http_html_sanitize_module]: https://github.com/youzan/ngx_http_html_sanitize_module
[url]: https://developer.mozilla.org/en-US/docs/Web/CSS/url
[linkable_attribute]: #linkable_attribute
[directive]: #directive
[html_sanitize]: #html_sanitize
[html_sanitize_hash_max_size]: #html_sanitize_hash_max_size
[html_sanitize_hash_bucket_size]: #html_sanitize_hash_bucket_size
[html_sanitize_element]: #html_sanitize_element
[html_sanitize_attribute]: #html_sanitize_attribute
[html_sanitize_style_property]: #html_sanitize_style_property
[html_sanitize_url_protocol]: #html_sanitize_url_protocol
[html_sanitize_url_domain]: #html_sanitize_url_domain
[html_sanitize_iframe_url_protocol]: #html_sanitize_iframe_url_protocol
[html_sanitize_iframe_url_domain]: #html_sanitize_iframe_url_domain
[querystring]: #querystring
[document]: #document
[html]: #html
[script]: #script
[style]: #style
[namespace]: #namespace
[context]: #context
[element]: #element
[attribute]: #attribute
[style_property]: #style_property
[style_property_value]: #style_property_value
[url_protocol]: #url_protocol
[url_domain]: #url_domain
[iframe_url_protocol]: #iframe_url_protocol
[iframe_url_domain]: #iframe_url_domain

## Name

[ngx_http_html_sanitize_module] - It's base on google's [gumbo-parser] as HTML5 parser and hackers-painters's [katana-parser] as inline CSS parser to sanitize HTML with whitelisted elements, whitelisted attributes and whitelisted CSS property.

## Status

Production Ready :-)


## Example

There is a example of [nginx] configuration according to the [https://dev.w3.org/html5/html-author/#the-elements](https://dev.w3.org/html5/html-author/#the-elements) as the following:

```nginx
server {
    listen 8888;

    location = /sanitize {
        # Explicitly set utf-8 encoding
        add_header Content-Type "text/html; charset=UTF-8";

        client_body_buffer_size 10M;
        client_max_body_size 10M;

        html_sanitize on;

        # Check https://dev.w3.org/html5/html-author/#the-elements

        # Root Element
        html_sanitize_element html;

        # Document Metadata
        html_sanitize_element head title base link meta style;

        # Scripting
        html_sanitize_element script noscript;

        # Sections
        html_sanitize_element body section nav article aside h1 h2 h3 h4 h5 h6 header footer address;

        # Grouping Content
        html_sanitize_element p hr br pre dialog blockquote ol ul li dl dt dd;

        # Text-Level Semantics
        html_sanitize_element a q cite em strong small mark dfn abbr time progress meter code var samp kbd sub sup span i b bdo ruby rt rp;

        # Edits
        html_sanitize_element ins del;

        # Embedded Content
        htlm_sanitize_element figure img iframe embed object param video audio source canvas map area;

        # Tabular Data
        html_sanitize_element table caption colgroup col tbody thead tfoot tr td th;

        # Forms
        html_sanitize_element form fieldset label input button select datalist optgroup option textare output;

        # Interactive Elements
        html_sanitize_element details command bb menu;

        # Miscellaneous Elements
        html_sanitize_element legend div;

        html_sanitize_attribute *.style;
        html_sanitize_attribute a.href a.hreflang a.name a.rel;
        html_sanitize_attribute col.span col.width colgroup.span colgroup.width;
        html_sanitize_attribute data.value del.cite del.datetime;
        html_sanitize_attribute img.align img.alt img.border img.height img.src img.width;
        html_sanitize_attribute ins.cite ins.datetime li.value ol.reversed ol.stasrt ol.type ul.type;
        html_sanitize_attribute table.align table.bgcolor table.border table.cellpadding table.cellspacing table.frame table.rules table.sortable table.summary table.width;
        html_sanitize_attribute td.abbr td.align td.axis td.colspan td.headers td.rowspan td.valign td.width;
        html_sanitize_attribute th.abbr th.align th.axis th.colspan th.rowspan th.scope th.sorted th.valign th.width;

        html_sanitize_style_property color font-size;

        html_sanitize_url_protocol http https tel;
        html_sanitize_url_domain *.google.com google.com;

        html_sanitize_iframe_url_protocol http https;
        html_sanitize_iframe_url_domain  facebook.com *.facebook.com;
    }
}
```

And It's recommanded to use the below commnand to sanitize HTML5:

```bash
$ curl -X POST -d "<h1>Hello World </h1>" http://127.0.0.1:8888/sanitize?element=2&attribute=1&style_property=1&style_property_value=1&url_protocol=1&url_domain=0&iframe_url_protocol=1&iframe_url_domain=0

<h1>Hello World </h1>
```

This querystring `element=2&attribute=1&style_property=1&style_property_value=1&url_protocol=1&url_domain=0&iframe_url_protocol=1&iframe_url_domain=0` is the as following:

* element=2: output whitelisted element by [html_sanitize_element]
* attribute=1: output any attribute by [html_sanitize_attribute]
* style_property=1: output any style property by [html_sanitize_style_property]
* style_property_value=1: check the style value for [url] function and [expression] function to avoid XSS inject by  [style_property_value]
* url_protocol=1: check whitelisted url_protocol for absoluted URL by [html_sanitize_url_protocol]
* url_domain=0: do not check url domain for absoluted URL
* iframe_url_protocol=1: is the same as [url_protocol] but only for `iframe.src` by [html_sanitize_iframe_url_protocol]
* iframe_url_domain=0: is the same as [url_domain] but only for `iframe.src` by [html_sanitize_iframe_url_domain]

With [ngx_http_html_sanitize_module], we have the ability to specify whether output HTML5's element 、attribute and inline CSS's property by [directive] and [querystring] as the following:

## whitelisted element

* disable element:

  if we do not want to output any element, we can do this as the following:

```bash
curl -X POST -d "<h1>h1</h1>" http://127.0.0.1:8888/sanitize?element=0

```

* enable element:

  if we want to output any element, we can do this as the following:
```bash
$ curl -X POST -d "<h1>h1</h1><h7>h7</h7>" http://127.0.0.1:8888/sanitize?element=1

<h1>h1</h1><h7>h7</h7>

```

* enable whitelisted element:

  if we want to output whitelisted element, we can do this as the following

```bash
$ curl -X POST -d "<h1>h1</h1><h7>h7</h7>" http://127.0.0.1:8888/sanitize?element=1

<h1>h1</h1>
```

## whitelisted attribute

* disable attribute:

  if we do not want to output any attribute, we can do this as the following:

```bash
curl -X POST -d "<h1 ha=\"ha\">h1</h1>" "http://127.0.0.1:8888/sanitize?element=1&attribute=0"

<h1>h1</h1>
```

* enable attribute:

  if we want to output any attribute, we can do this as the following:
```bash
$ curl -X POST -d "<h1 ha=\"ha\">h1</h1>" "http://127.0.0.1:8888/sanitize?element=1&attribute=1"

<h1 ha="ha">h1</h1>

```

* enable whitelisted attribute:

  if we want to output whitelisted element, we can do this as the following:

```bash
$ curl -X POST -d "<img src=\"/\" ha=\"ha\" />" "http://127.0.0.1:8888/sanitize?element=1&attribute=2"

<img src="/" />
```

## whitelisted style property

* disable style property:

  if we do not want to output any style property, we can do this as the following:

```bash
## It will do not output any style property
curl -X POST -d "<h1 style=\"color:red;\">h1</h1>" "http://127.0.0.1:8888/sanitize?element=1&attribute=1&style_property=0"

<h1>h1</h1>
```

* enable style property:

  if we want to output any style property, we can do this as the following:
```bash
$ curl -X POST -d "<h1 style=\"color:red;text-align:center;\">h1</h1>" "http://127.0.0.1:8888/sanitize?element=1&attribute=1&style_property=1"

<h1 style="color:red;text-align:center">h1</h1>
```

* enable whitelisted style property:

  if we want to output whitelisted style property, we can do this as the following:

```bash
$ curl -X POST -d "<h1 style=\"color:red;text-align:center;\" >h1</h1>" "http://127.0.0.1:8888/sanitize?element=1&attribute=1&style_property=2"

<h1 style="color:red;">h1</h1>
```

## Description

Now the implement of [ngx_http_html_sanitize_module] is based on [gumbo-parser] and [katana-parser]. And we make the combo upon it then run it on [nginx] to as a center web service maintained by professional security people for discarding language-level difference. If we want to gain more higher performance (here is the [brenchmark](#benchmark)), it's recommanded to write language-level library wrapering above pure c library to overcome the overhead of network transmission.

## Benchmark

Testing with `wrk -s benchmarks/shot.lua -d 60s "http://127.0.0.1:8888"` on Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz and 64GB memory

| Name | Size | Avg Latency | QPS
| ------------- |:-------------:| -----:| -----:|
| [hacker_news.html](/benchmarks/hacker_news.html) | 30KB | 9.06ms | 2921.82
| [baidu.html](/benchmarks/baidu.html) | 76KB | 13.41ms | 1815.75
| [arabic_newspapers.html](/benchmarks/arabic_newspapers.html) | 78KB | 16.58ms| 1112.70 |
| [bbc.html](/benchmakrs/bbc.html) | 115KB | 17.96ms |993.12
| [xinhua.html](/benchmarks/xinhua.html) | 323KB | 33.37ms | 275.39
| [google.html](/benchmakrs/google.html) | 336KB | 26.78ms | 351.54
| [yahoo.html](/benchmakrs/yahoo.html) | 430KB | 29.16ms | 323.04
| [wikipedia.html](/benchmakrs/wikipedia.html) | 511KB | 57.62ms | 160.10
| [html5_spec.html](/benchmarks/html5_spec.html) | 7.7MB | 1.63s | 2.00

## Directive

## html_sanitize

**syntax:** *html_sanitize on | off*

**default:** *html_sanitize on*

**context:** *location*

Specifies whether enable html sanitize handler on location context


## html_sanitize_hash_max_size

**syntax:** *html_sanitize_hash_max_size size*

**default:** *html_sanitize_hash_max_size 2048*

**context:** *location*

Sets the maximum size of the element、attribute、style_property、url_protocol、url_domain、iframe_url_protocol、iframe_url_domain hash tables.

## html_sanitize_hash_bucket_size

**syntax:** *html_sanitize_hash_bucket_size size*

**default:** *html_sanitize_hash_bucket_size 32|64|128*

**context:** location

Sets the bucket size for element、attribute、style_property、url_protocol、url_domain、iframe_url_protocol、iframe_url_domain. The default value depends on the size of the processor’s cache line.

## html_sanitize_element

**syntax:** *html_sanitize_element element ...*

**default:** -

**context:** location

Set the whitelisted HTML5 elements when enable whitelisted element by setting the querystring [element] whitelist mode as the following:

``` nginx
html_sanitize_element html head body;
```

## html_sanitize_attribute

**syntax:** *html_sanitize_attribute attribute ...*

**default:** -

**context:** location

Set the whitelisted HTML5 attributes when enable whitelisted element by setting the querystring [attribute] whitelist mode as the following:

``` nginx
html_sanitize_attribute a.href h1.class;
```

PS: attribute format must be the same as `element.attribute` and support `*.attribute` (prefix asterisk) and `element.*` (suffix asterisk)

## html_sanitize_style_property

**syntax:** *html_sanitize_style_property property ...*

**default:** -

**context:** location

Set the whitelisted CSS property when enable whitelisted element by setting the querystring [style_property] whitelist mode as the following:

```nginx
html_sanitize_style_property color background-color;
```

## html_sanitize_url_protocol

**syntax:** *html_sanitize_url_protocol [protocol] ...*

**default:** -

**context:** location

Set the allowed URL protocol at [linkable attribute](#linkable_attribute) when only the URL is absoluted rahter than related and enable URL protocol check by setting the querystring [url_protocol] check mode as the following:

```nginx
html_sanitize_url_protocol http https tel;
```

## html_sanitize_url_domain

**syntax:** *html_sanitize_url_domain domain ...*

**default:** -

**context:** location

Set the allowed URL domain at [linkable attribute](#linkable_attribute) when only the URL is absoluted rahter than relatived and enable URL protocol check、URL domain check by setting the querystring [url_protocol] check mode and the querystring [url_domain][#url_domain] check mode as the following:

```nginx
html_sanitize_url_domain *.google.com google.com;
```

## html_sanitize_iframe_url_protocol

**syntax:** *html_sanitize_iframe_url_protocol [protocol] ...*

**default:** -

**context:** location

is the same as [html_sanitize_url_protocol] but only for `iframe.src` attribute

```nginx
html_sanitize_iframe_url_protocol http https tel;
```

## html_sanitize_iframe_url_domain

**syntax:** *html_sanitize_iframe_url_domain [protocol] ...*

**default:** -

**context:** location

is the same as [html_sanitize_url_domain] but only for `iframe.src` attribute

```nginx
html_sanitize_iframe_url_domain *.facebook.com facebook.com;
```

## linkable_attribute
The linkable attribute is the following:

* a.href
* blockquote.cite
* q.cite
* del.cite
* img.src
* ins.cite
* iframe.src
* CSS URL function

## Querystring
the querystring from request URL is used to control the [ngx_http_html_sanitize_module] internal action.

## document
**value:** *0 or 1*

**default:** *0*

**context:** querystring

Specifies whether append `<!DOCTYPE>` to response body

## html
**value:** *0 or 1*

**default:** *0*

**context:** querystring

Specifies whether append `<html></html>` to response body


## script
**value:** *0 or 1*

**default:** *0*

**context:** querystring

Specifies whether allow `<script></script>`

## style
**value:** *0 or 1*

**default:** *0*

**context:** querystring

Specifies whether allow `<style></style>`

## namespace
**value:** *0、1 or 2*

**default:** *0*

**context:** querystring

Specifies the mode of gumbo-parser with the value as the following:

* GUMBO_NAMESPACE_HTML: 0
* GUMBO_NAMESPACE_SVG: 1
* GUMBO_NAMESPACE_MATHML: 2

## context
**value:** *[0, 150)*

**default:** *38(GUMBO_TAG_DIV)*

**context:** querystring

Specifies the context of gumbo-parser with the value at the this file [tag_enum.h](tag_enum.h)

## element
**value:** *0、1、2*

**default:** *0*

**context:** querystring

Specifies the mode of output element with the value as the following:

   * 0: do not output element
   * 1: output all elements
   * 2: output whitelisted elements

## attribute
**value:** *0、1、2*

**default:** *0*

**context:** querystring

Specifies the mode of output attribute with the value as the following:

   * 0: do not output attributes
   * 1: output all attributes
   * 2: output whitelisted attributes

## style_property
**value:** *0、1、2*

**default:** *0*

**context:** querystring

Specifies the mode of output CSS property with the value as the following:

  * 0: do not output CSS property
  * 1: output all CSS property
  * 2: output whitelisted CSS property

## style_property_value
**value:** *0、1*

**default:** *0*

**context:** querystring

Specifies the mode of output CSS property_value with the value as the following:

  * 0: do not check the CSS property's value
  * 1: check the CSS property's value for [URL] function and IE's expression function to avoid XSS inject

## url_protocol
**value:** *0、1*

**default:** *0*

**context:** querystring

Specifies whether check the URL protocol at [linkable_attribute]. The value is as the following:

  * 0: do not check the URL protocol
  * 1: output whitelisted URL protocol

## url_domain
**value:** *0、1*

**default:** *0*

**context:** querystring

Specifies whether check the URL domain at [linkable_attribute] when enable [url_protocol] check. The value is  as the following:

  * 0: do not check the URL domain
  * 1: output whitelisted URL domain

## iframe_url_protocol
**value:** *0、1*

**default:** *0*

**context:** querystring

is the same as [url_protocol] but only for `iframe.src`

## iframe_url_domain
**value:** *0、1*

**default:** *0*

**context:** querystring

is the same as [url_domain] but only for `iframe.src`

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-html-sanitize](https://github.com/dvershinin/ngx_http_html_sanitize_module){target=_blank}.

# *iconv*: NGINX iconv module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-iconv
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_iconv_module.so;
```


This document describes nginx-module-iconv [v0.14](https://github.com/calio/iconv-nginx-module/releases/tag/v0.14){target=_blank} 
released on May 15 2016.

<hr />
<!---
Don't edit this file manually! Instead you should generate it by using:
    wiki2markdown.pl doc/manpage.wiki
-->

## Name

iconv-nginx-module

## Description

This is a nginx module that uses libiconv to convert characters of different
encoding. It brings the 'set_iconv' command to nginx.

This module depends on the ngx_devel_kit(NDK) module.

## Usage

## set_iconv

**syntax:** *set_iconv &lt;destination_variable&gt; &lt;from_variable&gt; from=&lt;from_encoding&gt; to=&lt;to_encoding&gt;*

**default:** *none*

**phase:** *rewrite*


## iconv_buffer_size

**syntax:** *iconv_buffer_size &lt;size&gt;*

**default:** *iconv_buffer_size &lt;pagesize&gt;*


## iconv_filter

**syntax:** *iconv_filter from=&lt;from_encoding&gt; to=&lt;to_encoding&gt;*

**default:** *none*

**phase:** *output-filter*

Here is a basic example:

```nginx

 #nginx.conf

 location /foo {
     set $src '你好'; #in UTF-8
     set_iconv $dst $src from=utf8 to=gbk; #now $dst holds 你好 in GBK
 }

 #everything generated from /foo will be converted from utf8 to gbk
 location /bar {
     iconv_filter from=utf-8 to=gbk;
     iconv_buffer_size 1k;
     #content handler here
 }
```


## Changelog

This module's change logs are part of the OpenResty bundle's change logs. Please see
See <http://openresty.org/#Changes>


## See Also

* The [OpenResty](https://openresty.org) bundle.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-iconv](https://github.com/calio/iconv-nginx-module){target=_blank}.

# *image-filter*: NGINX image filter dynamic module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-image-filter
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_image_filter_module.so;
```

<hr />


## Directives

You may find information about configuration directives for this module at the following links:        

*   https://nginx.org/en/docs/http/ngx_http_image_filter_module.html#directives

# *immutable*: NGINX module for setting immutable caching on static assets


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-immutable
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_immutable_module.so;
```


This document describes nginx-module-immutable [v0.0.4](https://github.com/GetPageSpeed/ngx_immutable/releases/tag/v0.0.4){target=_blank} 
released on Nov 21 2022.

<hr />

[![Coverity Scan](https://img.shields.io/coverity/scan/GetPageSpeed-ngx_immutable)](https://scan.coverity.com/projects/GetPageSpeed-ngx_immutable)

This tiny NGINX module can help improve caching of your public static assets, by setting far future expiration with `immutable` attribute.

## Intended audience

Websites and frameworks which rely on the cache-busting pattern:

* static resources include version/hashes in their URLs, while never modifying the resources
* when necessary, updating the resources with newer versions that have new version-numbers/hashes, 
so that their URLs are different

Popular frameworks which use cache-busting:

* Magento 2
* Include your own here! 

## Synopsis

```nginx
http {
    server {
        location /static/ {
            immutable on;
        }
    }
}
```

will yield the following HTTP headers:

```
...
Cache-Control: public,max-age=31536000,stale-while-revalidate=31536000,stale-if-error=31536000,immutable
Expires: Thu, 31 Dec 2037 23:55:55 GMT 
...
```

How it's different to `expires max;`:

* Sets `immutable` attribute, e.g. `Cache-Control: public,max-age=31536000,immutable` for improved caching. 
That is 1 year and not 10 years, see why below.
* Sends `Expires` only when it's really necessary, e.g. when a client is requesting resources over `HTTP/1.0`
* Sets `public` attribute to ensure the assets can be cached by public caches, which is typically a desired thing.

Due to the [lacking support of `immutable`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control#browser_compatibility) in Chromium-based browsers, 
we also add `stale-while-revalidate=31536000,stale-if-error=31536000` which helps to improve cache hit-ratio in edge cases. 
Use of these directives allows serving cached responses beyond their cache lifetime, which is forever in case of immutable resources.

Thus, in most cases, `immutable on;` can be used as a better alternative to `expires max;` to implement the cache-busting pattern.

### Why 31536000 seconds (1 year?)

The [RFC](https://www.ietf.org/rfc/rfc2616.txt) defines to use one year to make a response as "never expires":

> To mark a response as “never expires,” an origin server sends an
> Expires date approximately one year from the time the response is
> sent. HTTP/1.1 servers SHOULD NOT send Expires dates more than one
> year in the future.

More details in [the article](https://ashton.codes/set-cache-control-max-age-1-year/).

## Example: Magento 2 production configuration

Provided that your store runs in production mode, you have already compiled all the assets.
This [sample config](https://github.com/magento/magento2/blob/2.3.4/nginx.conf.sample#L103-L134) can be optimized to:

```nginx
location /static/ {
    immutable on;

    # Remove signature of the static files that is used to overcome the browser cache
    location ~ ^/static/version {
        rewrite ^/static/(version\d*/)?(.*)$ /static/$2 last;
    }

    location ~* \.(ico|jpg|jpeg|png|gif|svg|js|css|swf|eot|ttf|otf|woff|woff2|json)$ {
        add_header X-Frame-Options "SAMEORIGIN";
    }
    location ~* \.(zip|gz|gzip|bz2|csv|xml)$ {
        add_header Cache-Control "no-store";
        add_header X-Frame-Options "SAMEORIGIN";
        immutable off;
    }
    add_header X-Frame-Options "SAMEORIGIN";
}
```

When used together with [`ngx_security_headers`](https://github.com/GetPageSpeed/ngx_security_headers), it can be simplified further:

```nginx
security_headers on;

location /static/ {
    immutable on;

    
    location ~ ^/static/version {
        rewrite ^/static/(version\d*/)?(.*)$ /static/$2 last;
    }

    location ~* \.(zip|gz|gzip|bz2|csv|xml)$ {
        add_header Cache-Control "no-store";
        immutable off;
    }
}
```

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-immutable](https://github.com/GetPageSpeed/ngx_immutable){target=_blank}.

# *ipscrub*: IP address anonymizer module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-ipscrub
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_ipscrub_module.so;
```


This document describes nginx-module-ipscrub [v1.0.1](https://github.com/masonicboom/ipscrub/releases/tag/v1.0.1){target=_blank} 
released on May 29 2018.

<hr />

`ipscrub` is an IP address anonymizer for [nginx](https://www.nginx.com) log files. It's an nginx module that generates an IP-based hash. You can use this hash to link requests from the same source, without identifying your users by IP address.

![Screenshot of nginx logs when using ipscrub](demo/demo.png)

## Security Model

1. On initialization, and again every `PERIOD`, generate `salt` using 128bits from `arc4random_buf()`.
2. On each request, generate masked IP address as `HASH(salt ++ IP address)`.
3. Log masked IP address.

`ipscrub` uses `arc4random` to generate random nonces (see [Theo de Raat's talk on arc4random](https://www.youtube.com/watch?v=aWmLWx8ut20) for a great overview). On Linux this requires installing [libbsd](https://libbsd.freedesktop.org/wiki/) (package libbsd-dev on Ubuntu/Debian). 

ALSO NOTE: the generated hash WILL change on each `PERIOD` transition, so you will only have continuity within each `PERIOD`. But because users can transition between networks at any time (e.g. wifi -> cellular), you'd have this type of issue even if you were storing raw IPs.

## Threat Model

1. Government presents you with an IP address and demands identification of user corresponding to that address.
2. Government identifies a user e.g. by email address, and demands IP address they had at some point in time.

In threat scenario (1), the goal is to compute the masked IP corresponding to a target IP address. This will only be possible if the demand is made before the end of the current `PERIOD`.

Scenario (2) is defended against because the server operator does not know the salt, and cannot infer it based on the request timestamp, because the salt is generated from a nonce that is only stored in memory. The server operator would have to be an accomplice in this case, but that is more simply accomplished by the server operator just recording the unmasked IP. So this security/threat model does not defend against a malicious server operator, but that is not the point. It does defend against an honest server operator being compelled in threat scenarios (1) and (2).

## Usage

### Configuration

In your `nginx.conf`,

1. At the top-level, load the module by adding the line `load_module ngx_ipscrub_module.so;` (NOTE: only if you built as a dynamic module).
1. Set `ipscrub_period_seconds <NUM SECONDS PER PERIOD>;` (optional).
1. In your `log_format` directives, replace `$remote_addr` with `$remote_addr_ipscrub`.
1. Reload your nginx config.

**NOTE**: nginx may still leak IP addresses in the error log. If this is a concern, disable error logging or wipe the log regularly.

### Running Tests

`make test`

### Changelog

- 1.0.1 fixed vulnerability to unmasking hashed IPs (thanks to [@marcan](https://github.com/marcan))
- 1.0.0 initial release

## GDPR

[GDPR](https://www.eugdpr.org) goes into effect on May 25, 2018. It legislates the handling of personal data about your users, including IP addresses.

From https://www.eugdpr.org/gdpr-faqs.html:

    What constitutes personal data?

    Any information related to a natural person or ‘Data Subject’, that can be used to directly or indirectly identify the person. It can be anything from a name, a photo, [...], or a computer IP address.

The hashes generated by `ipscrub` let you correlate nginx log entries by IP address, without actually storing IP addresses, reducing your GDPR surface area.

## YAGNI

Why are you logging IP addresses anyway? [You Ain't Gonna Need It](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it). If you want geolocation, just use [MaxMind's GeoIP module](https://nginx.org/en/docs/http/ngx_http_geoip_module.html) in conjunction with `ipscrub`.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-ipscrub](https://github.com/masonicboom/ipscrub){target=_blank}.

# *jpeg*: NGINX JPEG filter module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-jpeg
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_jpeg_filter_module.so;
```


This document describes nginx-module-jpeg [v1.0.1](https://github.com/ioppermann/modjpeg-nginx/releases/tag/v1.0.1){target=_blank} 
released on Sep 04 2018.

<hr />

Nginx filter module for adding overlays on JPEGs on-the-fly with [libmodjpeg](https://github.com/ioppermann/libmodjpeg).

> With libmodjpeg you can overlay a (masked) image onto an existing JPEG as lossless as possible. Changes in the JPEG only
> take place where the overlayed image is applied. All modifications happen in the DCT domain, thus the JPEG is decoded and
> encoded losslessly.

- [Typical Uses](#typical-uses)
- [Try it out](#try-it-out)
- [Installation](#installation)
- [Compatibility](#compatibility)
- [Synopsis](#synopsis)
- [Directives](#directives)
  - [jpeg_filter](#jpeg_filter)
  - [jpeg_filter_max_pixel](#jpeg_filter_max_pixel)
  - [jpeg_filter_buffer](#jpeg_filter_buffer)
  - [jpeg_filter_optimize](#jpeg_filter_optimize)
  - [jpeg_filter_progressive](#jpeg_filter_progressive)
  - [jpeg_filter_arithmetric](#jpeg_filter_arithmetric)
  - [jpeg_filter_graceful](#jpeg_filter_graceful)
  - [jpeg_filter_effect](#jpeg_filter_effect)
  - [jpeg_filter_dropon_align](#jpeg_filter_dropon_align)
  - [jpeg_filter_dropon_offset](#jpeg_filter_dropon_offset)
  - [jpeg_filter_dropon_file](#jpeg_filter_dropon_file)
  - [jpeg_filter_dropon_memory](#jpeg_filter_dropon_memory)
  - [Notes](#notes)
- [License](#license)
- [Acknowledgement](#acknowledgement)


## Typical Uses

This filter module can add overlays (e.g. a logo, visual watermark) on JPEGs when they are requested.

A few ideas:

- Consider you are a photographer and have a image gallery on your website. Without hardcoding your logo (brand, watermark, ...) into these images you can apply it the moment the image is requested. Whenever you update your logo, just update the nginx configuration and it's done. No need to re-process all your images.
- You have an online shop with thousands of product images. With just configuring nginx you can add your logo to all of the product images. You don't have to process all product images.
- You have a paid service. Add a watermark to all images if the user is not subscribed. If the user is subscribed, don't apply the watermark or put just a small logo on the images without touching the original images.
- On your website, registered users can upload images. Add the avatar of the user to the image who uploaded the image without processing it after the upload. If the user changes her avatar, all her images will automatically have the new avatar on them.


## Try it out

In order to try out this filter module, pull the docker image

```bash
docker pull ioppermann/modjpeg-nginx:latest
```

The docker container exposes TCP port 80 and expects a directory with images mounted on `/images`, e.g.

```bash
docker run -it --rm --name=modjpeg-nginx \
   --mount type=bind,src=$PWD/images,dst=/images,readonly \
   -p 8080:80 \
   ioppermann/modjpeg-nginx:latest
```

Now you can browse to [http://localhost:8080/](http://localhost:8080/) and click on the listed images. The modjpeg logo will be applied in the top left corner. By default
only images that are smaller than 10MB are processed by the filter. Stop the container by pressing `Ctrl-c`.

The filter can be controlled by these environment variables:

|Name|Default|Description|
|----|-------|-----------|
|MJ_GRACEFUL|on|See [jpeg_filter_graceful](#jpeg_filter_graceful)|
|MJ_BUFFER|10M|See [jpeg_filter_buffer](#jpeg_filter_buffer)|
|MJ_MAX_PIXEL|0|See [jpeg_filter_max_pixel](#jpeg_filter_max_pixel)|
|MJ_DROPON_ALIGN|"top left"|See [jpeg_filter_dropon_align](#jpeg_filter_dropon_align)|
|MJ_DROPON_OFFSET|"0 0"|See [jpeg_filter_dropon_offset](#jpeg_filter_dropon_offset)|
|MJ_DROPON_FILE|"/usr/local/nginx/conf/dropon.png"|See [jpeg_filter_dropon_file](#jpeg_filter_dropon_file)|

The following example will allow images with up to 150 megapixel (`MJ_MAX_PIXEL`) and 100MB in file size (`MJ_BUFFER`). The logo will be placed in bottom right corner (`MJ_DROPON_ALIGN`)
with an offset of -15px horizontally and vertically (`MJ_DROPON_OFFSET`).

```bash
docker run -it --rm --name=modjpeg-nginx \
   --mount type=bind,src=$PWD/images,dst=/images,readonly \
   -p 8080:80 \
   -e MJ_MAX_PIXEL=150000000 \
   -e MJ_BUFFER=100M \
   -e MJ_DROPON_ALIGN="bottom right" \
   -e MJ_DROPON_OFFSET="-15 -15" \
   ioppermann/modjpeg-nginx:latest
```

In order to change the logo, you can mount an additional volume or put it into the directory you already mount, e.g.

```bash
docker run -it --rm --name=modjpeg-nginx \
   --mount type=bind,src=$PWD/images,dst=/images,readonly \
   -p 8080:80 \
   -e MJ_DROPON_FILE="/images/logo.png" \
   ioppermann/modjpeg-nginx:latest
```


## Clone and install libmodjpeg
git clone https://github.com/ioppermann/libmodjpeg.git
cd libmodjpeg
cmake .
make
make install
cd ..

## Clone modjpeg-nginx
git clone https://github.com/ioppermann/modjpeg-nginx.git

## Download and install nginx
wget 'http://nginx.org/download/nginx-1.15.1.tar.gz'
tar -xvzf nginx-1.15.1.tar.gz
cd nginx-1.15.1

## Configure as static module, or ...
./configure --add_module=../modjpeg-nginx

## ... configure as dynamic module (as of nginx 1.9.11)
./configure --add_dynamic_module=../modjpeg-nginx

## If the libmodjpeg library is not found, add e.g. '--with-ld-opt=-L/usr/local/lib' to
## the configure options if it was installed to /usr/local/lib

## You may want to use the other './configure' options that are used
## in your current nginx build. Check the output of 'nginx -V'.

make
make install
```

If you configured modjpeg-nginx as dynamic module, you have to load the module in the beginning of the config

```nginx
...
load_module modules/ngx_http_jpeg_filter_module.so;
...
```


## Synopsis

```nginx
   ...

   location /gallery {
      # enable jpeg filter module
      jpeg_filter on;

      # limit image sizes to 9 megapixel
      jpeg_filter_max_pixel 9000000;

      # limit image file size to 5 megabytes
      jpeg_filter_buffer 5M;

      # deliver the images unmodified if one of the limits apply
      jpeg_filter_graceful on;

      # pixelate the image
      jpeg_filter_effect pixelate;

      # add a masked logo in the bottom right corner
      # with a distance of 10 pixel from the border
      jpeg_filter_dropon_align bottom right;
      jpeg_filter_dropon_offset -10 -10;
      jpeg_filter_dropon_file /path/to/logo.jpg /path/to/mask.jpg;
   }

   ...
```

Or use it with [OpenResty's ngx_http_lua_module](https://github.com/openresty/lua-nginx-module) and a PNG logo:

```nginx
   ...

   location /gallery {
      set_by_lua_block $valign {
         local a = { 'top', 'center', 'bottom' }
         return a[math.random(#a)]
      }

      set_by_lua_block $halign {
         local a = { 'left', 'center', 'right' }
         return a[math.random(#a)]
      }

      # enable jpeg filter module
      jpeg_filter on;

      # limit image sizes to 9 megapixel
      jpeg_filter_max_pixel 9000000;

      # limit image file size to 5 megabytes
      jpeg_filter_buffer 5M;

      # deliver the images unmodified if one of the limits apply
      jpeg_filter_graceful on;

      # pixelate the image
      jpeg_filter_effect pixelate;

      # add a logo in a random position
      jpeg_filter_dropon_align $valign $halign;
      jpeg_filter_dropon_file /path/to/logo.png;
   }

   ...
```

Or generate a logo with [Lua-GD](http://ittner.github.io/lua-gd/):

```nginx
http {
   ...
   ...
   server {
      ...
      location /gallery {
      	   set_by_lua_block $logobytestream {
              local gd = require "gd"

              local im = gd.create(210, 70)
              local white = im:colorAllocate(255, 255, 255)
              local black = im:colorAllocate(0, 0, 0)
              im:filledRectangle(0, 0, 140, 80, white)
              im:string(gd.FONT_LARGE, 10, 10, "Hello modjpeg", black)
              im:string(gd.FONT_LARGE, 10, 40, os.date("%c"), black);
              return im:jpegStr(85)
      	   }

   	   # enable jpeg filter module
   	   jpeg_filter on;

           # limit image sizes to 9 megapixel
           jpeg_filter_max_pixel 9000000;

           # limit image file size to 5 megabytes
           jpeg_filter_buffer 5M;

           # deliver the images unmodified if one of the limits apply
           jpeg_filter_graceful on;

           # pixelate the image
           jpeg_filter_effect pixelate;

           # add a generated logo in the bottom right corner
           # with a distance of 10 pixel from the border
           jpeg_filter_dropon_align bottom right;
           jpeg_filter_dropon_offset -10 -10;
           jpeg_filter_dropon_memory $logobytestream;
      }
      ...
   }
   ...
}
```

## Directives

- [jpeg_filter](#jpeg_filter)
- [jpeg_filter_max_pixel](#jpeg_filter_max_pixel)
- [jpeg_filter_buffer](#jpeg_filter_buffer)
- [jpeg_filter_optimize](#jpeg_filter_optimize)
- [jpeg_filter_progressive](#jpeg_filter_progressive)
- [jpeg_filter_arithmetric](#jpeg_filter_arithmetric)
- [jpeg_filter_graceful](#jpeg_filter_graceful)
- [jpeg_filter_effect](#jpeg_filter_effect)
- [jpeg_filter_dropon_align](#jpeg_filter_dropon_align)
- [jpeg_filter_dropon_offset](#jpeg_filter_dropon_offset)
- [jpeg_filter_dropon_file](#jpeg_filter_dropon_file)
- [jpeg_filter_dropon_memory](#jpeg_filter_dropon_memory)
- [Notes](#notes)


### jpeg_filter

__Syntax:__ `jpeg_filter on | off`

__Default:__ `jpeg_filter off`

__Context:__ `location`

Enable the jpeg filter module.

This directive is turned off by default.


### jpeg_filter_max_pixel

__Syntax:__ `jpeg_filter_max_pixel pixel`

__Default:__ `0`

__Context:__ `http, server, location`

Maximum number of pixel in image to operate on. If the image has more pixel (width * height) than `pixel`, the jpeg filter will return a  "415 Unsupported Media Type".
Set [jpeg_filter_graceful](#jpeg_filter_graceful) to `on` to deliver the image unchanged. Set the maximum pixel to 0 in order ignore the image dimensions.

This directive is set to 0 by default.


### jpeg_filter_buffer

__Syntax:__ `jpeg_filter_buffer size`

__Default:__ `2M`

__Context:__ `http, server, location`

The maximum file size of the image to operate on. If the file size if bigger than `size`, the jpeg filter will return a "415 Unsupported Media Type".
Set [jpeg_filter_graceful](#jpeg_filter_graceful) to `on` to deliver the image unchanged.

This directive is set to 2 megabyte by default.


### jpeg_filter_optimize

__Syntax:__ `jpeg_filter_optimize on | off`

__Default:__ `off`

__Context:__ `http, server, location`

Upon delivery, optimize the Huffman tables of the image.

This directive is turned off by default.


### jpeg_filter_progressive

__Syntax:__ `jpeg_filter_progressive on | off`

__Default:__ `off`

__Context:__ `http, server, location`

Upon delivery, enable progressive encoding of the image.

This directive is turned off by default.


### jpeg_filter_arithmetric

__Syntax:__ `jpeg_filter_arithmetric on | off`

__Default:__ `off`

__Context:__ `http, server, location`

Upon delivery, enable arithmetric encoding of the image.
This will override the [jpeg_filter_optimize](#jpeg_filter_optimize) directive.
Arithmetric encoding is usually not supported by browsers.

This directive is turned off by default.


### jpeg_filter_graceful

__Syntax:__ `jpeg_filter_graceful on | off`

__Default:__ `off`

__Context:__ `http, server, location`

Allow to deliver the unchanged image in case the directives [jpeg_filter_max_width](#jpeg_filter_max_width), [jpeg_filter_max_height](#jpeg_filter_max_height), or [jpeg_filter_buffer](#jpeg_filter_buffer) would return a "415 Unsupported Media Type" error.

This directive is turned off by default.


### jpeg_filter_effect

__Syntax:__ `jpeg_filter_effect grayscale | pixelate`

__Syntax:__ `jpeg_filter_effect darken | brighten value`

__Syntax:__ `jpeg_filter_effect tintblue | tintyellow | tintred | tintgreen value`

__Default:__ `-`

__Context:__ `location`

Apply an effect to the image.

`grayscale` will remove all color components from the image. This only applies to images in the YCbCr color space.

`pixelate` will pixelate the image in blocks of 8x8 pixel by setting the AC coefficients in all components to 0.

`darken` will darken the image by decreasing the DC coefficients in the Y component by `value`. This only applies to images in the YCbCr color space.

`brighten` will brighten the image by increasing the DC coefficients in the Y component by `value`. This only applies to images in the YCbCr color space.

`tintblue` will tint the image blue by increasing the DC coefficients in the Cb component by `value`. This only applies to images in the YCbCr color space.

`tintyellow` will tint the image blue by decreasing the DC coefficients in the Cb component by `value`. This only applies to images in the YCbCr color space.

`tintred` will tint the image red by increasing the DC coefficients in the Cr component by `value`. This only applies to images in the YCbCr color space.

`tintgreen` will tint the image green by decreasing the DC coefficients in the Cr component by `value`. This only applies to images in the YCbCr color space.

This directive is not set by default.

All parameters can contain variables.


### jpeg_filter_dropon_align

__Syntax:__ `jpeg_filter_dropon_align [top | center | bottom] [left | center | right]`

__Default:__ `center center`

__Context:__ `location`

Align the dropon on the image. Use the directive [jpeg_filter_dropon_offset](#jpeg_filter_dropon_offset) to offset the dropon from the alignment.

This directive must be set before [jpeg_filter_dropon](#jpeg_filter_dropon) in order to have an effect on the dropon.

This directive will apply the dropon in the center of the image by default.

All parameters can contain variables.


### jpeg_filter_dropon_offset

__Syntax:__ `jpeg_filter_dropon_offset vertical horizontal`

__Default:__ `0 0`

__Context:__ `location`

Offset the dropon by `vertical` and `horizontal` pixels from the alignment given with the [jpeg_filter_dropon_align](#jpeg_filter_dropon_align) directive.
Use a negative value to move the dropon up or left and a positive value to move the dropon down or right.

This directive must be set before [jpeg_filter_dropon](#jpeg_filter_dropon) in order to have an effect on the dropon.

This directive will not apply an offset by default.

All parameters can contain variables.


### jpeg_filter_dropon_file

__Syntax:__ `jpeg_filter_dropon_file image`

__Syntax:__ `jpeg_filter_dropon_file image mask`

__Default:__ `-`

__Context:__ `location`

Apply a dropon to the image. The dropon is given by a path to a JPEG or PNG image for `image` and optionally a path to a JPEG image for `mask`. If no mask image is
provided, the image will be applied without transcluency. If a mask image is provided, only the luminance component will be used. For the mask, black means
fully transcluent and white means fully opaque. Any values inbetween will blend the underlying image and the dropon accordingly. If `image` is a path to a PNG, the
mask will be ignored.

This directive is not set by default.

All parameters can contain variables.

If none of the parameters contain variables, the dropon is loaded during loading of the configuration. If at least one parameter contains variables, the dropon
will be loaded during processing of the request. After processing the request, the dropon will be unloaded.

PNG files as dropon are supported only if libmodjpeg has been compiled with PNG support.


### jpeg_filter_dropon_memory

__Syntax:__ `jpeg_filter_dropon_memory $image`

__Syntax:__ `jpeg_filter_dropon_memory $image $mask`

__Default:__ `-`

__Context:__ `location`

Apply a dropon to the image. The dropon is given by a variable holding a JPEG or PNG image bytestream for `$image` and optionally a variable to a JPEG image bytestream for `$mask`.
If no mask image is provided, the image will be applied without transcluency. If a mask image is provided, only the luminance component will be used. For the mask,
black means fully transcluent and white means fully opaque. Any values inbetween will blend the underlying image and the dropon accordingly. If `$image` is a PNG, the
mask will be ignored.

This directive is not set by default.

All parameters are expected to be variables.

The dropon will always be loaded during processing of the request. After processing the request, the dropon will be unloaded.

PNG bytestreams as dropon are supported only if libmodjpeg has been compiled with PNG support.


### Notes

The directives `jpeg_filter_effect`, `jpeg_filter_dropon_align`, `jpeg_filter_dropon_offset`, and `jpeg_filter_dropon` are applied in the order they
appear in the nginx config file, i.e. it makes a difference if you apply first an effect and then add a dropon or vice versa. In the former case the dropon will be
unaffected by the effect and in the latter case the effect will be also applied on the dropon.


## Acknowledgement

This module is heavily inspired by the nginx image filter module with
insights from
["Emiller’s Guide To Nginx Module Development"](https://www.evanmiller.org/nginx-modules-guide.html)
and the
[nginx development guide](https://nginx.org/en/docs/dev/development_guide.html).

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-jpeg](https://github.com/ioppermann/modjpeg-nginx){target=_blank}.

# *js-challenge*: NGINX Javascript challenge module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-js-challenge
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_js_challenge_module.so;
```


This document describes nginx-module-js-challenge [v0.0.1](https://github.com/dvershinin/ngx_http_js_challenge_module/releases/tag/v0.0.1){target=_blank} 
released on Jun 27 2022.

A request for stable release exists. Vote up [here](https://github.com/simon987/ngx_http_js_challenge_module/issues/4).
<hr />

## ngx_http_js_challenge_module

![GitHub](https://img.shields.io/github/license/simon987/ngx_http_js_challenge_module.svg)
[![CodeFactor](https://www.codefactor.io/repository/github/simon987/ngx_http_js_challenge_module/badge)](https://www.codefactor.io/repository/github/simon987/ngx_http_js_challenge_module)


[Demo website](https://ngx-js-demo.simon987.net/)

Simple javascript proof-of-work based access for Nginx with virtually no overhead.

Easy installation: just add `load_module /path/to/ngx_http_js_challenge_module.so;` to your
`nginx.conf` file and follow the [configuration instructions](#configuration).

<p align="center">
  <img width="600px" src="throughput.png"/>
</p>

### Configuration

**Simple configuration**
```nginx
server {
    js_challenge on;
    js_challenge_secret "change me!";

    # ...
}
```


**Advanced configuration**
```nginx
server {
    js_challenge on;
    js_challenge_secret "change me!";
    js_challenge_html /path/to/body.html;
    js_challenge_bucket_duration 3600;
    js_challenge_title "Verifying your browser...";

    location /static {
        js_challenge off;
        alias /static_files/;
    }

    location /sensitive {
        js_challenge_bucket_duration 600;
        #...
    }

    #...
}
```

* `js_challenge on|off` Toggle javascript challenges for this config block
* `js_challenge_secret "secret"` Secret for generating the challenges. DEFAULT: "changeme"
* `js_challenge_html "/path/to/file.html"` Path to html file to be inserted in the `<body>` tag of the interstitial page
* `js_challenge_title "title"` Will be inserted in the `<title>` tag of the interstitial page. DEFAULT: "Verifying your browser..."
* `js_challenge_bucket_duration time` Interval to prompt js challenge, in seconds. DEFAULT: 3600

### Build from source

These steps have to be performed on machine with compatible configuration (same nginx, glibc, openssl version etc.)

1. Install dependencies
    ```bash
    apt install libperl-dev libgeoip-dev libgd-dev libxslt1-dev libpcre3-dev
    ```
2. Download nginx tarball corresponding to your current version (Check with `nginx -v`)
    ```bash
   wget https://nginx.org/download/nginx-1.16.1.tar.gz
   tar -xzf nginx-1.16.1.tar.gz
   export NGINX_PATH=$(pwd)/nginx-1.16.1/
    ```
3. Compile the module
    ```bash
    git clone https://github.com/simon987/ngx_http_js_challenge_module
    cd ngx_http_js_challenge_module
    ./build.sh
    ```
4. The dynamic module can be found at `${NGINX_PATH}/objs/ngx_http_js_challenge_module.so`



### Known limitations / TODO

* Users with cookies disabled will be stuck in an infinite refresh loop (TODO: redirect with a known query param, if no cookie is specified but the query arg is set, display an error page)
* If nginx is behind a reverse proxy/load balancer, the same challenge will be sent to different users and/or the response cookie will be invalidated when the user is re-routed to another server. (TODO: use the x-real-ip header when available) 

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-js-challenge](https://github.com/dvershinin/ngx_http_js_challenge_module){target=_blank}.

# *json-var*: NGINX JSON variables module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-json-var
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_json_var_module.so;
```


This document describes nginx-module-json-var [v1.1](https://github.com/dvershinin/nginx-json-var-module/releases/tag/v1.1){target=_blank} 
released on Feb 11 2022.

<hr />

Adds the ability to group Nginx variable expressions as json.

## Configuration

### json_var
* **syntax**: `json_var $variable { ... }`
* **default**: `none`
* **context**: `http`

Creates a new variable whose value is a json containing the items listed within the block.
Parameters inside the `json_var` block specify a field that should be included in the resulting json.
Each parameter has to contain two arguments - key and value. 
The value can contain nginx variables.

## Sample configuration
```nginx
http {
	json_var $output {
		timestamp $time_local;
		remoteAddr $remote_addr;
		xForwardedFor $http_x_forwarded_for;
		userAgent $http_user_agent;
		params $args;
	}
	
	server {
		location /get_json/ {
			return 200 $output;
		}
	}
```
Hitting `http://domain/get_json/?key1=value1&key2=value2` can return a json like:
```
{
	"timestamp": "21/Jul/2017:12:44:18 -0400",
	"remoteAddr": "127.0.0.1",
	"xForwardedFor": "",
	"userAgent": "curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3",
	"params": "key1=value1&key2=value2"
}
```

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-json-var](https://github.com/dvershinin/nginx-json-var-module){target=_blank}.

# *[BETA!] json*: NGINX JSON module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-json
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_json_module.so;
```


This document describes nginx-module-json [v0](https://github.com/dvershinin/ngx_http_json_module/releases/tag/v0){target=_blank} 
released on Dec 16 2023.

Production stability is *not guaranteed*.
<hr />

### Directives:

    Syntax:	 json_load $json string;
    Default: ——
    Context: http, server, location

Loads string (may contains variables) into (json) variable $json.

    Syntax:	 json_dump $string $json [name ...];
    Default: ——
    Context: http, server, location

Dumps (json) variable $json into (string) variable $string (may point path by names).

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-json](https://github.com/dvershinin/ngx_http_json_module){target=_blank}.

# *jwt*: NGINX JWT Module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-jwt
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_auth_jwt_module.so;
```


This document describes nginx-module-jwt [v3.4.0](https://github.com/max-lt/nginx-jwt-module/releases/tag/v3.4.0){target=_blank} 
released on Nov 01 2023.

<hr />
[github-license-url]: /blob/master/LICENSE
[action-docker-url]: https://github.com/max-lt/nginx-jwt-module/actions/workflows/docker.yml
[github-container-url]: https://github.com/max-lt/nginx-jwt-module/pkgs/container/nginx-jwt-module

## Nginx jwt auth module
[![License](https://img.shields.io/github/license/maxx-t/nginx-jwt-module.svg)][github-license-url]

This is an NGINX module to check for a valid JWT.

Inspired by [TeslaGov](https://github.com/TeslaGov/ngx-http-auth-jwt-module), [ch1bo](https://github.com/ch1bo/nginx-jwt) and [tizpuppi](https://github.com/tizpuppi/ngx_http_auth_jwt_module), this module intend to be as light as possible and to remain simple.
 - Docker image based on the [official nginx Dockerfile](https://github.com/nginxinc/docker-nginx) (alpine).
 - Light image (~10MB).

### Module:

#### Example Configuration:
```nginx
## nginx.conf
load_module /usr/lib/nginx/modules/ngx_http_auth_jwt_module.so;
```

```nginx
## server.conf
server {
    auth_jwt_key "0123456789abcdef" hex; # Your key as hex string
    auth_jwt     off;

    location /secured-by-cookie/ {
        auth_jwt $cookie_MyCookieName;
    }

    location /secured-by-auth-header/ {
        auth_jwt on;
    }

    location /secured-by-auth-header-too/ {
        auth_jwt_key "another-secret"; # Your key as utf8 string
        auth_jwt on;
    }

    location /secured-by-rsa-key/ {
        auth_jwt_key /etc/keys/rsa-public.pem file; # Your key from a PEM file
        auth_jwt on;
    }

    location /not-secure/ {}
}
```

> Note: don't forget to [load](http://nginx.org/en/docs/ngx_core_module.html#load_module) the module in the main context: <br>`load_module /usr/lib/nginx/modules/ngx_http_auth_jwt_module.so;`

### Directives:

#### auth_jwt

    Syntax:	 auth_jwt $variable | on | off;
    Default: auth_jwt off;
    Context: http, server, location

Enables validation of JWT.

The `auth_jwt $variable` value can be used to set a custom way to get the JWT, for example to get it from a cookie instead of the default `Authentication` header: ` auth_jwt $cookie_MyCookieName;`

<hr>

#### auth_jwt_key

    Syntax:	 auth_jwt_key value [encoding];
    Default: ——
    Context: http, server, location

Specifies the key for validating JWT signature (must be hexadecimal).<br>
The *encoding* otpion may be `hex | utf8 | base64 | file` (default is `utf8`).<br>
The `file` option requires the *value* to be a valid file path (pointing to a PEM encoded key).

<hr>

#### auth_jwt_alg

    Syntax:	 auth_jwt_alg any | HS256 | HS384 | HS512 | RS256 | RS384 | RS512 | ES256 | ES384 | ES512;
    Default: auth_jwt_alg any;
    Context: http, server, location

Specifies which algorithm the server expects to receive in the JWT.

<hr>

#### auth_jwt_require

    Syntax:	 auth_jwt_require $value ... [error=401 | 403];
    Default: ——
    Context: http, server, location

Specifies additional checks for JWT validation. The authentication will succeed only if all the values are not empty and are not equal to “0”.

These directives are inherited from the previous configuration level if and only if there are no auth_jwt_require directives defined on the current level.

If any of the checks fails, the 401 error code is returned. The optional error parameter allows redefining the error code to 403.

Example:
```nginx
## server.conf

map $jwt_claim_role $jwt_has_admin_role {
    \"admin\"  1;
}

map $jwt_claim_scope $jwt_has_restricted_scope {
    \"restricted\"  1;
}

server {
  # ...

  location /auth-require {
    auth_jwt_require $jwt_has_admin_role error=403;
    # ...
  }

  location /auth-compound-require {
    auth_jwt_require $jwt_has_admin_role $jwt_has_restricted_scope error=403;
    # ...
  }
}
```

> Note that as `$jwt_claim_` returns a JSON-encoded value, we check form `\"value\"` (and not  `value`)

### Embedded Variables:
The ngx_http_auth_jwt_module module supports embedded variables:
- $jwt_header_*name* returns the specified header value
- $jwt_claim_*name* returns the specified claim value
- $jwt_headers returns headers
- $jwt_payload returns payload

> Note that as all returned values are JSON-encoded, so string will be surrounded by `"` character

### Image:
Image is generated with Github Actions (see [nginx-jwt-module:latest][github-container-url])

```
docker pull ghcr.io/max-lt/nginx-jwt-module:latest
```

#### Simply create your image from Github's generated one
```dockerfile
FROM ghcr.io/max-lt/nginx-jwt-module:latest

## Copy you nginx conf
## Don't forget to include this module in your configuration
## load_module /usr/lib/nginx/modules/ngx_http_auth_jwt_module.so;
COPY my-nginx-conf /etc/nginx

EXPOSE 8000

STOPSIGNAL SIGTERM

CMD ["nginx", "-g", "daemon off;"]
```

## or
docker build -f Dockerfile -t jwt-nginx .
```

### Test:

#### Default usage:
```bash
make test # Will build a test image & run test suite
```

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-jwt](https://github.com/max-lt/nginx-jwt-module){target=_blank}.

# *length-hiding*: NGINX Length Hiding Filter Module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-length-hiding
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_length_hiding_filter_module.so;
```


This document describes nginx-module-length-hiding [v1.1.1](https://github.com/nulab/nginx-length-hiding-filter-module/releases/tag/1.1.1){target=_blank} 
released on Jul 19 2017.

<hr />

## Nginx Length Hiding Filter Module


## Introduction

In [BREACH site](http://breachattack.com/), the mitigations against BREACH attack are given as follows:

1. Disabling HTTP compression
2. Separating secrets from user input
3. Randomizing secrets per request
4. Masking secrets (effectively randomizing by XORing with a random secret per request)
5. Protecting vulnerable pages with CSRF
6. Length hiding (by adding random number of bytes to the responses)
7. Rate-limiting the requests

BREACH relies on HTTP compression and it's reasonable to disable it to secure your website. However without compresseion, some websites may meet severe performance degression or the cost may increase if you're charged based on the volume of traffic like AWS. In such case it may be difficult to turn off HTML compression for whole responses from your website and need to adopt other proper ways.

Other mitigations listed from the 2nd to 5th above are basically applicable to your application but the 6th one, Length hiding, can be done on nginx. This filter module provides functionality to append randomly generated HTML comment to the end of response body to hide correct response length and make it difficult for attackers to guess secure token.

The sample of randomly appended HTML comment is here.
```
<!-- random-length HTML comment: JnSLGWeWYWsoJ4dXS3ubLw3YOu3zfGTotlzx7UJUo26xuXICQ2cbpVy1Dprgv8Icj6QfOZx2Ptp9HxCVoevTxhKzMzV6xeYXao0oCngRWJRb4Tvive1iBAXLzrHlLg6jKwNKXrct0tJuA2TvWIRVIng6UoffIbCQLPbi63PwmWemOxVi6m3CPa6hCbAK2CaBR1jLux7UJa4WNN4H0yIDMElMglWWouY5m5FUqAn0afMmtErj0zkA2LMWxisZRES38XLoYycySmaBrIih5IixUsJFR0ei4uZ0IifgV5SnitoNzMusSQem9npObHuU2HKApneAjwnFdPSQZA9sRdSOE8agDI05P832mV1JIcOjsg0FgzxvSG7UEX0HdqBqp2jPOYYW0k5gGtmkiXWydRJfn9lGomxReUeqq2Aec69gplEM6a8aqH5TFgXrGK8jcaPISQlsKkMxJQ7Fp6fVDbmI59xCIvlk -->
```
For every response, length of the random strings will vary within a given range.

This idea originally came from [breach-mitigation-rails](https://github.com/meldium/breach-mitigation-rails/). Thanks team!

## Warning

As said in breach-migration-rails, BREACH is complicated and wide-ranging attack and this module provides only PARTIAL protection. To secure your website or service wholly, you need to review BREACH paper and find proper way according to your own website or service.

## Configuration Directives


### length_hiding

* syntax: length_hiding on | off
* default: off
* context: http, server, location, if in location

Enables or disables adding random generated HTML comment.

### length_hiding_max

* syntax: length_hiding_max size
* default: 2048
* context: http, server, location

Sets maximum length of random generated string used in HTML comment. The size should be within a range from 256 and 2048.

## Example Configuration

Enable this module for specific location ('/hiding'). In this example, the length of random strings will be less than 1024.
```nginx
server {
    listen       443 default_server deferred ssl spdy;
    server_name  example.com;
    length_hiding_max 1024;

    location /hiding {
        length_hiding on;
    }
}
```

If this module is built as dynamic module, do NOT forget including `load_module` line in nginx configuration.
```
load_module modules/ngx_http_length_hiding_filter_module.so;
```

## Services using this module

* [Cacoo](https://cacoo.com/)
* [Backlog](https://backlogtool.com/)
* [Typetalk](https://typetalk.in/)
* [Nulab Account](https://apps.nulab-inc.com/)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-length-hiding](https://github.com/nulab/nginx-length-hiding-filter-module){target=_blank}.

# *let*: NGINX let module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-let
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_let_module.so;
```


This document describes nginx-module-let [v0.0.5](https://github.com/dvershinin/nginx-let-module/releases/tag/v0.0.5){target=_blank} 
released on Jan 27 2023.

<hr />
----------------
## NGINX let module

Adds support for arithmetic operations to NGINX config.

(c) 2011 Roman Arutyunyan, arut@qip.ru



## Examples:

## adds variable $value equal to evaluated expression value

let $value ( $uid + 0x12 ) * $offset - 100 ;

let $remainer $number % 100 ;

let $welcome "Hi, " . $user . ", you have " . $num . " data items";
## echo $welcome ;

let_rand $randval from to;


IMPORTANT NOTE:

let-module uses NGINX config parser as lexer.
That means you should add spaces around each token.

let $value (1+2);             # ERROR!
let $value ( 1 + 2 );         # OK

let $value 1 + (2 * $uid);    # ERROR!
let $value 1 + ( 2 * $uid );  # OK



## Features supported:

- operations with unsigned integers:

  + - * / %

- string operations:

  . (concatenation)

- hexadecimal numbers

- grouping with parentheses



## Notes:

Use the following command to rebuild parser generator if you need that

bison -d let.y

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-let](https://github.com/dvershinin/nginx-let-module){target=_blank}.

# *[BETA!] log-zmq*: ZeroMQ logger module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-log-zmq
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_log_zmq_module.so;
```


This document describes nginx-module-log-zmq [v0](https://github.com/dvershinin/nginx-log-zmq/releases/tag/v0){target=_blank} 
released on Nov 28 2021.

Production stability is *not guaranteed*.
<hr />

ZeroMQ logger module for nginx.

[ZeroMQ](http://zeromq.org), \zero-em-queue\, is a protocol for messages exchange. It's a easy
way to communicate using any language or platform via inproc, IPC, TCP, TPIC or multicast.
It's asynchronous and only requires a small library.



## Status

This module is already production ready.

## Description

This is a nginx logger module integrated with [ZeroMQ](http://zeromq.org) library.

`nginx-log-zmq` provides a very efficient way to log data for one or more PUB/SUB subscribers, over one or more different endpoints. This can be useful for data gathering and processing.

The message format can be the same as the tradicional log format which gives a interesting way to `tail` data via the network or exploring other text formats like JSON. As with the traditional log, it's possible to use nginx variables updated each request.

All messages are sent asynchronously and do not block the normal behaviour of the nginx server. As expected, the connections are resilient to network failures.

## Synopsis

```nginx
	http {
		# simple message to an IPC endpoint with 4 threads and 1000 queue elements

		log_zmq_server main "/tmp/main.ipc" ipc 4 1000;
		log_zmq_endpoint  main "/topic/";

		log_zmq_format main '{"remote_addr":"$remote_addr"}'

		# send messages to a subscriber listening at 127.0.0.1:5556

		log_zmq_server secondary 127.0.0.1:5556 tcp 4 1000;

		# set secondary endpoint
		log_zmq_endpoint secondary "/endpoint/";

		# set format using multiline
		log_zmq_format secondary '{"request_uri":"$request_uri",'
								   '{"status":"$status"}';


		server {

			location /status {
				# mute all messages from log_zmq for this location

				log_zmq_off all;
			}

			location /endpoint {
				# mute main messages from log_zmq for this location

				log_zmq_off main;
			}
		}
	}
```

## Directives

## log_zmq_server
**syntax:** *log_zmq_server &lt;definition_name&gt; &lt;address&gt; &lt;ipc|tcp&gt; &lt;threads&gt; &lt;queue size&gt;*

**default:** no

**context:** http

Configures a server (PUB/SUB subscriber) to connect to.

The following options are required:

**definition_name** &lt;name&gt; - the name that nginx will use to identify this logger instance.

**address** &lt;path&gt;|&lt;ipaddress&gt;:&lt;port&gt; - the subscriber's address. If you are using the IPC
protocol, you should specify the `<path>` for the unix socket. If you are using the TCP
protocol, you should specify the `<ipaddress>` and `<port>` where your ZeroMQ subscriber is listening.

**protocol** &lt;ipc|tcp&gt; - the protocol to be used for communication.

**threads** &lt;integer&gt; - the number of I/O threads to be used.

**queue_size** &lt;integer&gt; - the maximum queue size for messages waiting to be sent.


## log_zmq_endpoint

**syntax:** *log_zmq_endpoint &lt;definition_name&gt; "&lt;topic&gt;"*

**default:** no

**context:** http

Configures the topic for the ZeroMQ messages.

**definition_name** &lt;name&gt; - the name that nginx will use to identify this logger instance.

**topic** &lt;topic&gt; - the topic for the messages. This is a string (which can be a nginx variable) prepended to every sent message. For example, if you send the message "hello" to the "/talk:" topic, the message will end up as "/talk:hello".

Example:

```nginx
http {
	log_zmq_server main "/tmp/example.ipc" 4 1000;

	# send a message for for an topic based on response status
	log_zmq_endpoint main "/remote/$status";
}
```


## log_zmq_format

**syntax:** *log_zmq_format &lt;definition_name&gt; "&lt;format&gt;"*

**default:** no

**context:** http

Configures the ZeroMQ message format.

**definition_name** &lt;name&gt; - the name that nginx will use to identify this logger instance.

**format** &lt;format&gt; - the format for the messages. This defines the actual messages sent to the PUB/SUB subscriber. It follows the sames rules as the standard `log_format` directive. It is possible to use nginx variables here, and also to break it over multiple lines.

```nginx
http {
	log_zmq_format main '{"line1": value,'
                          '{"line2": value}';
}
```


## log_zmq_off

**syntax:** *log_zmq_off &lt;definition_name&gt;|all*

**default:** no

**context:** location

Turn off ZeroMQ logging in the current context.

**definition_name** &lt;name&gt; the name of the logger instance to be muted. If the special `all` name is used, all logger instances are muted.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-log-zmq](https://github.com/dvershinin/nginx-log-zmq){target=_blank}.

# *lua*: Lua scripting support for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-lua
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_lua_module.so;
```


This document describes nginx-module-lua [v0.10.25](https://github.com/openresty/lua-nginx-module/releases/tag/v0.10.25){target=_blank} 
released on Jun 19 2023.

<hr />

ngx_http_lua_module - Embed the power of Lua into Nginx HTTP Servers.

This module is a core component of [OpenResty](https://openresty.org). If you are using this module,
then you are essentially using OpenResty.

[the installation instructions](#installation).

This is a core component of OpenResty. If you are using this module, then you are essentially using OpenResty :)

## Status

Production ready.

## Videos

* YouTube video "[Hello World HTTP Example with OpenResty/Lua](https://youtu.be/eSfYLvVQMxw)"

    [![Hello World HTTP Example with OpenResty/Lua](https://img.youtube.com/vi/eSfYLvVQMxw/0.jpg)](https://youtu.be/eSfYLvVQMxw)

* YouTube video "[Write Your Own Lua Modules in OpenResty/Nginx Applications](https://youtu.be/vfYxOMl5LVY)"

    [![Write Your Own Lua Modules in OpenResty/Nginx Applications](https://img.youtube.com/vi/vfYxOMl5LVY/0.jpg)](https://youtu.be/vfYxOMl5LVY)

* YouTube video "[OpenResty's resty Command-Line Utility Demo](https://youtu.be/L1c7aw4mSOo)"

    [![OpenResty's resty Command-Line Utility Demo](https://img.youtube.com/vi/L1c7aw4mSOo/0.jpg)](https://youtu.be/L1c7aw4mSOo)

* YouTube video "[Measure Execution Time of Lua Code Correctly in OpenResty](https://youtu.be/VkRYW_qLoME)"

    [![Measure Execution Time of Lua Code Correctly in OpenResty](https://img.youtube.com/vi/VkRYW_qLoME/0.jpg)](https://youtu.be/VkRYW_qLoME)

* YouTube video "[Precompile Lua Modules into LuaJIT Bytecode to Speedup OpenResty Startup](https://youtu.be/EP7c0BM2yNo)"

    [![Precompile Lua Modules into LuaJIT Bytecode to Speedup OpenResty Startup](https://img.youtube.com/vi/EP7c0BM2yNo/0.jpg)](https://youtu.be/EP7c0BM2yNo)

You are welcome to subscribe to our [official YouTube channel, OpenResty](https://www.youtube.com/channel/UCXVmwF-UCScv2ftsGoMqxhw).


## Synopsis
```nginx

 # set search paths for pure Lua external libraries (';;' is the default path):

 # set search paths for Lua external libraries written in C (can also use ';;'):

 server {
     location /lua_content {
         # MIME type determined by default_type:
         default_type 'text/plain';

         content_by_lua_block {
             ngx.say('Hello,world!')
         }
     }

     location /nginx_var {
         # MIME type determined by default_type:
         default_type 'text/plain';

         # try access /nginx_var?a=hello,world
         content_by_lua_block {
             ngx.say(ngx.var.arg_a)
         }
     }

     location = /request_body {
         client_max_body_size 50k;
         client_body_buffer_size 50k;

         content_by_lua_block {
             ngx.req.read_body()  -- explicitly read the req body
             local data = ngx.req.get_body_data()
             if data then
                 ngx.say("body data:")
                 ngx.print(data)
                 return
             end

             -- body may get buffered in a temp file:
             local file = ngx.req.get_body_file()
             if file then
                 ngx.say("body is in file ", file)
             else
                 ngx.say("no body found")
             end
         }
     }

     # transparent non-blocking I/O in Lua via subrequests
     # (well, a better way is to use cosockets)
     location = /lua {
         # MIME type determined by default_type:
         default_type 'text/plain';

         content_by_lua_block {
             local res = ngx.location.capture("/some_other_location")
             if res then
                 ngx.say("status: ", res.status)
                 ngx.say("body:")
                 ngx.print(res.body)
             end
         }
     }

     location = /foo {
         rewrite_by_lua_block {
             res = ngx.location.capture("/memc",
                 { args = { cmd = "incr", key = ngx.var.uri } }
             )
         }

         proxy_pass http://blah.blah.com;
     }

     location = /mixed {
         rewrite_by_lua_file /path/to/rewrite.lua;
         access_by_lua_file /path/to/access.lua;
         content_by_lua_file /path/to/content.lua;
     }

     # use nginx var in code path
     # CAUTION: contents in nginx var must be carefully filtered,
     # otherwise there'll be great security risk!
     location ~ ^/app/([-_a-zA-Z0-9/]+) {
         set $path $1;
         content_by_lua_file /path/to/lua/app/root/$path.lua;
     }

     location / {
        client_max_body_size 100k;
        client_body_buffer_size 100k;

        access_by_lua_block {
            -- check the client IP address is in our black list
            if ngx.var.remote_addr == "132.5.72.3" then
                ngx.exit(ngx.HTTP_FORBIDDEN)
            end

            -- check if the URI contains bad words
            if ngx.var.uri and
                   string.match(ngx.var.request_body, "evil")
            then
                return ngx.redirect("/terms_of_use.html")
            end

            -- tests passed
        }

        # proxy_pass/fastcgi_pass/etc settings
     }
 }
```


## Description

This module embeds [LuaJIT 2.0/2.1](https://luajit.org/luajit.html) into Nginx.
It is a core component of [OpenResty](https://openresty.org). If you are using
this module, then you are essentially using OpenResty.

Since version `v0.10.16` of this module, the standard Lua
interpreter (also known as "PUC-Rio Lua") is not supported anymore. This
document interchangeably uses the terms "Lua" and "LuaJIT" to refer to the
LuaJIT interpreter.

By leveraging Nginx's subrequests, this module allows the integration of the
powerful Lua threads (known as Lua "coroutines") into the Nginx event model.

Unlike [Apache's mod_lua](https://httpd.apache.org/docs/trunk/mod/mod_lua.html)
and [Lighttpd's mod_magnet](http://redmine.lighttpd.net/wiki/1/Docs:ModMagnet),
Lua code executed using this module can be *100% non-blocking* on network
traffic as long as the [Nginx API for Lua](#nginx-api-for-lua) provided by
this module is used to handle requests to upstream services such as MySQL,
PostgreSQL, Memcached, Redis, or upstream HTTP web services.

At least the following Lua libraries and Nginx modules can be used with this
module:

* [lua-resty-memcached](https://github.com/openresty/lua-resty-memcached)
* [lua-resty-mysql](https://github.com/openresty/lua-resty-mysql)
* [lua-resty-redis](https://github.com/openresty/lua-resty-redis)
* [lua-resty-dns](https://github.com/openresty/lua-resty-dns)
* [lua-resty-upload](https://github.com/openresty/lua-resty-upload)
* [lua-resty-websocket](https://github.com/openresty/lua-resty-websocket)
* [lua-resty-lock](https://github.com/openresty/lua-resty-lock)
* [lua-resty-logger-socket](https://github.com/cloudflare/lua-resty-logger-socket)
* [lua-resty-lrucache](https://github.com/openresty/lua-resty-lrucache)
* [lua-resty-string](https://github.com/openresty/lua-resty-string)
* [ngx_memc](http://github.com/openresty/memc-nginx-module)
* [ngx_postgres](https://github.com/FRiCKLE/ngx_postgres)
* [ngx_redis2](http://github.com/openresty/redis2-nginx-module)
* [ngx_redis](http://wiki.nginx.org/HttpRedisModule)
* [ngx_proxy](http://nginx.org/en/docs/http/ngx_http_proxy_module.html)
* [ngx_fastcgi](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html)

Almost any Nginx modules can be used with this ngx_lua module by means of
[ngx.location.capture](#ngxlocationcapture) or
[ngx.location.capture_multi](#ngxlocationcapture_multi) but it is
recommended to use those `lua-resty-*` libraries instead of creating
subrequests to access the Nginx upstream modules because the former is usually
much more flexible and memory-efficient.

The Lua interpreter (also known as "Lua State" or "LuaJIT VM instance") is
shared across all the requests in a single Nginx worker process to minimize
memory use. Request contexts are segregated using lightweight Lua coroutines.

Loaded Lua modules persist in the Nginx worker process level resulting in a
small memory footprint in Lua even when under heavy loads.

This module is plugged into Nginx's "http" subsystem so it can only speaks
downstream communication protocols in the HTTP family (HTTP 0.9/1.0/1.1/2.0,
WebSockets, etc...).  If you want to do generic TCP communications with the
downstream clients, then you should use the
[ngx_stream_lua](https://github.com/openresty/stream-lua-nginx-module#readme)
module instead, which offers a compatible Lua API.


## Typical Uses

Just to name a few:

* Mashup'ing and processing outputs of various Nginx upstream outputs (proxy, drizzle, postgres, redis, memcached, and etc) in Lua,
* doing arbitrarily complex access control and security checks in Lua before requests actually reach the upstream backends,
* manipulating response headers in an arbitrary way (by Lua)
* fetching backend information from external storage backends (like redis, memcached, mysql, postgresql) and use that information to choose which upstream backend to access on-the-fly,
* coding up arbitrarily complex web applications in a content handler using synchronous but still non-blocking access to the database backends and other storage,
* doing very complex URL dispatch in Lua at rewrite phase,
* using Lua to implement advanced caching mechanism for Nginx's subrequests and arbitrary locations.

The possibilities are unlimited as the module allows bringing together various
elements within Nginx as well as exposing the power of the Lua language to the
user. The module provides the full flexibility of scripting while offering
performance levels comparable with native C language programs both in terms of
CPU time as well as memory footprint thanks to LuaJIT 2.x.

Other scripting language implementations typically struggle to match this
performance level.


## Nginx Compatibility

The latest version of this module is compatible with the following versions of Nginx:

* 1.19.x  (last tested: 1.19.3)
* 1.17.x  (last tested: 1.17.8)
* 1.15.x  (last tested: 1.15.8)
* 1.14.x
* 1.13.x  (last tested: 1.13.6)
* 1.12.x
* 1.11.x  (last tested: 1.11.2)
* 1.10.x
* 1.9.x (last tested: 1.9.15)
* 1.8.x
* 1.7.x (last tested: 1.7.10)
* 1.6.x

Nginx cores older than 1.6.0 (exclusive) are *not* supported.


## Code Repository

The code repository of this project is hosted on GitHub at
[openresty/lua-nginx-module](https://github.com/openresty/lua-nginx-module).


## LuaJIT bytecode support

Watch YouTube video "[Measure Execution Time of Lua Code Correctly in OpenResty](https://youtu.be/VkRYW_qLoME)"

[![Precompile Lua Modules into LuaJIT Bytecode to Speedup OpenResty Startup](https://img.youtube.com/vi/EP7c0BM2yNo/0.jpg)](https://youtu.be/EP7c0BM2yNo)

As from the `v0.5.0rc32` release, all `*_by_lua_file` configure directives (such as [content_by_lua_file](#content_by_lua_file)) support loading LuaJIT 2.0/2.1 raw bytecode files directly:

```bash

 /path/to/luajit/bin/luajit -b /path/to/input_file.lua /path/to/output_file.ljbc
```

The `-bg` option can be used to include debug information in the LuaJIT bytecode file:

```bash

 /path/to/luajit/bin/luajit -bg /path/to/input_file.lua /path/to/output_file.ljbc
```

Please refer to the official LuaJIT documentation on the `-b` option for more details:

<https://luajit.org/running.html#opt_b>

Note that the bytecode files generated by LuaJIT 2.1 is *not* compatible with
LuaJIT 2.0, and vice versa. The support for LuaJIT 2.1 bytecode was first added
in ngx_lua v0.9.3.

Attempts to load standard Lua 5.1 bytecode files into ngx_lua instances linked
to LuaJIT 2.0/2.1 (or vice versa) will result in an Nginx error message such as
the one below:


    [error] 13909#0: *1 failed to load Lua inlined code: bad byte-code header in /path/to/test_file.luac


Loading bytecode files via the Lua primitives like `require` and
`dofile` should always work as expected.


## System Environment Variable Support

If you want to access the system environment variable, say, `foo`, in Lua via the standard Lua API [os.getenv](https://www.lua.org/manual/5.1/manual.html#pdf-os.getenv), then you should also list this environment variable name in your `nginx.conf` file via the [env directive](https://nginx.org/en/docs/ngx_core_module.html#env). For example,

```nginx

 env foo;
```


## HTTP 1.0 support

The HTTP 1.0 protocol does not support chunked output and requires an explicit `Content-Length` header when the response body is not empty in order to support the HTTP 1.0 keep-alive.
So when a HTTP 1.0 request is made and the [lua_http10_buffering](#lua_http10_buffering) directive is turned `on`, ngx_lua will buffer the
output of [ngx.say](#ngxsay) and [ngx.print](#ngxprint) calls and also postpone sending response headers until all the response body output is received.
At that time ngx_lua can calculate the total length of the body and construct a proper `Content-Length` header to return to the HTTP 1.0 client.
If the `Content-Length` response header is set in the running Lua code, however, this buffering will be disabled even if the [lua_http10_buffering](#lua_http10_buffering) directive is turned `on`.

For large streaming output responses, it is important to disable the [lua_http10_buffering](#lua_http10_buffering) directive to minimise memory usage.

Note that common HTTP benchmark tools such as `ab` and `http_load` issue HTTP 1.0 requests by default.
To force `curl` to send HTTP 1.0 requests, use the `-0` option.


## Statically Linking Pure Lua Modules

With LuaJIT 2.x, it is possible to statically link the bytecode of pure Lua
modules into the Nginx executable.

You can use the `luajit` executable to compile `.lua` Lua
module files to `.o` object files containing the exported bytecode
data, and then link the `.o` files directly in your Nginx build.

Below is a trivial example to demonstrate this. Consider that we have the following `.lua` file named `foo.lua`:

```lua

 -- foo.lua
 local _M = {}

 function _M.go()
     print("Hello from foo")
 end

 return _M
```

And then we compile this `.lua` file to `foo.o` file:

```bash

 /path/to/luajit/bin/luajit -bg foo.lua foo.o
```

What matters here is the name of the `.lua` file, which determines how you use this module later on the Lua land. The file name `foo.o` does not matter at all except the `.o` file extension (which tells `luajit` what output format is used). If you want to strip the Lua debug information from the resulting bytecode, you can just specify the `-b` option above instead of `-bg`.

Then when building Nginx or OpenResty, pass the `--with-ld-opt="foo.o"` option to the `./configure` script:

```bash

 ./configure --with-ld-opt="/path/to/foo.o" ...
```

Finally, you can just do the following in any Lua code run by ngx_lua:

```lua

 local foo = require "foo"
 foo.go()
```

And this piece of code no longer depends on the external `foo.lua` file any more because it has already been compiled into the `nginx` executable.

If you want to use dot in the Lua module name when calling `require`, as in

```lua

 local foo = require "resty.foo"
```

then you need to rename the `foo.lua` file to `resty_foo.lua` before compiling it down to a `.o` file with the `luajit` command-line utility.

It is important to use exactly the same version of LuaJIT when compiling `.lua` files to `.o` files as building nginx + ngx_lua. This is because the LuaJIT bytecode format may be incompatible between different LuaJIT versions. When the bytecode format is incompatible, you will see a Lua runtime error saying that the Lua module is not found.

When you have multiple `.lua` files to compile and link, then just specify their `.o` files at the same time in the value of the `--with-ld-opt` option. For instance,

```bash

 ./configure --with-ld-opt="/path/to/foo.o /path/to/bar.o" ...
```

If you have too many `.o` files, then it might not be feasible to name them all in a single command. In this case, you can build a static library (or archive) for your `.o` files, as in

```bash

 ar rcus libmyluafiles.a *.o
```

then you can link the `myluafiles` archive as a whole to your nginx executable:

```bash

 ./configure \
     --with-ld-opt="-L/path/to/lib -Wl,--whole-archive -lmyluafiles -Wl,--no-whole-archive"
```

where `/path/to/lib` is the path of the directory containing the `libmyluafiles.a` file. It should be noted that the linker option `--whole-archive` is required here because otherwise our archive will be skipped because no symbols in our archive are mentioned in the main parts of the nginx executable.


## Data Sharing within an Nginx Worker

To globally share data among all the requests handled by the same Nginx worker
process, encapsulate the shared data into a Lua module, use the Lua
`require` builtin to import the module, and then manipulate the
shared data in Lua. This works because required Lua modules are loaded only
once and all coroutines will share the same copy of the module (both its code
and data).

Note that the use of global Lua variables is *strongly discouraged*, as it may
lead to unexpected race conditions between concurrent requests.

Here is a small example on sharing data within an Nginx worker via a Lua module:

```lua

 -- mydata.lua
 local _M = {}

 local data = {
     dog = 3,
     cat = 4,
     pig = 5,
 }

 function _M.get_age(name)
     return data[name]
 end

 return _M
```

and then accessing it from `nginx.conf`:

```nginx

 location /lua {
     content_by_lua_block {
         local mydata = require "mydata"
         ngx.say(mydata.get_age("dog"))
     }
 }
```

The `mydata` module in this example will only be loaded and run on the first request to the location `/lua`,
and all subsequent requests to the same Nginx worker process will use the reloaded instance of the
module as well as the same copy of the data in it, until a `HUP` signal is sent to the Nginx master process to force a reload.
This data sharing technique is essential for high performance Lua applications based on this module.

Note that this data sharing is on a *per-worker* basis and not on a *per-server* basis. That is, when there are multiple Nginx worker processes under an Nginx master, data sharing cannot cross the process boundary between these workers.

It is usually recommended to share read-only data this way. You can also share changeable data among all the concurrent requests of each Nginx worker process as
long as there is *no* nonblocking I/O operations (including [ngx.sleep](#ngxsleep))
in the middle of your calculations. As long as you do not give the
control back to the Nginx event loop and ngx_lua's light thread
scheduler (even implicitly), there can never be any race conditions in
between. For this reason, always be very careful when you want to share changeable data on the
worker level. Buggy optimizations can easily lead to hard-to-debug
race conditions under load.

If server-wide data sharing is required, then use one or more of the following approaches:

1. Use the [ngx.shared.DICT](#ngxshareddict) API provided by this module.
1. Use only a single Nginx worker and a single server (this is however not recommended when there is a multi core CPU or multiple CPUs in a single machine).
1. Use data storage mechanisms such as `memcached`, `redis`, `MySQL` or `PostgreSQL`. [The OpenResty official releases](https://openresty.org) come with a set of companion Nginx modules and Lua libraries that provide interfaces with these data storage mechanisms.


## Known Issues


## TCP socket connect operation issues

The [tcpsock:connect](#tcpsockconnect) method may indicate `success` despite connection failures such as with `Connection Refused` errors.

However, later attempts to manipulate the cosocket object will fail and return the actual error status message generated by the failed connect operation.

This issue is due to limitations in the Nginx event model and only appears to affect Mac OS X.


## Lua Coroutine Yielding/Resuming

* Because Lua's `dofile` and `require` builtins are currently implemented as C functions in LuaJIT 2.0/2.1, if the Lua file being loaded by `dofile` or `require` invokes [ngx.location.capture*](#ngxlocationcapture), [ngx.exec](#ngxexec), [ngx.exit](#ngxexit), or other API functions requiring yielding in the *top-level* scope of the Lua file, then the Lua error "attempt to yield across C-call boundary" will be raised. To avoid this, put these calls requiring yielding into your own Lua functions in the Lua file instead of the top-level scope of the file.


## Lua Variable Scope

Care must be taken when importing modules, and this form should be used:

```lua

 local xxx = require('xxx')
```

instead of the old deprecated form:

```lua

 require('xxx')
```

Here is the reason: by design, the global environment has exactly the same lifetime as the Nginx request handler associated with it. Each request handler has its own set of Lua global variables and that is the idea of request isolation. The Lua module is actually loaded by the first Nginx request handler and is cached by the `require()` built-in in the `package.loaded` table for later reference, and the `module()` builtin used by some Lua modules has the side effect of setting a global variable to the loaded module table. But this global variable will be cleared at the end of the request handler,  and every subsequent request handler all has its own (clean) global environment. So one will get Lua exception for accessing the `nil` value.

The use of Lua global variables is a generally inadvisable in the ngx_lua context as:

1. the misuse of Lua globals has detrimental side effects on concurrent requests when such variables should instead be local in scope,
1. Lua global variables require Lua table look-ups in the global environment which is computationally expensive, and
1. some Lua global variable references may include typing errors which make such difficult to debug.

It is therefore *highly* recommended to always declare such within an appropriate local scope instead.

```lua

 -- Avoid
 foo = 123
 -- Recommended
 local foo = 123

 -- Avoid
 function foo() return 123 end
 -- Recommended
 local function foo() return 123 end
```

To find all instances of Lua global variables in your Lua code, run the [lua-releng tool](https://github.com/openresty/nginx-devel-utils/blob/master/lua-releng) across all `.lua` source files:

    $ lua-releng
    Checking use of Lua global variables in file lib/foo/bar.lua ...
            1       [1489]  SETGLOBAL       7 -1    ; contains
            55      [1506]  GETGLOBAL       7 -3    ; setvar
            3       [1545]  GETGLOBAL       3 -4    ; varexpand

The output says that the line 1489 of file `lib/foo/bar.lua` writes to a global variable named `contains`, the line 1506 reads from the global variable `setvar`, and line 1545 reads the global `varexpand`.

This tool will guarantee that local variables in the Lua module functions are all declared with the `local` keyword, otherwise a runtime exception will be thrown. It prevents undesirable race conditions while accessing such variables. See [Data Sharing within an Nginx Worker](#data-sharing-within-an-nginx-worker) for the reasons behind this.


## Locations Configured by Subrequest Directives of Other Modules

The [ngx.location.capture](#ngxlocationcapture) and [ngx.location.capture_multi](#ngxlocationcapture_multi) directives cannot capture locations that include the [add_before_body](http://nginx.org/en/docs/http/ngx_http_addition_module.html#add_before_body), [add_after_body](http://nginx.org/en/docs/http/ngx_http_addition_module.html#add_after_body), [auth_request](https://nginx.org/en/docs/http/ngx_http_auth_request_module.html#auth_request), [echo_location](http://github.com/openresty/echo-nginx-module#echo_location), [echo_location_async](http://github.com/openresty/echo-nginx-module#echo_location_async), [echo_subrequest](http://github.com/openresty/echo-nginx-module#echo_subrequest), or [echo_subrequest_async](http://github.com/openresty/echo-nginx-module#echo_subrequest_async) directives.

```nginx

 location /foo {
     content_by_lua_block {
         res = ngx.location.capture("/bar")
     }
 }
 location /bar {
     echo_location /blah;
 }
 location /blah {
     echo "Success!";
 }
```

```nginx

 $ curl -i http://example.com/foo
```

will not work as expected.


## Cosockets Not Available Everywhere

Due to internal limitations in the Nginx core, the cosocket API is disabled in the following contexts: [set_by_lua*](#set_by_lua), [log_by_lua*](#log_by_lua), [header_filter_by_lua*](#header_filter_by_lua), and [body_filter_by_lua](#body_filter_by_lua).

The cosockets are currently also disabled in the [init_by_lua*](#init_by_lua) and [init_worker_by_lua*](#init_worker_by_lua) directive contexts but we may add support for these contexts in the future because there is no limitation in the Nginx core (or the limitation might be worked around).

There exists a workaround, however, when the original context does *not* need to wait for the cosocket results. That is, creating a zero-delay timer via the [ngx.timer.at](#ngxtimerat) API and do the cosocket results in the timer handler, which runs asynchronously as to the original context creating the timer.


## Special Escaping Sequences

**NOTE** Following the `v0.9.17` release, this pitfall can be avoided by using the `*_by_lua_block {}` configuration directives.

PCRE sequences such as `\d`, `\s`, or `\w`, require special attention because in string literals, the backslash character, `\`, is stripped out by both the Lua language parser and by the Nginx config file parser before processing if not within a `*_by_lua_block {}` directive. So the following snippet will not work as expected:

```nginx

 # nginx.conf
 ? location /test {
 ?     content_by_lua '
 ?         local regex = "\d+"  -- THIS IS WRONG OUTSIDE OF A *_by_lua_block DIRECTIVE
 ?         local m = ngx.re.match("hello, 1234", regex)
 ?         if m then ngx.say(m[0]) else ngx.say("not matched!") end
 ?     ';
 ? }
 # evaluates to "not matched!"
```

To avoid this, *double* escape the backslash:

```nginx

 # nginx.conf
 location /test {
     content_by_lua '
         local regex = "\\\\d+"
         local m = ngx.re.match("hello, 1234", regex)
         if m then ngx.say(m[0]) else ngx.say("not matched!") end
     ';
 }
 # evaluates to "1234"
```

Here, `\\\\d+` is stripped down to `\\d+` by the Nginx config file parser and this is further stripped down to `\d+` by the Lua language parser before running.

Alternatively, the regex pattern can be presented as a long-bracketed Lua string literal by encasing it in "long brackets", `[[...]]`, in which case backslashes have to only be escaped once for the Nginx config file parser.

```nginx

 # nginx.conf
 location /test {
     content_by_lua '
         local regex = [[\\d+]]
         local m = ngx.re.match("hello, 1234", regex)
         if m then ngx.say(m[0]) else ngx.say("not matched!") end
     ';
 }
 # evaluates to "1234"
```

Here, `[[\\d+]]` is stripped down to `[[\d+]]` by the Nginx config file parser and this is processed correctly.

Note that a longer from of the long bracket, `[=[...]=]`, may be required if the regex pattern contains `[...]` sequences.
The `[=[...]=]` form may be used as the default form if desired.

```nginx

 # nginx.conf
 location /test {
     content_by_lua '
         local regex = [=[[0-9]+]=]
         local m = ngx.re.match("hello, 1234", regex)
         if m then ngx.say(m[0]) else ngx.say("not matched!") end
     ';
 }
 # evaluates to "1234"
```

An alternative approach to escaping PCRE sequences is to ensure that Lua code is placed in external script files and executed using the various `*_by_lua_file` directives.
With this approach, the backslashes are only stripped by the Lua language parser and therefore only need to be escaped once each.

```lua

 -- test.lua
 local regex = "\\d+"
 local m = ngx.re.match("hello, 1234", regex)
 if m then ngx.say(m[0]) else ngx.say("not matched!") end
 -- evaluates to "1234"
```

Within external script files, PCRE sequences presented as long-bracketed Lua string literals do not require modification.

```lua

 -- test.lua
 local regex = [[\d+]]
 local m = ngx.re.match("hello, 1234", regex)
 if m then ngx.say(m[0]) else ngx.say("not matched!") end
 -- evaluates to "1234"
```

As noted earlier, PCRE sequences presented within `*_by_lua_block {}` directives (available following the `v0.9.17` release) do not require modification.

```nginx

 # nginx.conf
 location /test {
     content_by_lua_block {
         local regex = [[\d+]]
         local m = ngx.re.match("hello, 1234", regex)
         if m then ngx.say(m[0]) else ngx.say("not matched!") end
     }
 }
 # evaluates to "1234"
```

**NOTE** You are recommended to use `by_lua_file` when the Lua code is very long.


## Mixing with SSI Not Supported

Mixing SSI with ngx_lua in the same Nginx request is not supported at all. Just use ngx_lua exclusively. Everything you can do with SSI can be done atop ngx_lua anyway and it can be more efficient when using ngx_lua.


## SPDY Mode Not Fully Supported

Certain Lua APIs provided by ngx_lua do not work in Nginx's SPDY mode yet: [ngx.location.capture](#ngxlocationcapture), [ngx.location.capture_multi](#ngxlocationcapture_multi), and [ngx.req.socket](#ngxreqsocket).


## Missing data on short circuited requests

Nginx may terminate a request early with (at least):

* 400 (Bad Request)
* 405 (Not Allowed)
* 408 (Request Timeout)
* 413 (Request Entity Too Large)
* 414 (Request URI Too Large)
* 494 (Request Headers Too Large)
* 499 (Client Closed Request)
* 500 (Internal Server Error)
* 501 (Not Implemented)

This means that phases that normally run are skipped, such as the rewrite or
access phase. This also means that later phases that are run regardless, e.g.
[log_by_lua](#log_by_lua), will not have access to information that is normally set in those
phases.


## Changes

The changes made in every release of this module are listed in the change logs of the OpenResty bundle:

<https://openresty.org/#Changes>


## Test Suite

The following dependencies are required to run the test suite:

* Nginx version >= 1.4.2

* Perl modules:
	* Test::Nginx: <https://github.com/openresty/test-nginx>

* Nginx modules:
	* [ngx_devel_kit](https://github.com/simplresty/ngx_devel_kit)
	* [ngx_set_misc](https://github.com/openresty/set-misc-nginx-module)
	* [ngx_auth_request](http://mdounin.ru/files/ngx_http_auth_request_module-0.2.tar.gz) (this is not needed if you're using Nginx 1.5.4+.
	* [ngx_echo](https://github.com/openresty/echo-nginx-module)
	* [ngx_memc](https://github.com/openresty/memc-nginx-module)
	* [ngx_srcache](https://github.com/openresty/srcache-nginx-module)
	* ngx_lua (i.e., this module)
	* [ngx_lua_upstream](https://github.com/openresty/lua-upstream-nginx-module)
	* [ngx_headers_more](https://github.com/openresty/headers-more-nginx-module)
	* [ngx_drizzle](https://github.com/openresty/drizzle-nginx-module)
	* [ngx_rds_json](https://github.com/openresty/rds-json-nginx-module)
	* [ngx_coolkit](https://github.com/FRiCKLE/ngx_coolkit)
	* [ngx_redis2](https://github.com/openresty/redis2-nginx-module)

The order in which these modules are added during configuration is important because the position of any filter module in the
filtering chain determines the final output, for example. The correct adding order is shown above.

* 3rd-party Lua libraries:
	* [lua-cjson](http://www.kyne.com.au/~mark/software/lua-cjson.php)

* Applications:
	* mysql: create database 'ngx_test', grant all privileges to user 'ngx_test', password is 'ngx_test'
	* memcached: listening on the default port, 11211.
	* redis: listening on the default port, 6379.

See also the [developer build script](https://github.com/openresty/lua-nginx-module/blob/master/util/build.sh) for more details on setting up the testing environment.

To run the whole test suite in the default testing mode:

    cd /path/to/lua-nginx-module
    export PATH=/path/to/your/nginx/sbin:$PATH
    prove -I/path/to/test-nginx/lib -r t


To run specific test files:

    cd /path/to/lua-nginx-module
    export PATH=/path/to/your/nginx/sbin:$PATH
    prove -I/path/to/test-nginx/lib t/002-content.t t/003-errors.t


To run a specific test block in a particular test file, add the line `--- ONLY` to the test block you want to run, and then use the `prove` utility to run that `.t` file.

There are also various testing modes based on mockeagain, valgrind, and etc. Refer to the [Test::Nginx documentation](https://search.cpan.org/perldoc?Test::Nginx) for more details for various advanced testing modes. See also the test reports for the Nginx test cluster running on Amazon EC2: <https://qa.openresty.org>.


## See Also

Blog posts:

* [Introduction to Lua-Land CPU Flame Graphs](https://blog.openresty.com/en/lua-cpu-flame-graph/?src=gh_ngxlua)
* [How OpenResty and Nginx Allocate and Manage Memory](https://blog.openresty.com/en//how-or-alloc-mem?src=gh_ngxlua)
* [How OpenResty and Nginx Shared Memory Zones Consume RAM](https://blog.openresty.com/en/how-nginx-shm-consume-ram/?src=gh_ngxlua)
* [Memory Fragmentation in OpenResty and Nginx's Shared Memory Zones](https://blog.openresty.com/en/nginx-shm-frag/?src=gh_ngxlua)

Other related modules and libraries:

* [ngx_stream_lua_module](https://github.com/openresty/stream-lua-nginx-module#readme) for an official port of this module for the Nginx "stream" subsystem (doing generic downstream TCP communications).
* [lua-resty-memcached](https://github.com/openresty/lua-resty-memcached) library based on ngx_lua cosocket.
* [lua-resty-redis](https://github.com/openresty/lua-resty-redis) library based on ngx_lua cosocket.
* [lua-resty-mysql](https://github.com/openresty/lua-resty-mysql) library based on ngx_lua cosocket.
* [lua-resty-upload](https://github.com/openresty/lua-resty-upload) library based on ngx_lua cosocket.
* [lua-resty-dns](https://github.com/openresty/lua-resty-dns) library based on ngx_lua cosocket.
* [lua-resty-websocket](https://github.com/openresty/lua-resty-websocket) library for both WebSocket server and client, based on ngx_lua cosocket.
* [lua-resty-string](https://github.com/openresty/lua-resty-string) library based on [LuaJIT FFI](https://luajit.org/ext_ffi.html).
* [lua-resty-lock](https://github.com/openresty/lua-resty-lock) library for a nonblocking simple lock API.
* [lua-resty-cookie](https://github.com/cloudflare/lua-resty-cookie) library for HTTP cookie manipulation.
* [Routing requests to different MySQL queries based on URI arguments](https://openresty.org/#RoutingMySQLQueriesBasedOnURIArgs)
* [Dynamic Routing Based on Redis and Lua](https://openresty.org/#DynamicRoutingBasedOnRedis)
* [Using LuaRocks with ngx_lua](https://openresty.org/#UsingLuaRocks)
* [Introduction to ngx_lua](https://github.com/openresty/lua-nginx-module/wiki/Introduction)
* [ngx_devel_kit](https://github.com/simplresty/ngx_devel_kit)
* [echo-nginx-module](http://github.com/openresty/echo-nginx-module)
* [drizzle-nginx-module](http://github.com/openresty/drizzle-nginx-module)
* [postgres-nginx-module](https://github.com/FRiCKLE/ngx_postgres)
* [memc-nginx-module](http://github.com/openresty/memc-nginx-module)
* [The OpenResty bundle](https://openresty.org)
* [Nginx Systemtap Toolkit](https://github.com/openresty/nginx-systemtap-toolkit)


## Directives

* [lua_load_resty_core](#lua_load_resty_core)
* [lua_capture_error_log](#lua_capture_error_log)
* [lua_use_default_type](#lua_use_default_type)
* [lua_malloc_trim](#lua_malloc_trim)
* [lua_code_cache](#lua_code_cache)
* [lua_thread_cache_max_entries](#lua_thread_cache_max_entries)
* [lua_regex_cache_max_entries](#lua_regex_cache_max_entries)
* [lua_regex_match_limit](#lua_regex_match_limit)
* [lua_package_path](#lua_package_path)
* [lua_package_cpath](#lua_package_cpath)
* [init_by_lua](#init_by_lua)
* [init_by_lua_block](#init_by_lua_block)
* [init_by_lua_file](#init_by_lua_file)
* [init_worker_by_lua](#init_worker_by_lua)
* [init_worker_by_lua_block](#init_worker_by_lua_block)
* [init_worker_by_lua_file](#init_worker_by_lua_file)
* [exit_worker_by_lua_block](#exit_worker_by_lua_block)
* [exit_worker_by_lua_file](#exit_worker_by_lua_file)
* [set_by_lua](#set_by_lua)
* [set_by_lua_block](#set_by_lua_block)
* [set_by_lua_file](#set_by_lua_file)
* [content_by_lua](#content_by_lua)
* [content_by_lua_block](#content_by_lua_block)
* [content_by_lua_file](#content_by_lua_file)
* [server_rewrite_by_lua_block](#server_rewrite_by_lua_block)
* [server_rewrite_by_lua_file](#server_rewrite_by_lua_file)
* [rewrite_by_lua](#rewrite_by_lua)
* [rewrite_by_lua_block](#rewrite_by_lua_block)
* [rewrite_by_lua_file](#rewrite_by_lua_file)
* [access_by_lua](#access_by_lua)
* [access_by_lua_block](#access_by_lua_block)
* [access_by_lua_file](#access_by_lua_file)
* [header_filter_by_lua](#header_filter_by_lua)
* [header_filter_by_lua_block](#header_filter_by_lua_block)
* [header_filter_by_lua_file](#header_filter_by_lua_file)
* [body_filter_by_lua](#body_filter_by_lua)
* [body_filter_by_lua_block](#body_filter_by_lua_block)
* [body_filter_by_lua_file](#body_filter_by_lua_file)
* [log_by_lua](#log_by_lua)
* [log_by_lua_block](#log_by_lua_block)
* [log_by_lua_file](#log_by_lua_file)
* [balancer_by_lua_block](#balancer_by_lua_block)
* [balancer_by_lua_file](#balancer_by_lua_file)
* [lua_need_request_body](#lua_need_request_body)
* [ssl_client_hello_by_lua_block](#ssl_client_hello_by_lua_block)
* [ssl_client_hello_by_lua_file](#ssl_client_hello_by_lua_file)
* [ssl_certificate_by_lua_block](#ssl_certificate_by_lua_block)
* [ssl_certificate_by_lua_file](#ssl_certificate_by_lua_file)
* [ssl_session_fetch_by_lua_block](#ssl_session_fetch_by_lua_block)
* [ssl_session_fetch_by_lua_file](#ssl_session_fetch_by_lua_file)
* [ssl_session_store_by_lua_block](#ssl_session_store_by_lua_block)
* [ssl_session_store_by_lua_file](#ssl_session_store_by_lua_file)
* [lua_shared_dict](#lua_shared_dict)
* [lua_socket_connect_timeout](#lua_socket_connect_timeout)
* [lua_socket_send_timeout](#lua_socket_send_timeout)
* [lua_socket_send_lowat](#lua_socket_send_lowat)
* [lua_socket_read_timeout](#lua_socket_read_timeout)
* [lua_socket_buffer_size](#lua_socket_buffer_size)
* [lua_socket_pool_size](#lua_socket_pool_size)
* [lua_socket_keepalive_timeout](#lua_socket_keepalive_timeout)
* [lua_socket_log_errors](#lua_socket_log_errors)
* [lua_ssl_ciphers](#lua_ssl_ciphers)
* [lua_ssl_crl](#lua_ssl_crl)
* [lua_ssl_protocols](#lua_ssl_protocols)
* [lua_ssl_trusted_certificate](#lua_ssl_trusted_certificate)
* [lua_ssl_verify_depth](#lua_ssl_verify_depth)
* [lua_ssl_conf_command](#lua_ssl_conf_command)
* [lua_http10_buffering](#lua_http10_buffering)
* [rewrite_by_lua_no_postpone](#rewrite_by_lua_no_postpone)
* [access_by_lua_no_postpone](#access_by_lua_no_postpone)
* [lua_transform_underscores_in_response_headers](#lua_transform_underscores_in_response_headers)
* [lua_check_client_abort](#lua_check_client_abort)
* [lua_max_pending_timers](#lua_max_pending_timers)
* [lua_max_running_timers](#lua_max_running_timers)
* [lua_sa_restart](#lua_sa_restart)
* [lua_worker_thread_vm_pool_size](#lua_worker_thread_vm_pool_size)


The basic building blocks of scripting Nginx with Lua are directives. Directives are used to specify when the user Lua code is run and
how the result will be used. Below is a diagram showing the order in which directives are executed.

![Lua Nginx Modules Directives](https://cloud.githubusercontent.com/assets/2137369/15272097/77d1c09e-1a37-11e6-97ef-d9767035fc3e.png)


## lua_load_resty_core

**syntax:** *lua_load_resty_core on|off*

**default:** *lua_load_resty_core on*

**context:** *http*

This directive is deprecated since the `v0.10.16` release of this
module. The `resty.core` module from
[lua-resty-core](https://github.com/openresty/lua-resty-core) is now mandatorily
loaded during the Lua VM initialization. Specifying this directive will have no
effect.

This directive was first introduced in the `v0.10.15` release and
used to optionally load the `resty.core` module.

[Back to TOC](#directives)

## lua_capture_error_log

**syntax:** *lua_capture_error_log size*

**default:** *none*

**context:** *http*

Enables a buffer of the specified `size` for capturing all the Nginx error log message data (not just those produced
by this module or the Nginx http subsystem, but everything) without touching files or disks.

You can use units like `k` and `m` in the `size` value, as in

```nginx

 lua_capture_error_log 100k;
```

As a rule of thumb, a 4KB buffer can usually hold about 20 typical error log messages. So do the maths!

This buffer never grows. If it is full, new error log messages will replace the oldest ones in the buffer.

The size of the buffer must be bigger than the maximum length of a single error log message (which is 4K in OpenResty and 2K in stock NGINX).

You can read the messages in the buffer on the Lua land via the
[get_logs()](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/errlog.md#get_logs)
function of the
[ngx.errlog](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/errlog.md#readme)
module of the [lua-resty-core](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/errlog.md#readme)
library. This Lua API function will return the captured error log messages and
also remove these already read from the global capturing buffer, making room
for any new error log data. For this reason, the user should not configure this
buffer to be too big if the user read the buffered error log data fast enough.

Note that the log level specified in the standard [error_log](https://nginx.org/r/error_log) directive
*does* have effect on this capturing facility. It only captures log
messages of a level no lower than the specified log level in the [error_log](https://nginx.org/r/error_log) directive.
The user can still choose to set an even higher filtering log level on the fly via the Lua API function
[errlog.set_filter_level](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/errlog.md#set_filter_level).
So it is more flexible than the static [error_log](https://nginx.org/r/error_log) directive.

It is worth noting that there is no way to capture the debugging logs
without building OpenResty or Nginx with the `./configure`
option `--with-debug`. And enabling debugging logs is
strongly discouraged in production builds due to high overhead.

This directive was first introduced in the `v0.10.9` release.

[Back to TOC](#directives)

## lua_use_default_type

**syntax:** *lua_use_default_type on | off*

**default:** *lua_use_default_type on*

**context:** *http, server, location, location if*

Specifies whether to use the MIME type specified by the [default_type](https://nginx.org/en/docs/http/ngx_http_core_module.html#default_type) directive for the default value of the `Content-Type` response header. Deactivate this directive if a default `Content-Type` response header for Lua request handlers is not desired.

This directive is turned on by default.

This directive was first introduced in the `v0.9.1` release.

[Back to TOC](#directives)

## lua_malloc_trim

**syntax:** *lua_malloc_trim &lt;request-count&gt;*

**default:** *lua_malloc_trim 1000*

**context:** *http*

Asks the underlying `libc` runtime library to release its cached free memory back to the operating system every
`N` requests processed by the Nginx core. By default, `N` is 1000. You can configure the request count
by using your own numbers. Smaller numbers mean more frequent releases, which may introduce higher CPU time consumption and
smaller memory footprint while larger numbers usually lead to less CPU time overhead and relatively larger memory footprint.
Just tune the number for your own use cases.

Configuring the argument to `0` essentially turns off the periodical memory trimming altogether.

```nginx

 lua_malloc_trim 0;  # turn off trimming completely
```

The current implementation uses an Nginx log phase handler to do the request counting. So the appearance of the
[log_subrequest on](https://nginx.org/en/docs/http/ngx_http_core_module.html#log_subrequest) directives in `nginx.conf`
may make the counting faster when subrequests are involved. By default, only "main requests" count.

Note that this directive does *not* affect the memory allocated by LuaJIT's own allocator based on the `mmap`
system call.

This directive was first introduced in the `v0.10.7` release.

[Back to TOC](#directives)

## lua_code_cache
**syntax:** *lua_code_cache on | off*

**default:** *lua_code_cache on*

**context:** *http, server, location, location if*

Enables or disables the Lua code cache for Lua code in `*_by_lua_file` directives (like [set_by_lua_file](#set_by_lua_file) and
[content_by_lua_file](#content_by_lua_file)) and Lua modules.

When turning off, every request served by ngx_lua will run in a separate Lua VM instance, starting from the `0.9.3` release. So the Lua files referenced in [set_by_lua_file](#set_by_lua_file),
[content_by_lua_file](#content_by_lua_file), [access_by_lua_file](#access_by_lua_file),
and etc will not be cached
and all Lua modules used will be loaded from scratch. With this in place, developers can adopt an edit-and-refresh approach.

Please note however, that Lua code written inlined within nginx.conf
such as those specified by [set_by_lua](#set_by_lua), [content_by_lua](#content_by_lua),
[access_by_lua](#access_by_lua), and [rewrite_by_lua](#rewrite_by_lua) will not be updated when you edit the inlined Lua code in your `nginx.conf` file because only the Nginx config file parser can correctly parse the `nginx.conf`
file and the only way is to reload the config file
by sending a `HUP` signal or just to restart Nginx.

Even when the code cache is enabled, Lua files which are loaded by `dofile` or `loadfile`
in *_by_lua_file cannot be cached (unless you cache the results yourself). Usually you can either use the [init_by_lua](#init_by_lua)
or [init_by_lua_file](#init-by_lua_file) directives to load all such files or just make these Lua files true Lua modules
and load them via `require`.

The ngx_lua module does not support the `stat` mode available with the
Apache `mod_lua` module (yet).

Disabling the Lua code cache is strongly
discouraged for production use and should only be used during
development as it has a significant negative impact on overall performance. For example, the performance of a "hello world" Lua example can drop by an order of magnitude after disabling the Lua code cache.

[Back to TOC](#directives)

## lua_thread_cache_max_entries

**syntax:** *lua_thread_cache_max_entries &lt;num&gt;*

**default:** *lua_thread_cache_max_entries 1024*

**context:** *http*

Specifies the maximum number of entries allowed in the worker process level lua thread object cache.

This cache recycles the lua thread GC objects among all our "light threads".

A zero value of `<num>` disables the cache.

Note that this feature requires OpenResty's LuaJIT with the new C API `lua_resetthread`.

This feature was first introduced in verson `v0.10.9`.

[Back to TOC](#directives)

## lua_regex_cache_max_entries

**syntax:** *lua_regex_cache_max_entries &lt;num&gt;*

**default:** *lua_regex_cache_max_entries 1024*

**context:** *http*

Specifies the maximum number of entries allowed in the worker process level compiled regex cache.

The regular expressions used in [ngx.re.match](#ngxrematch), [ngx.re.gmatch](#ngxregmatch), [ngx.re.sub](#ngxresub), and [ngx.re.gsub](#ngxregsub) will be cached within this cache if the regex option `o` (i.e., compile-once flag) is specified.

The default number of entries allowed is 1024 and when this limit is reached, new regular expressions will not be cached (as if the `o` option was not specified) and there will be one, and only one, warning in the `error.log` file:


    2011/08/27 23:18:26 [warn] 31997#0: *1 lua exceeding regex cache max entries (1024), ...


If you are using the `ngx.re.*` implementation of [lua-resty-core](https://github.com/openresty/lua-resty-core) by loading the `resty.core.regex` module (or just the `resty.core` module), then an LRU cache is used for the regex cache being used here.

Do not activate the `o` option for regular expressions (and/or `replace` string arguments for [ngx.re.sub](#ngxresub) and [ngx.re.gsub](#ngxregsub)) that are generated *on the fly* and give rise to infinite variations to avoid hitting the specified limit.

[Back to TOC](#directives)

## lua_regex_match_limit

**syntax:** *lua_regex_match_limit &lt;num&gt;*

**default:** *lua_regex_match_limit 0*

**context:** *http*

Specifies the "match limit" used by the PCRE library when executing the [ngx.re API](#ngxrematch). To quote the PCRE manpage, "the limit ... has the effect of limiting the amount of backtracking that can take place."

When the limit is hit, the error string "pcre_exec() failed: -8" will be returned by the [ngx.re API](#ngxrematch) functions on the Lua land.

When setting the limit to 0, the default "match limit" when compiling the PCRE library is used. And this is the default value of this directive.

This directive was first introduced in the `v0.8.5` release.

[Back to TOC](#directives)

## lua_package_path

**syntax:** *lua_package_path &lt;lua-style-path-str&gt;*

**default:** *The content of LUA_PATH environment variable or Lua's compiled-in defaults.*

**context:** *http*

Sets the Lua module search path used by scripts specified by [set_by_lua](#set_by_lua),
[content_by_lua](#content_by_lua) and others. The path string is in standard Lua path form, and `;;`
can be used to stand for the original search paths.

As from the `v0.5.0rc29` release, the special notation `$prefix` or `${prefix}` can be used in the search path string to indicate the path of the `server prefix` usually determined by the `-p PATH` command-line option while starting the Nginx server.

[Back to TOC](#directives)

## lua_package_cpath

**syntax:** *lua_package_cpath &lt;lua-style-cpath-str&gt;*

**default:** *The content of LUA_CPATH environment variable or Lua's compiled-in defaults.*

**context:** *http*

Sets the Lua C-module search path used by scripts specified by [set_by_lua](#set_by_lua),
[content_by_lua](#content_by_lua) and others. The cpath string is in standard Lua cpath form, and `;;`
can be used to stand for the original cpath.

As from the `v0.5.0rc29` release, the special notation `$prefix` or `${prefix}` can be used in the search path string to indicate the path of the `server prefix` usually determined by the `-p PATH` command-line option while starting the Nginx server.

[Back to TOC](#directives)

## init_by_lua

**syntax:** *init_by_lua &lt;lua-script-str&gt;*

**context:** *http*

**phase:** *loading-config*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [init_by_lua_block](#init_by_lua_block) directive instead.

Similar to the [init_by_lua_block](#init_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping).

For instance,

```nginx

 init_by_lua '
     print("I need no extra escaping here, for example: \r\nblah")
 '
```

This directive was first introduced in the `v0.5.5` release.

[Back to TOC](#directives)

## init_by_lua_block

**syntax:** *init_by_lua_block { lua-script }*

**context:** *http*

**phase:** *loading-config*


When Nginx receives the `HUP` signal and starts reloading the config file, the Lua VM will also be re-created and `init_by_lua_block` will run again on the new Lua VM. In case that the [lua_code_cache](#lua_code_cache) directive is turned off (default on), the `init_by_lua_block` handler will run upon every request because in this special mode a standalone Lua VM is always created for each request.

Usually you can pre-load Lua modules at server start-up by means of this hook and take advantage of modern operating systems' copy-on-write (COW) optimization. Here is an example for pre-loading Lua modules:

```nginx

 # this runs before forking out nginx worker processes:
 init_by_lua_block { require "cjson" }

 server {
     location = /api {
         content_by_lua_block {
             -- the following require() will just  return
             -- the already loaded module from package.loaded:
             ngx.say(require "cjson".encode{dog = 5, cat = 6})
         }
     }
 }
```

You can also initialize the [lua_shared_dict](#lua_shared_dict) shm storage at this phase. Here is an example for this:

```nginx

 lua_shared_dict dogs 1m;

 init_by_lua_block {
     local dogs = ngx.shared.dogs
     dogs:set("Tom", 56)
 }

 server {
     location = /api {
         content_by_lua_block {
             local dogs = ngx.shared.dogs
             ngx.say(dogs:get("Tom"))
         }
     }
 }
```

But note that, the [lua_shared_dict](#lua_shared_dict)'s shm storage will not be cleared through a config reload (via the `HUP` signal, for example). So if you do *not* want to re-initialize the shm storage in your `init_by_lua_block` code in this case, then you just need to set a custom flag in the shm storage and always check the flag in your `init_by_lua_block` code.

Because the Lua code in this context runs before Nginx forks its worker processes (if any), data or code loaded here will enjoy the [Copy-on-write (COW)](https://en.wikipedia.org/wiki/Copy-on-write) feature provided by many operating systems among all the worker processes, thus saving a lot of memory.

Do *not* initialize your own Lua global variables in this context because use of Lua global variables have performance penalties and can lead to global namespace pollution (see the [Lua Variable Scope](#lua-variable-scope) section for more details). The recommended way is to use proper [Lua module](https://www.lua.org/manual/5.1/manual.html#5.3) files (but do not use the standard Lua function [module()](https://www.lua.org/manual/5.1/manual.html#pdf-module) to define Lua modules because it pollutes the global namespace as well) and call [require()](https://www.lua.org/manual/5.1/manual.html#pdf-require) to load your own module files in `init_by_lua_block` or other contexts ([require()](https://www.lua.org/manual/5.1/manual.html#pdf-require) does cache the loaded Lua modules in the global `package.loaded` table in the Lua registry so your modules will only loaded once for the whole Lua VM instance).

Only a small set of the [Nginx API for Lua](#nginx-api-for-lua) is supported in this context:

* Logging APIs: [ngx.log](#ngxlog) and [print](#print),
* Shared Dictionary API: [ngx.shared.DICT](#ngxshareddict).

More Nginx APIs for Lua may be supported in this context upon future user requests.

Basically you can safely use Lua libraries that do blocking I/O in this very context because blocking the master process during server start-up is completely okay. Even the Nginx core does blocking I/O (at least on resolving upstream's host names) at the configure-loading phase.

You should be very careful about potential security vulnerabilities in your Lua code registered in this context because the Nginx master process is often run under the `root` account.

This directive was first introduced in the `v0.9.17` release.

See also the following blog posts for more details on OpenResty and Nginx's shared memory zones:

* [How OpenResty and Nginx Shared Memory Zones Consume RAM](https://blog.openresty.com/en/how-nginx-shm-consume-ram/?src=gh_ngxlua)
* [Memory Fragmentation in OpenResty and Nginx's Shared Memory Zones](https://blog.openresty.com/en/nginx-shm-frag/?src=gh_ngxlua)

[Back to TOC](#directives)

## init_by_lua_file

**syntax:** *init_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http*

**phase:** *loading-config*

Equivalent to [init_by_lua_block](#init_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code or [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.5.5` release.

[Back to TOC](#directives)

## init_worker_by_lua

**syntax:** *init_worker_by_lua &lt;lua-script-str&gt;*

**context:** *http*

**phase:** *starting-worker*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [init_worker_by_lua_block](#init_worker_by_lua_block) directive instead.

Similar to the [init_worker_by_lua_block](#init_worker_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping).

For instance,

```nginx

 init_worker_by_lua '
     print("I need no extra escaping here, for example: \r\nblah")
 ';
```

This directive was first introduced in the `v0.9.5` release.

This hook no longer runs in the cache manager and cache loader processes since the `v0.10.12` release.

[Back to TOC](#directives)

## init_worker_by_lua_block

**syntax:** *init_worker_by_lua_block { lua-script }*

**context:** *http*

**phase:** *starting-worker*

Runs the specified Lua code upon every Nginx worker process's startup when the master process is enabled. When the master process is disabled, this hook will just run after [init_by_lua*](#init_by_lua_block).

This hook is often used to create per-worker reoccurring timers (via the [ngx.timer.at](#ngxtimerat) Lua API), either for backend health-check or other timed routine work. Below is an example,

```nginx

 init_worker_by_lua_block {
     local delay = 3  -- in seconds
     local new_timer = ngx.timer.at
     local log = ngx.log
     local ERR = ngx.ERR
     local check

     check = function(premature)
         if not premature then
             -- do the health check or other routine work
             local ok, err = new_timer(delay, check)
             if not ok then
                 log(ERR, "failed to create timer: ", err)
                 return
             end
         end

         -- do something in timer
     end

     local hdl, err = new_timer(delay, check)
     if not hdl then
         log(ERR, "failed to create timer: ", err)
         return
     end

     -- other job in init_worker_by_lua
 }
```

This directive was first introduced in the `v0.9.17` release.

This hook no longer runs in the cache manager and cache loader processes since the `v0.10.12` release.

[Back to TOC](#directives)

## init_worker_by_lua_file

**syntax:** *init_worker_by_lua_file &lt;lua-file-path&gt;*

**context:** *http*

**phase:** *starting-worker*

Similar to [init_worker_by_lua_block](#init_worker_by_lua_block), but accepts the file path to a Lua source file or Lua bytecode file.

This directive was first introduced in the `v0.9.5` release.

This hook no longer runs in the cache manager and cache loader processes since the `v0.10.12` release.

[Back to TOC](#directives)

## exit_worker_by_lua_block

**syntax:** *exit_worker_by_lua_block { lua-script }*

**context:** *http*

**phase:** *exiting-worker*

Runs the specified Lua code upon every Nginx worker process's exit when the master process is enabled. When the master process is disabled, this hook will run before the Nginx process exits.

This hook is often used to release resources allocated by each worker (e.g. resources allocated by [init_worker_by_lua*](#init_worker_by_lua_block)), or to prevent workers from exiting abnormally.

For example,

```nginx

 exit_worker_by_lua_block {
     print("log from exit_worker_by_lua_block")
 }
```

It's not allowed to create a timer (even a 0-delay timer) here since it runs after all timers have been processed.

This directive was first introduced in the `v0.10.18` release.

[Back to TOC](#directives)

## exit_worker_by_lua_file

**syntax:** *exit_worker_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http*

**phase:** *exiting-worker*

Similar to [exit_worker_by_lua_block](#exit_worker_by_lua_block), but accepts the file path to a Lua source file or Lua bytecode file.

This directive was first introduced in the `v0.10.18` release.

[Back to TOC](#directives)

## set_by_lua

**syntax:** *set_by_lua $res &lt;lua-script-str&gt; [$arg1 $arg2 ...]*

**context:** *server, server if, location, location if*

**phase:** *rewrite*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [set_by_lua_block](#set_by_lua_block) directive instead.

Similar to the [set_by_lua_block](#set_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping), and
1. this directive support extra arguments after the Lua script.

For example,

```nginx

 set_by_lua $res ' return 32 + math.cos(32) ';
 # $res now has the value "32.834223360507" or alike.
```

As from the `v0.5.0rc29` release, Nginx variable interpolation is disabled in the `<lua-script-str>` argument of this directive and therefore, the dollar sign character (`$`) can be used directly.

This directive requires the [ngx_devel_kit](https://github.com/simplresty/ngx_devel_kit) module.

[Back to TOC](#directives)

## set_by_lua_block

**syntax:** *set_by_lua_block $res { lua-script }*

**context:** *server, server if, location, location if*

**phase:** *rewrite*

Executes code specified inside a pair of curly braces (`{}`), and returns string output to `$res`.
The code inside a pair of curly braces (`{}`) can make [API calls](#nginx-api-for-lua) and can retrieve input arguments from the `ngx.arg` table (index starts from `1` and increases sequentially).

This directive is designed to execute short, fast running code blocks as the Nginx event loop is blocked during code execution. Time consuming code sequences should therefore be avoided.

This directive is implemented by injecting custom commands into the standard [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html)'s command list. Because [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html) does not support nonblocking I/O in its commands, Lua APIs requiring yielding the current Lua "light thread" cannot work in this directive.

At least the following API functions are currently disabled within the context of `set_by_lua_block`:

* Output API functions (e.g., [ngx.say](#ngxsay) and [ngx.send_headers](#ngxsend_headers))
* Control API functions (e.g., [ngx.exit](#ngxexit))
* Subrequest API functions (e.g., [ngx.location.capture](#ngxlocationcapture) and [ngx.location.capture_multi](#ngxlocationcapture_multi))
* Cosocket API functions (e.g., [ngx.socket.tcp](#ngxsockettcp) and [ngx.req.socket](#ngxreqsocket)).
* Sleeping API function [ngx.sleep](#ngxsleep).

In addition, note that this directive can only write out a value to a single Nginx variable at
a time. However, a workaround is possible using the [ngx.var.VARIABLE](#ngxvarvariable) interface.

```nginx

 location /foo {
     set $diff ''; # we have to predefine the $diff variable here

     set_by_lua_block $sum {
         local a = 32
         local b = 56

         ngx.var.diff = a - b  -- write to $diff directly
         return a + b          -- return the $sum value normally
     }

     echo "sum = $sum, diff = $diff";
 }
```

This directive can be freely mixed with all directives of the [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html), [set-misc-nginx-module](http://github.com/openresty/set-misc-nginx-module), and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module) modules. All of these directives will run in the same order as they appear in the config file.

```nginx

 set $foo 32;
 set_by_lua_block $bar { return tonumber(ngx.var.foo) + 1 }
 set $baz "bar: $bar";  # $baz == "bar: 33"
```

No special escaping is required in the Lua code block.

This directive requires the [ngx_devel_kit](https://github.com/simplresty/ngx_devel_kit) module.

This directive was first introduced in the `v0.9.17` release.

[Back to TOC](#directives)

## set_by_lua_file

**syntax:** *set_by_lua_file $res &lt;path-to-lua-script-file&gt; [$arg1 $arg2 ...]*

**context:** *server, server if, location, location if*

**phase:** *rewrite*

Equivalent to [set_by_lua_block](#set_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

Nginx variable interpolation is supported in the `<path-to-lua-script-file>` argument string of this directive. But special care must be taken for injection attacks.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

When the Lua code cache is turned on (by default), the user code is loaded once at the first request and cached
and the Nginx config must be reloaded each time the Lua source file is modified.
The Lua code cache can be temporarily disabled during development by
switching [lua_code_cache](#lua_code_cache) `off` in `nginx.conf` to avoid reloading Nginx.

This directive requires the [ngx_devel_kit](https://github.com/simplresty/ngx_devel_kit) module.

[Back to TOC](#directives)

## content_by_lua

**syntax:** *content_by_lua &lt;lua-script-str&gt;*

**context:** *location, location if*

**phase:** *content*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [content_by_lua_block](#content_by_lua_block) directive instead.

Similar to the [content_by_lua_block](#content_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping).

For instance,

```nginx

 content_by_lua '
     ngx.say("I need no extra escaping here, for example: \r\nblah")
 ';
```

[Back to TOC](#directives)

## content_by_lua_block

**syntax:** *content_by_lua_block { lua-script }*

**context:** *location, location if*

**phase:** *content*

For instance,

```nginx

 content_by_lua_block {
     ngx.say("I need no extra escaping here, for example: \r\nblah")
 }
```

Acts as a "content handler" and executes Lua code string specified in `{ lua-script }` for every request.
The Lua code may make [API calls](#nginx-api-for-lua) and is executed as a new spawned coroutine in an independent global environment (i.e. a sandbox).

Do not use this directive and other content handler directives in the same location. For example, this directive and the [proxy_pass](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass) directive should not be used in the same location.

This directive was first introduced in the `v0.9.17` release.

[Back to TOC](#directives)

## content_by_lua_file

**syntax:** *content_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *location, location if*

**phase:** *content*

Equivalent to [content_by_lua_block](#content_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

If the file is not found, a `404 Not Found` status code will be returned, and a `503 Service Temporarily Unavailable` status code will be returned in case of errors in reading other files.

Nginx variables can be used in the `<path-to-lua-script-file>` string to provide flexibility. This however carries some risks and is not ordinarily recommended.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

When the Lua code cache is turned on (by default), the user code is loaded once at the first request and cached
and the Nginx config must be reloaded each time the Lua source file is modified.
The Lua code cache can be temporarily disabled during development by
switching [lua_code_cache](#lua_code_cache) `off` in `nginx.conf` to avoid reloading Nginx.

Nginx variables are supported in the file path for dynamic dispatch, for example:

```nginx

 # CAUTION: contents in nginx var must be carefully filtered,
 # otherwise there'll be great security risk!
 location ~ ^/app/([-_a-zA-Z0-9/]+) {
     set $path $1;
     content_by_lua_file /path/to/lua/app/root/$path.lua;
 }
```

But be very careful about malicious user inputs and always carefully validate or filter out the user-supplied path components.

[Back to TOC](#directives)

## server_rewrite_by_lua_block

**syntax:** *server_rewrite_by_lua_block { lua-script }*

**context:** *http, server*

**phase:** *server rewrite*

Acts as a server rewrite phase handler and executes Lua code string specified in `{ lua-script }` for every request.
The Lua code may make [API calls](#nginx-api-for-lua) and is executed as a new spawned coroutine in an independent global environment (i.e. a sandbox).

```nginx

 server {
     ...

     server_rewrite_by_lua_block {
         ngx.ctx.a = "server_rewrite_by_lua_block in http"
     }

     location /lua {
         content_by_lua_block {
             ngx.say(ngx.ctx.a)
             ngx.log(ngx.INFO, ngx.ctx.a)
        	}
     }
 }
```

Just as any other rewrite phase handlers, [server_rewrite_by_lua_block](#server_rewrite_by_lua_block) also runs in subrequests.

```nginx

 server {
     server_rewrite_by_lua_block {
         ngx.log(ngx.INFO, "is_subrequest:", ngx.is_subrequest)
     }

     location /lua {
         content_by_lua_block {
             local res = ngx.location.capture("/sub")
             ngx.print(res.body)
         }
     }

     location /sub {
         content_by_lua_block {
             ngx.say("OK")
         }
     }
 }
```

Note that when calling `ngx.exit(ngx.OK)` within a [server_rewrite_by_lua_block](#server_rewrite_by_lua_block) handler, the Nginx request processing control flow will still continue to the content handler. To terminate the current request from within a [server_rewrite_by_lua_block](#server_rewrite_by_lua_block) handler, call [ngx.exit](#ngxexit) with status >= 200 (`ngx.HTTP_OK`) and status < 300 (`ngx.HTTP_SPECIAL_RESPONSE`) for successful quits and `ngx.exit(ngx.HTTP_INTERNAL_SERVER_ERROR)` (or its friends) for failures.


```nginx

 server_rewrite_by_lua_block {
     ngx.exit(503)
 }

 location /bar {
     ...
     # never exec
 }
```


[Back to TOC](#directives)

## server_rewrite_by_lua_file

**syntax:** *server_rewrite_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http, server*

**phase:** *server rewrite*

Equivalent to [server_rewrite_by_lua_block](#server_rewrite_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.10.22` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

Nginx variables can be used in the `<path-to-lua-script-file>` string to provide flexibility. This however carries some risks and is not ordinarily recommended.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

When the Lua code cache is turned on (by default), the user code is loaded once at the first request and cached and the Nginx config must be reloaded each time the Lua source file is modified. The Lua code cache can be temporarily disabled during development by switching [lua_code_cache](#lua_code_cache) `off` in `nginx.conf` to avoid reloading Nginx.

[Back to TOC](#directives)

## rewrite_by_lua

**syntax:** *rewrite_by_lua &lt;lua-script-str&gt;*

**context:** *http, server, location, location if*

**phase:** *rewrite tail*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [rewrite_by_lua_block](#rewrite_by_lua_block) directive instead.

Similar to the [rewrite_by_lua_block](#rewrite_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping).

For instance,

```nginx

 rewrite_by_lua '
     do_something("hello, world!\nhiya\n")
 ';
```

[Back to TOC](#directives)

## rewrite_by_lua_block

**syntax:** *rewrite_by_lua_block { lua-script }*

**context:** *http, server, location, location if*

**phase:** *rewrite tail*

Acts as a rewrite phase handler and executes Lua code string specified in `{ lua-script }` for every request.
The Lua code may make [API calls](#nginx-api-for-lua) and is executed as a new spawned coroutine in an independent global environment (i.e. a sandbox).

Note that this handler always runs *after* the standard [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html). So the following will work as expected:

```nginx

 location /foo {
     set $a 12; # create and initialize $a
     set $b ""; # create and initialize $b
     rewrite_by_lua_block {
         ngx.var.b = tonumber(ngx.var.a) + 1
     }
     echo "res = $b";
 }
```

because `set $a 12` and `set $b ""` run *before* [rewrite_by_lua_block](#rewrite_by_lua_block).

On the other hand, the following will not work as expected:

```nginx

 ?  location /foo {
 ?      set $a 12; # create and initialize $a
 ?      set $b ''; # create and initialize $b
 ?      rewrite_by_lua_block {
 ?          ngx.var.b = tonumber(ngx.var.a) + 1
 ?      }
 ?      if ($b = '13') {
 ?         rewrite ^ /bar redirect;
 ?         break;
 ?      }
 ?
 ?      echo "res = $b";
 ?  }
```

because `if` runs *before* [rewrite_by_lua_block](#rewrite_by_lua_block) even if it is placed after [rewrite_by_lua_block](#rewrite_by_lua_block) in the config.

The right way of doing this is as follows:

```nginx

 location /foo {
     set $a 12; # create and initialize $a
     set $b ''; # create and initialize $b
     rewrite_by_lua_block {
         ngx.var.b = tonumber(ngx.var.a) + 1
         if tonumber(ngx.var.b) == 13 then
             return ngx.redirect("/bar")
         end
     }

     echo "res = $b";
 }
```

Note that the [ngx_eval](http://www.grid.net.ru/nginx/eval.en.html) module can be approximated by using [rewrite_by_lua_block](#rewrite_by_lua_block). For example,

```nginx

 location / {
     eval $res {
         proxy_pass http://foo.com/check-spam;
     }

     if ($res = 'spam') {
         rewrite ^ /terms-of-use.html redirect;
     }

     fastcgi_pass ...;
 }
```

can be implemented in ngx_lua as:

```nginx

 location = /check-spam {
     internal;
     proxy_pass http://foo.com/check-spam;
 }

 location / {
     rewrite_by_lua_block {
         local res = ngx.location.capture("/check-spam")
         if res.body == "spam" then
             return ngx.redirect("/terms-of-use.html")
         end
     }

     fastcgi_pass ...;
 }
```

Just as any other rewrite phase handlers, [rewrite_by_lua_block](#rewrite_by_lua_block) also runs in subrequests.

Note that when calling `ngx.exit(ngx.OK)` within a [rewrite_by_lua_block](#rewrite_by_lua_block) handler, the Nginx request processing control flow will still continue to the content handler. To terminate the current request from within a [rewrite_by_lua_block](#rewrite_by_lua_block) handler, call [ngx.exit](#ngxexit) with status >= 200 (`ngx.HTTP_OK`) and status < 300 (`ngx.HTTP_SPECIAL_RESPONSE`) for successful quits and `ngx.exit(ngx.HTTP_INTERNAL_SERVER_ERROR)` (or its friends) for failures.

If the [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html)'s [rewrite](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#rewrite) directive is used to change the URI and initiate location re-lookups (internal redirections), then any [rewrite_by_lua_block](#rewrite_by_lua_block) or [rewrite_by_lua_file_block](#rewrite_by_lua_file_block) code sequences within the current location will not be executed. For example,

```nginx

 location /foo {
     rewrite ^ /bar;
     rewrite_by_lua_block {
         ngx.exit(503)
     }
 }
 location /bar {
     ...
 }
```

Here the Lua code `ngx.exit(503)` will never run. This will be the case if `rewrite ^ /bar last` is used as this will similarly initiate an internal redirection. If the `break` modifier is used instead, there will be no internal redirection and the `rewrite_by_lua_block` code will be executed.

The `rewrite_by_lua_block` code will always run at the end of the `rewrite` request-processing phase unless [rewrite_by_lua_no_postpone](#rewrite_by_lua_no_postpone) is turned on.

This directive was first introduced in the `v0.9.17` release.

[Back to TOC](#directives)

## rewrite_by_lua_file

**syntax:** *rewrite_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http, server, location, location if*

**phase:** *rewrite tail*

Equivalent to [rewrite_by_lua_block](#rewrite_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

Nginx variables can be used in the `<path-to-lua-script-file>` string to provide flexibility. This however carries some risks and is not ordinarily recommended.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

When the Lua code cache is turned on (by default), the user code is loaded once at the first request and cached and the Nginx config must be reloaded each time the Lua source file is modified. The Lua code cache can be temporarily disabled during development by switching [lua_code_cache](#lua_code_cache) `off` in `nginx.conf` to avoid reloading Nginx.

The `rewrite_by_lua_file` code will always run at the end of the `rewrite` request-processing phase unless [rewrite_by_lua_no_postpone](#rewrite_by_lua_no_postpone) is turned on.

Nginx variables are supported in the file path for dynamic dispatch just as in [content_by_lua_file](#content_by_lua_file).

[Back to TOC](#directives)

## access_by_lua

**syntax:** *access_by_lua &lt;lua-script-str&gt;*

**context:** *http, server, location, location if*

**phase:** *access tail*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [access_by_lua_block](#access_by_lua_block) directive instead.

Similar to the [access_by_lua_block](#access_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping).

For instance,

```nginx

 access_by_lua '
     do_something("hello, world!\nhiya\n")
 ';
```

[Back to TOC](#directives)

## access_by_lua_block

**syntax:** *access_by_lua_block { lua-script }*

**context:** *http, server, location, location if*

**phase:** *access tail*

Acts as an access phase handler and executes Lua code string specified in `{ <lua-script }` for every request.
The Lua code may make [API calls](#nginx-api-for-lua) and is executed as a new spawned coroutine in an independent global environment (i.e. a sandbox).

Note that this handler always runs *after* the standard [ngx_http_access_module](http://nginx.org/en/docs/http/ngx_http_access_module.html). So the following will work as expected:

```nginx

 location / {
     deny    192.168.1.1;
     allow   192.168.1.0/24;
     allow   10.1.1.0/16;
     deny    all;

     access_by_lua_block {
         local res = ngx.location.capture("/mysql", { ... })
         ...
     }

     # proxy_pass/fastcgi_pass/...
 }
```

That is, if a client IP address is in the blacklist, it will be denied before the MySQL query for more complex authentication is executed by [access_by_lua_block](#access_by_lua_block).

Note that the [ngx_auth_request](http://mdounin.ru/hg/ngx_http_auth_request_module/) module can be approximated by using [access_by_lua_block](#access_by_lua_block):

```nginx

 location / {
     auth_request /auth;

     # proxy_pass/fastcgi_pass/postgres_pass/...
 }
```

can be implemented in ngx_lua as:

```nginx

 location / {
     access_by_lua_block {
         local res = ngx.location.capture("/auth")

         if res.status == ngx.HTTP_OK then
             return
         end

         if res.status == ngx.HTTP_FORBIDDEN then
             ngx.exit(res.status)
         end

         ngx.exit(ngx.HTTP_INTERNAL_SERVER_ERROR)
     }

     # proxy_pass/fastcgi_pass/postgres_pass/...
 }
```

As with other access phase handlers, [access_by_lua_block](#access_by_lua_block) will *not* run in subrequests.

Note that when calling `ngx.exit(ngx.OK)` within a [access_by_lua_block](#access_by_lua_block) handler, the Nginx request processing control flow will still continue to the content handler. To terminate the current request from within a [access_by_lua_block](#access_by_lua_block) handler, call [ngx.exit](#ngxexit) with status >= 200 (`ngx.HTTP_OK`) and status < 300 (`ngx.HTTP_SPECIAL_RESPONSE`) for successful quits and `ngx.exit(ngx.HTTP_INTERNAL_SERVER_ERROR)` (or its friends) for failures.

Starting from the `v0.9.20` release, you can use the [access_by_lua_no_postpone](#access_by_lua_no_postpone)
directive to control when to run this handler inside the "access" request-processing phase
of Nginx.

This directive was first introduced in the `v0.9.17` release.

[Back to TOC](#directives)

## access_by_lua_file

**syntax:** *access_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http, server, location, location if*

**phase:** *access tail*

Equivalent to [access_by_lua_block](#access_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

Nginx variables can be used in the `<path-to-lua-script-file>` string to provide flexibility. This however carries some risks and is not ordinarily recommended.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

When the Lua code cache is turned on (by default), the user code is loaded once at the first request and cached
and the Nginx config must be reloaded each time the Lua source file is modified.
The Lua code cache can be temporarily disabled during development by switching [lua_code_cache](#lua_code_cache) `off` in `nginx.conf` to avoid repeatedly reloading Nginx.

Nginx variables are supported in the file path for dynamic dispatch just as in [content_by_lua_file](#content_by_lua_file).

[Back to TOC](#directives)

## header_filter_by_lua

**syntax:** *header_filter_by_lua &lt;lua-script-str&gt;*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [header_filter_by_lua_block](#header_filter_by_lua_block) directive instead.

Similar to the [header_filter_by_lua_block](#header_filter_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping).

For instance,

```nginx

 header_filter_by_lua '
     ngx.header["content-length"] = nil
 ';
```

This directive was first introduced in the `v0.2.1rc20` release.

[Back to TOC](#directives)

## header_filter_by_lua_block

**syntax:** *header_filter_by_lua_block { lua-script }*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

Uses Lua code specified in `{ lua-script }` to define an output header filter.

Note that the following API functions are currently disabled within this context:

* Output API functions (e.g., [ngx.say](#ngxsay) and [ngx.send_headers](#ngxsend_headers))
* Control API functions (e.g., [ngx.redirect](#ngxredirect) and [ngx.exec](#ngxexec))
* Subrequest API functions (e.g., [ngx.location.capture](#ngxlocationcapture) and [ngx.location.capture_multi](#ngxlocationcapture_multi))
* Cosocket API functions (e.g., [ngx.socket.tcp](#ngxsockettcp) and [ngx.req.socket](#ngxreqsocket)).

Here is an example of overriding a response header (or adding one if absent) in our Lua header filter:

```nginx

 location / {
     proxy_pass http://mybackend;
     header_filter_by_lua_block {
         ngx.header.Foo = "blah"
     }
 }
```

This directive was first introduced in the `v0.9.17` release.

[Back to TOC](#directives)

## header_filter_by_lua_file

**syntax:** *header_filter_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

Equivalent to [header_filter_by_lua_block](#header_filter_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.2.1rc20` release.

[Back to TOC](#directives)

## body_filter_by_lua

**syntax:** *body_filter_by_lua &lt;lua-script-str&gt;*

**context:** *http, server, location, location if*

**phase:** *output-body-filter*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [body_filter_by_lua_block](#body_filter_by_lua_block) directive instead.

Similar to the [body_filter_by_lua_block](#body_filter_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping).

For instance,

```nginx

 body_filter_by_lua '
     local data, eof = ngx.arg[1], ngx.arg[2]
 ';
```

This directive was first introduced in the `v0.5.0rc32` release.

[Back to TOC](#directives)

## body_filter_by_lua_block

**syntax:** *body_filter_by_lua_block { lua-script-str }*

**context:** *http, server, location, location if*

**phase:** *output-body-filter*

Uses Lua code specified in `{ lua-script }` to define an output body filter.

The input data chunk is passed via [ngx.arg](#ngxarg)\[1\] (as a Lua string value) and the "eof" flag indicating the end of the response body data stream is passed via [ngx.arg](#ngxarg)\[2\] (as a Lua boolean value).

Behind the scene, the "eof" flag is just the `last_buf` (for main requests) or `last_in_chain` (for subrequests) flag of the Nginx chain link buffers. (Before the `v0.7.14` release, the "eof" flag does not work at all in subrequests.)

The output data stream can be aborted immediately by running the following Lua statement:

```lua

 return ngx.ERROR
```

This will truncate the response body and usually result in incomplete and also invalid responses.

The Lua code can pass its own modified version of the input data chunk to the downstream Nginx output body filters by overriding [ngx.arg](#ngxarg)\[1\] with a Lua string or a Lua table of strings. For example, to transform all the lowercase letters in the response body, we can just write:

```nginx

 location / {
     proxy_pass http://mybackend;
     body_filter_by_lua_block {
         ngx.arg[1] = string.upper(ngx.arg[1])
     }
 }
```

When setting `nil` or an empty Lua string value to `ngx.arg[1]`, no data chunk will be passed to the downstream Nginx output filters at all.

Likewise, new "eof" flag can also be specified by setting a boolean value to [ngx.arg](#ngxarg)\[2\]. For example,

```nginx

 location /t {
     echo hello world;
     echo hiya globe;

     body_filter_by_lua_block {
         local chunk = ngx.arg[1]
         if string.match(chunk, "hello") then
             ngx.arg[2] = true  -- new eof
             return
         end

         -- just throw away any remaining chunk data
         ngx.arg[1] = nil
     }
 }
```

Then `GET /t` will just return the output


    hello world


That is, when the body filter sees a chunk containing the word "hello", then it will set the "eof" flag to true immediately, resulting in truncated but still valid responses.

When the Lua code may change the length of the response body, then it is required to always clear out the `Content-Length` response header (if any) in a header filter to enforce streaming output, as in

```nginx

 location /foo {
     # fastcgi_pass/proxy_pass/...

     header_filter_by_lua_block {
         ngx.header.content_length = nil
     }
     body_filter_by_lua_block {
         ngx.arg[1] = string.len(ngx.arg[1]) .. "\n"
     }
 }
```

Note that the following API functions are currently disabled within this context due to the limitations in Nginx output filter's current implementation:

* Output API functions (e.g., [ngx.say](#ngxsay) and [ngx.send_headers](#ngxsend_headers))
* Control API functions (e.g., [ngx.exit](#ngxexit) and [ngx.exec](#ngxexec))
* Subrequest API functions (e.g., [ngx.location.capture](#ngxlocationcapture) and [ngx.location.capture_multi](#ngxlocationcapture_multi))
* Cosocket API functions (e.g., [ngx.socket.tcp](#ngxsockettcp) and [ngx.req.socket](#ngxreqsocket)).

Nginx output filters may be called multiple times for a single request because response body may be delivered in chunks. Thus, the Lua code specified by in this directive may also run multiple times in the lifetime of a single HTTP request.

This directive was first introduced in the `v0.9.17` release.

[Back to TOC](#directives)

## body_filter_by_lua_file

**syntax:** *body_filter_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http, server, location, location if*

**phase:** *output-body-filter*

Equivalent to [body_filter_by_lua_block](#body_filter_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.5.0rc32` release.

[Back to TOC](#directives)

## log_by_lua

**syntax:** *log_by_lua &lt;lua-script-str&gt;*

**context:** *http, server, location, location if*

**phase:** *log*

**NOTE** Use of this directive is *discouraged* following the `v0.9.17` release. Use the [log_by_lua_block](#log_by_lua_block) directive instead.

Similar to the [log_by_lua_block](#log_by_lua_block) directive, but accepts the Lua source directly in an Nginx string literal (which requires
special character escaping).

For instance,

```nginx

 log_by_lua '
     print("I need no extra escaping here, for example: \r\nblah")
 ';
```

This directive was first introduced in the `v0.5.0rc31` release.

[Back to TOC](#directives)

## log_by_lua_block

**syntax:** *log_by_lua_block { lua-script }*

**context:** *http, server, location, location if*

**phase:** *log*

Runs the Lua source code inlined as the `{ lua-script }` at the `log` request processing phase. This does not replace the current access logs, but runs before.

Note that the following API functions are currently disabled within this context:

* Output API functions (e.g., [ngx.say](#ngxsay) and [ngx.send_headers](#ngxsend_headers))
* Control API functions (e.g., [ngx.exit](#ngxexit))
* Subrequest API functions (e.g., [ngx.location.capture](#ngxlocationcapture) and [ngx.location.capture_multi](#ngxlocationcapture_multi))
* Cosocket API functions (e.g., [ngx.socket.tcp](#ngxsockettcp) and [ngx.req.socket](#ngxreqsocket)).

Here is an example of gathering average data for [$upstream_response_time](http://nginx.org/en/docs/http/ngx_http_upstream_module.html#var_upstream_response_time):

```nginx

 lua_shared_dict log_dict 5M;

 server {
     location / {
         proxy_pass http://mybackend;

         log_by_lua_block {
             local log_dict = ngx.shared.log_dict
             local upstream_time = tonumber(ngx.var.upstream_response_time)

             local sum = log_dict:get("upstream_time-sum") or 0
             sum = sum + upstream_time
             log_dict:set("upstream_time-sum", sum)

             local newval, err = log_dict:incr("upstream_time-nb", 1)
             if not newval and err == "not found" then
                 log_dict:add("upstream_time-nb", 0)
                 log_dict:incr("upstream_time-nb", 1)
             end
         }
     }

     location = /status {
         content_by_lua_block {
             local log_dict = ngx.shared.log_dict
             local sum = log_dict:get("upstream_time-sum")
             local nb = log_dict:get("upstream_time-nb")

             if nb and sum then
                 ngx.say("average upstream response time: ", sum / nb,
                         " (", nb, " reqs)")
             else
                 ngx.say("no data yet")
             end
         }
     }
 }
```

This directive was first introduced in the `v0.9.17` release.

[Back to TOC](#directives)

## log_by_lua_file

**syntax:** *log_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http, server, location, location if*

**phase:** *log*

Equivalent to [log_by_lua_block](#log_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.5.0rc31` release.

[Back to TOC](#directives)

## balancer_by_lua_block

**syntax:** *balancer_by_lua_block { lua-script }*

**context:** *upstream*

**phase:** *content*

This directive runs Lua code as an upstream balancer for any upstream entities defined
by the `upstream {}` configuration block.

For instance,

```nginx

 upstream foo {
     server 127.0.0.1;
     balancer_by_lua_block {
         -- use Lua to do something interesting here
         -- as a dynamic balancer
     }
 }

 server {
     location / {
         proxy_pass http://foo;
     }
 }
```

The resulting Lua load balancer can work with any existing Nginx upstream modules
like [ngx_proxy](https://nginx.org/en/docs/http/ngx_http_proxy_module.html) and
[ngx_fastcgi](https://nginx.org/en/docs/http/ngx_http_fastcgi_module.html).

Also, the Lua load balancer can work with the standard upstream connection pool mechanism,
i.e., the standard [keepalive](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive) directive.
Just ensure that the [keepalive](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive) directive
is used *after* this `balancer_by_lua_block` directive in a single `upstream {}` configuration block.

The Lua load balancer can totally ignore the list of servers defined in the `upstream {}` block
and select peer from a completely dynamic server list (even changing per request) via the
[ngx.balancer](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/balancer.md) module
from the [lua-resty-core](https://github.com/openresty/lua-resty-core) library.

The Lua code handler registered by this directive might get called more than once in a single
downstream request when the Nginx upstream mechanism retries the request on conditions
specified by directives like the [proxy_next_upstream](https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_next_upstream)
directive.

This Lua code execution context does not support yielding, so Lua APIs that may yield
(like cosockets and "light threads") are disabled in this context. One can usually work
around this limitation by doing such operations in an earlier phase handler (like
[access_by_lua*](#access_by_lua)) and passing along the result into this context
via the [ngx.ctx](#ngxctx) table.

This directive was first introduced in the `v0.10.0` release.

[Back to TOC](#directives)

## balancer_by_lua_file

**syntax:** *balancer_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *upstream*

**phase:** *content*

Equivalent to [balancer_by_lua_block](#balancer_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.10.0` release.

[Back to TOC](#directives)

## lua_need_request_body

**syntax:** *lua_need_request_body &lt;on|off&gt;*

**default:** *off*

**context:** *http, server, location, location if*

**phase:** *depends on usage*

Determines whether to force the request body data to be read before running rewrite/access/content_by_lua* or not. The Nginx core does not read the client request body by default and if request body data is required, then this directive should be turned `on` or the [ngx.req.read_body](#ngxreqread_body) function should be called within the Lua code.

To read the request body data within the [$request_body](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_body) variable,
[client_body_buffer_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size) must have the same value as [client_max_body_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size). Because when the content length exceeds [client_body_buffer_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size) but less than [client_max_body_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size), Nginx will buffer the data into a temporary file on the disk, which will lead to empty value in the [$request_body](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_body) variable.

If the current location includes [rewrite_by_lua*](#rewrite_by_lua) directives,
then the request body will be read just before the [rewrite_by_lua*](#rewrite_by_lua) code is run (and also at the
`rewrite` phase). Similarly, if only [content_by_lua](#content_by_lua) is specified,
the request body will not be read until the content handler's Lua code is
about to run (i.e., the request body will be read during the content phase).

It is recommended however, to use the [ngx.req.read_body](#ngxreqread_body) and [ngx.req.discard_body](#ngxreqdiscard_body) functions for finer control over the request body reading process instead.

This also applies to [access_by_lua*](#access_by_lua).

[Back to TOC](#directives)

## ssl_client_hello_by_lua_block

**syntax:** *ssl_client_hello_by_lua_block { lua-script }*

**context:** *http, server*

**phase:** *right-after-client-hello-message-was-processed*

This directive runs user Lua code when Nginx is about to post-process the SSL client hello message for the downstream
SSL (https) connections.

It is particularly useful for dynamically setting the SSL protocols according to the SNI.

It is also useful to do some custom operations according to the per-connection information in the client hello message.

For example, one can parse custom client hello extension and do the corresponding handling in pure Lua.

This Lua handler will always run whether the SSL session is resumed (via SSL session IDs or TLS session tickets) or not.
While the `ssl_certificate_by_lua*` Lua handler will only runs when initiating a full SSL handshake.

The [ngx.ssl.clienthello](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl/clienthello.md) Lua modules
provided by the [lua-resty-core](https://github.com/openresty/lua-resty-core/#readme)
library are particularly useful in this context.

Note that this handler runs in extremely early stage of SSL handshake, before the SSL client hello extensions are parsed.
So you can not use some Lua API like `ssl.server_name()` which is dependent on the later stage's processing.

Also note that only the directive in default server is valid for several virtual servers with the same IP address and port.

Below is a trivial example using the
[ngx.ssl.clienthello](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl/clienthello.md) module
at the same time:

```nginx

 server {
     listen 443 ssl;
     server_name   test.com;
     ssl_certificate /path/to/cert.crt;
     ssl_certificate_key /path/to/key.key;
     ssl_client_hello_by_lua_block {
         local ssl_clt = require "ngx.ssl.clienthello"
         local host, err = ssl_clt.get_client_hello_server_name()
         if host == "test.com" then
             ssl_clt.set_protocols({"TLSv1", "TLSv1.1"})
         elseif host == "test2.com" then
             ssl_clt.set_protocols({"TLSv1.2", "TLSv1.3"})
         elseif not host then
             ngx.log(ngx.ERR, "failed to get the SNI name: ", err)
             ngx.exit(ngx.ERROR)
         else
             ngx.log(ngx.ERR, "unknown SNI name: ", host)
             ngx.exit(ngx.ERROR)
         end
     }
     ...
 }
 server {
     listen 443 ssl;
     server_name   test2.com;
     ssl_certificate /path/to/cert.crt;
     ssl_certificate_key /path/to/key.key;
     ...
 }
```

See more information in the [ngx.ssl.clienthello](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl/clienthello.md)
Lua modules' official documentation.

Uncaught Lua exceptions in the user Lua code immediately abort the current SSL session, so does the
[ngx.exit](#ngxexit) call with an error code like `ngx.ERROR`.

This Lua code execution context *does* support yielding, so Lua APIs that may yield
(like cosockets, sleeping, and "light threads")
are enabled in this context

Note, you need to configure the [ssl_certificate](https://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_certificate)
and [ssl_certificate_key](https://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_certificate_key)
to avoid the following error while starting NGINX:


    nginx: [emerg] no ssl configured for the server


This directive requires OpenSSL 1.1.1 or greater.

If you are using the [official pre-built
packages](https://openresty.org/en/linux-packages.html) for
[OpenResty](https://openresty.org/) 1.21.4.1 or later, then everything should
work out of the box.

If you are not using the Nginx core shipped with
[OpenResty](https://openresty.org) 1.21.4.1 or later, you will need to apply
patches to the standard Nginx core:

<https://openresty.org/en/nginx-ssl-patches.html>

This directive was first introduced in the `v0.10.21` release.

[Back to TOC](#directives)

## ssl_client_hello_by_lua_file

**syntax:** *ssl_client_hello_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http, server*

**phase:** *right-after-client-hello-message-was-processed*

Equivalent to [ssl_client_hello_by_lua_block](#ssl_client_hello_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.10.21` release.

[Back to TOC](#directives)

## ssl_certificate_by_lua_block

**syntax:** *ssl_certificate_by_lua_block { lua-script }*

**context:** *server*

**phase:** *right-before-SSL-handshake*

This directive runs user Lua code when Nginx is about to start the SSL handshake for the downstream
SSL (https) connections.

It is particularly useful for setting the SSL certificate chain and the corresponding private key on a per-request
basis. It is also useful to load such handshake configurations nonblockingly from the remote (for example,
with the [cosocket](#ngxsockettcp) API). And one can also do per-request OCSP stapling handling in pure
Lua here as well.

Another typical use case is to do SSL handshake traffic control nonblockingly in this context,
with the help of the [lua-resty-limit-traffic#readme](https://github.com/openresty/lua-resty-limit-traffic)
library, for example.

One can also do interesting things with the SSL handshake requests from the client side, like
rejecting old SSL clients using the SSLv3 protocol or even below selectively.

The [ngx.ssl](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl.md)
and [ngx.ocsp](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ocsp.md) Lua modules
provided by the [lua-resty-core](https://github.com/openresty/lua-resty-core/#readme)
library are particularly useful in this context. You can use the Lua API offered by these two Lua modules
to manipulate the SSL certificate chain and private key for the current SSL connection
being initiated.

This Lua handler does not run at all, however, when Nginx/OpenSSL successfully resumes
the SSL session via SSL session IDs or TLS session tickets for the current SSL connection. In
other words, this Lua handler only runs when Nginx has to initiate a full SSL handshake.

Below is a trivial example using the
[ngx.ssl](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl.md) module
at the same time:

```nginx

 server {
     listen 443 ssl;
     server_name   test.com;

     ssl_certificate_by_lua_block {
         print("About to initiate a new SSL handshake!")
     }

     location / {
         root html;
     }
 }
```

See more complicated examples in the [ngx.ssl](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl.md)
and [ngx.ocsp](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ocsp.md)
Lua modules' official documentation.

Uncaught Lua exceptions in the user Lua code immediately abort the current SSL session, so does the
[ngx.exit](#ngxexit) call with an error code like `ngx.ERROR`.

This Lua code execution context *does* support yielding, so Lua APIs that may yield
(like cosockets, sleeping, and "light threads")
are enabled in this context.

Note, however, you still need to configure the [ssl_certificate](https://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_certificate) and
[ssl_certificate_key](https://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_certificate_key)
directives even though you will not use this static certificate and private key at all. This is
because the NGINX core requires their appearance otherwise you are seeing the following error
while starting NGINX:


    nginx: [emerg] no ssl configured for the server


This directive requires OpenSSL 1.0.2e or greater.

If you are using the [official pre-built
packages](https://openresty.org/en/linux-packages.html) for
[OpenResty](https://openresty.org/) 1.9.7.2 or later, then everything should
work out of the box.

If you are not using the Nginx core shipped with
[OpenResty](https://openresty.org) 1.9.7.2 or later, you will need to apply
patches to the standard Nginx core:

<https://openresty.org/en/nginx-ssl-patches.html>

This directive was first introduced in the `v0.10.0` release.

[Back to TOC](#directives)

## ssl_certificate_by_lua_file

**syntax:** *ssl_certificate_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *server*

**phase:** *right-before-SSL-handshake*

Equivalent to [ssl_certificate_by_lua_block](#ssl_certificate_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or, as from the `v0.5.0rc32` release, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.10.0` release.

[Back to TOC](#directives)

## ssl_session_fetch_by_lua_block

**syntax:** *ssl_session_fetch_by_lua_block { lua-script }*

**context:** *http*

**phase:** *right-before-SSL-handshake*

This directive runs Lua code to look up and load the SSL session (if any) according to the session ID
provided by the current SSL handshake request for the downstream.

The Lua API for obtaining the current session ID and loading a cached SSL session data
is provided in the [ngx.ssl.session](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl/session.md)
Lua module shipped with the [lua-resty-core](https://github.com/openresty/lua-resty-core#readme)
library.

Lua APIs that may yield, like [ngx.sleep](#ngxsleep) and [cosockets](#ngxsockettcp),
are enabled in this context.

This hook, together with the [ssl_session_store_by_lua*](#ssl_session_store_by_lua_block) hook,
can be used to implement distributed caching mechanisms in pure Lua (based
on the [cosocket](#ngxsockettcp) API, for example). If a cached SSL session is found
and loaded into the current SSL connection context,
SSL session resumption can then get immediately initiated and bypass the full SSL handshake process which is very expensive in terms of CPU time.

Please note that TLS session tickets are very different and it is the clients' responsibility
to cache the SSL session state when session tickets are used. SSL session resumptions based on
TLS session tickets would happen automatically without going through this hook (nor the
[ssl_session_store_by_lua*](#ssl_session_store_by_lua_block) hook). This hook is mainly
for older or less capable SSL clients that can only do SSL sessions by session IDs.

When [ssl_certificate_by_lua*](#ssl_certificate_by_lua_block) is specified at the same time,
this hook usually runs before [ssl_certificate_by_lua*](#ssl_certificate_by_lua_block).
When the SSL session is found and successfully loaded for the current SSL connection,
SSL session resumption will happen and thus bypass the [ssl_certificate_by_lua*](#ssl_certificate_by_lua_block)
hook completely. In this case, Nginx also bypasses the [ssl_session_store_by_lua*](#ssl_session_store_by_lua_block)
hook, for obvious reasons.

To easily test this hook locally with a modern web browser, you can temporarily put the following line
in your https server block to disable the TLS session ticket support:

    ssl_session_tickets off;

But do not forget to comment this line out before publishing your site to the world.

If you are using the [official pre-built packages](https://openresty.org/en/linux-packages.html) for [OpenResty](https://openresty.org/)
1.11.2.1 or later, then everything should work out of the box.

If you are not using one of the [OpenSSL
packages](https://openresty.org/en/linux-packages.html) provided by
[OpenResty](https://openresty.org), you will need to apply patches to OpenSSL
in order to use this directive:

<https://openresty.org/en/openssl-patches.html>

Similarly, if you are not using the Nginx core shipped with
[OpenResty](https://openresty.org) 1.11.2.1 or later, you will need to apply
patches to the standard Nginx core:

<https://openresty.org/en/nginx-ssl-patches.html>

This directive was first introduced in the `v0.10.6` release.

Note that this directive can only be used in the **http context** starting
with the `v0.10.7` release since SSL session resumption happens
before server name dispatch.

[Back to TOC](#directives)

## ssl_session_fetch_by_lua_file

**syntax:** *ssl_session_fetch_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http*

**phase:** *right-before-SSL-handshake*

Equivalent to [ssl_session_fetch_by_lua_block](#ssl_session_fetch_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or rather, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.10.6` release.

Note that: this directive is only allowed to used in **http context** from the `v0.10.7` release
(because SSL session resumption happens before server name dispatch).

[Back to TOC](#directives)

## ssl_session_store_by_lua_block

**syntax:** *ssl_session_store_by_lua_block { lua-script }*

**context:** *http*

**phase:** *right-after-SSL-handshake*

This directive runs Lua code to fetch and save the SSL session (if any) according to the session ID
provided by the current SSL handshake request for the downstream. The saved or cached SSL
session data can be used for future SSL connections to resume SSL sessions without going
through the full SSL handshake process (which is very expensive in terms of CPU time).

Lua APIs that may yield, like [ngx.sleep](#ngxsleep) and [cosockets](#ngxsockettcp),
are *disabled* in this context. You can still, however, use the [ngx.timer.at](#ngxtimerat) API
to create 0-delay timers to save the SSL session data asynchronously to external services (like `redis` or `memcached`).

The Lua API for obtaining the current session ID and the associated session state data
is provided in the [ngx.ssl.session](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl/session.md#readme)
Lua module shipped with the [lua-resty-core](https://github.com/openresty/lua-resty-core#readme)
library.

To easily test this hook locally with a modern web browser, you can temporarily put the following line
in your https server block to disable the TLS session ticket support:

    ssl_session_tickets off;

But do not forget to comment this line out before publishing your site to the world.

This directive was first introduced in the `v0.10.6` release.

Note that: this directive is only allowed to used in **http context** from the `v0.10.7` release
(because SSL session resumption happens before server name dispatch).

[Back to TOC](#directives)

## ssl_session_store_by_lua_file

**syntax:** *ssl_session_store_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *http*

**phase:** *right-after-SSL-handshake*

Equivalent to [ssl_session_store_by_lua_block](#ssl_session_store_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code, or rather, the [LuaJIT bytecode](#luajit-bytecode-support) to be executed.

When a relative path like `foo/bar.lua` is given, they will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option while starting the Nginx server.

This directive was first introduced in the `v0.10.6` release.

Note that: this directive is only allowed to used in **http context** from the `v0.10.7` release
(because SSL session resumption happens before server name dispatch).

[Back to TOC](#directives)

## lua_shared_dict

**syntax:** *lua_shared_dict &lt;name&gt; &lt;size&gt;*

**default:** *no*

**context:** *http*

**phase:** *depends on usage*

Declares a shared memory zone, `<name>`, to serve as storage for the shm based Lua dictionary `ngx.shared.<name>`.

Shared memory zones are always shared by all the Nginx worker processes in the current Nginx server instance.

The `<size>` argument accepts size units such as `k` and `m`:

```nginx

 http {
     lua_shared_dict dogs 10m;
     ...
 }
```

The hard-coded minimum size is 8KB while the practical minimum size depends
on actual user data set (some people start with 12KB).

See [ngx.shared.DICT](#ngxshareddict) for details.

This directive was first introduced in the `v0.3.1rc22` release.

[Back to TOC](#directives)

## lua_socket_connect_timeout

**syntax:** *lua_socket_connect_timeout &lt;time&gt;*

**default:** *lua_socket_connect_timeout 60s*

**context:** *http, server, location*

This directive controls the default timeout value used in TCP/unix-domain socket object's [connect](#tcpsockconnect) method and can be overridden by the [settimeout](#tcpsocksettimeout) or [settimeouts](#tcpsocksettimeouts) methods.

The `<time>` argument can be an integer, with an optional time unit, like `s` (second), `ms` (millisecond), `m` (minute). The default time unit is `s`, i.e., "second". The default setting is `60s`.

This directive was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#directives)

## lua_socket_send_timeout

**syntax:** *lua_socket_send_timeout &lt;time&gt;*

**default:** *lua_socket_send_timeout 60s*

**context:** *http, server, location*

Controls the default timeout value used in TCP/unix-domain socket object's [send](#tcpsocksend) method and can be overridden by the [settimeout](#tcpsocksettimeout) or [settimeouts](#tcpsocksettimeouts) methods.

The `<time>` argument can be an integer, with an optional time unit, like `s` (second), `ms` (millisecond), `m` (minute). The default time unit is `s`, i.e., "second". The default setting is `60s`.

This directive was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#directives)

## lua_socket_send_lowat

**syntax:** *lua_socket_send_lowat &lt;size&gt;*

**default:** *lua_socket_send_lowat 0*

**context:** *http, server, location*

Controls the `lowat` (low water) value for the cosocket send buffer.

[Back to TOC](#directives)

## lua_socket_read_timeout

**syntax:** *lua_socket_read_timeout &lt;time&gt;*

**default:** *lua_socket_read_timeout 60s*

**context:** *http, server, location*

**phase:** *depends on usage*

This directive controls the default timeout value used in TCP/unix-domain socket object's [receive](#tcpsockreceive) method and iterator functions returned by the [receiveuntil](#tcpsockreceiveuntil) method. This setting can be overridden by the [settimeout](#tcpsocksettimeout) or [settimeouts](#tcpsocksettimeouts) methods.

The `<time>` argument can be an integer, with an optional time unit, like `s` (second), `ms` (millisecond), `m` (minute). The default time unit is `s`, i.e., "second". The default setting is `60s`.

This directive was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#directives)

## lua_socket_buffer_size

**syntax:** *lua_socket_buffer_size &lt;size&gt;*

**default:** *lua_socket_buffer_size 4k/8k*

**context:** *http, server, location*

Specifies the buffer size used by cosocket reading operations.

This buffer does not have to be that big to hold everything at the same time because cosocket supports 100% non-buffered reading and parsing. So even `1` byte buffer size should still work everywhere but the performance could be terrible.

This directive was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#directives)

## lua_socket_pool_size

**syntax:** *lua_socket_pool_size &lt;size&gt;*

**default:** *lua_socket_pool_size 30*

**context:** *http, server, location*

Specifies the size limit (in terms of connection count) for every cosocket connection pool associated with every remote server (i.e., identified by either the host-port pair or the unix domain socket file path).

Default to 30 connections for every pool.

When the connection pool exceeds the available size limit, the least recently used (idle) connection already in the pool will be closed to make room for the current connection.

Note that the cosocket connection pool is per Nginx worker process rather than per Nginx server instance, so size limit specified here also applies to every single Nginx worker process.

This directive was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#directives)

## lua_socket_keepalive_timeout

**syntax:** *lua_socket_keepalive_timeout &lt;time&gt;*

**default:** *lua_socket_keepalive_timeout 60s*

**context:** *http, server, location*

This directive controls the default maximal idle time of the connections in the cosocket built-in connection pool. When this timeout reaches, idle connections will be closed and removed from the pool. This setting can be overridden by cosocket objects' [setkeepalive](#tcpsocksetkeepalive) method.

The `<time>` argument can be an integer, with an optional time unit, like `s` (second), `ms` (millisecond), `m` (minute). The default time unit is `s`, i.e., "second". The default setting is `60s`.

This directive was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#directives)

## lua_socket_log_errors

**syntax:** *lua_socket_log_errors on|off*

**default:** *lua_socket_log_errors on*

**context:** *http, server, location*

This directive can be used to toggle error logging when a failure occurs for the TCP or UDP cosockets. If you are already doing proper error handling and logging in your Lua code, then it is recommended to turn this directive off to prevent data flushing in your Nginx error log files (which is usually rather expensive).

This directive was first introduced in the `v0.5.13` release.

[Back to TOC](#directives)

## lua_ssl_ciphers

**syntax:** *lua_ssl_ciphers &lt;ciphers&gt;*

**default:** *lua_ssl_ciphers DEFAULT*

**context:** *http, server, location*

Specifies the enabled ciphers for requests to a SSL/TLS server in the [tcpsock:sslhandshake](#tcpsocksslhandshake) method. The ciphers are specified in the format understood by the OpenSSL library.

The full list can be viewed using the “openssl ciphers” command.

This directive was first introduced in the `v0.9.11` release.

[Back to TOC](#directives)

## lua_ssl_crl

**syntax:** *lua_ssl_crl &lt;file&gt;*

**default:** *no*

**context:** *http, server, location*

Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of the SSL/TLS server in the [tcpsock:sslhandshake](#tcpsocksslhandshake) method.

This directive was first introduced in the `v0.9.11` release.

[Back to TOC](#directives)

## lua_ssl_protocols

**syntax:** *lua_ssl_protocols \[SSLv2\] \[SSLv3\] \[TLSv1\] [TLSv1.1] [TLSv1.2] [TLSv1.3]*

**default:** *lua_ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2*

**context:** *http, server, location*

Enables the specified protocols for requests to a SSL/TLS server in the [tcpsock:sslhandshake](#tcpsocksslhandshake) method.

The support for the `TLSv1.3` parameter requires version `v0.10.12` *and* OpenSSL 1.1.1.

This directive was first introduced in the `v0.9.11` release.

[Back to TOC](#directives)

## lua_ssl_trusted_certificate

**syntax:** *lua_ssl_trusted_certificate &lt;file&gt;*

**default:** *no*

**context:** *http, server, location*

Specifies a file path with trusted CA certificates in the PEM format used to verify the certificate of the SSL/TLS server in the [tcpsock:sslhandshake](#tcpsocksslhandshake) method.

This directive was first introduced in the `v0.9.11` release.

See also [lua_ssl_verify_depth](#lua_ssl_verify_depth).

[Back to TOC](#directives)

## lua_ssl_verify_depth

**syntax:** *lua_ssl_verify_depth &lt;number&gt;*

**default:** *lua_ssl_verify_depth 1*

**context:** *http, server, location*

Sets the verification depth in the server certificates chain.

This directive was first introduced in the `v0.9.11` release.

See also [lua_ssl_trusted_certificate](#lua_ssl_trusted_certificate).

[Back to TOC](#directives)

## lua_ssl_conf_command

**syntax:** *lua_ssl_conf_command &lt;command&gt;*

**default:** *no*

**context:** *http, server, location*

Sets arbitrary OpenSSL configuration [commands](https://www.openssl.org/docs/man1.1.1/man3/SSL_CONF_cmd.html).

The directive is supported when using OpenSSL 1.0.2 or higher and nginx 1.19.4 or higher. According to the specify command, higher OpenSSL version may be needed.

Several `lua_ssl_conf_command` directives can be specified on the same level:

```nginx

 lua_ssl_conf_command Options PrioritizeChaCha;
 lua_ssl_conf_command Ciphersuites TLS_CHACHA20_POLY1305_SHA256;
```

Configuration commands are applied after OpenResty own configuration for SSL, so they can be used to override anything set by OpenResty.

Note though that configuring OpenSSL directly with `lua_ssl_conf_command` might result in a behaviour OpenResty does not expect, and should be done with care.

This directive was first introduced in the `v0.10.21` release.



[Back to TOC](#directives)

## lua_http10_buffering

**syntax:** *lua_http10_buffering on|off*

**default:** *lua_http10_buffering on*

**context:** *http, server, location, location-if*

Enables or disables automatic response buffering for HTTP 1.0 (or older) requests. This buffering mechanism is mainly used for HTTP 1.0 keep-alive which relies on a proper `Content-Length` response header.

If the Lua code explicitly sets a `Content-Length` response header before sending the headers (either explicitly via [ngx.send_headers](#ngxsend_headers) or implicitly via the first [ngx.say](#ngxsay) or [ngx.print](#ngxprint) call), then the HTTP 1.0 response buffering will be disabled even when this directive is turned on.

To output very large response data in a streaming fashion (via the [ngx.flush](#ngxflush) call, for example), this directive MUST be turned off to minimize memory usage.

This directive is turned `on` by default.

This directive was first introduced in the `v0.5.0rc19` release.

[Back to TOC](#directives)

## rewrite_by_lua_no_postpone

**syntax:** *rewrite_by_lua_no_postpone on|off*

**default:** *rewrite_by_lua_no_postpone off*

**context:** *http*

Controls whether or not to disable postponing [rewrite_by_lua*](#rewrite_by_lua) directives to run at the end of the `rewrite` request-processing phase. By default, this directive is turned off and the Lua code is postponed to run at the end of the `rewrite` phase.

This directive was first introduced in the `v0.5.0rc29` release.

[Back to TOC](#directives)

## access_by_lua_no_postpone

**syntax:** *access_by_lua_no_postpone on|off*

**default:** *access_by_lua_no_postpone off*

**context:** *http*

Controls whether or not to disable postponing [access_by_lua*](#access_by_lua) directives to run at the end of the `access` request-processing phase. By default, this directive is turned off and the Lua code is postponed to run at the end of the `access` phase.

This directive was first introduced in the `v0.9.20` release.

[Back to TOC](#directives)

## lua_transform_underscores_in_response_headers

**syntax:** *lua_transform_underscores_in_response_headers on|off*

**default:** *lua_transform_underscores_in_response_headers on*

**context:** *http, server, location, location-if*

Controls whether to transform underscores (`_`) in the response header names specified in the [ngx.header.HEADER](#ngxheaderheader) API to hyphens (`-`).

This directive was first introduced in the `v0.5.0rc32` release.

[Back to TOC](#directives)

## lua_check_client_abort

**syntax:** *lua_check_client_abort on|off*

**default:** *lua_check_client_abort off*

**context:** *http, server, location, location-if*

This directive controls whether to check for premature client connection abortion.

When this directive is on, the ngx_lua module will monitor the premature connection close event on the downstream connections and when there is such an event, it will call the user Lua function callback (registered by [ngx.on_abort](#ngxon_abort)) or just stop and clean up all the Lua "light threads" running in the current request's request handler when there is no user callback function registered.

According to the current implementation, however, if the client closes the connection before the Lua code finishes reading the request body data via [ngx.req.socket](#ngxreqsocket), then ngx_lua will neither stop all the running "light threads" nor call the user callback (if [ngx.on_abort](#ngxon_abort) has been called). Instead, the reading operation on [ngx.req.socket](#ngxreqsocket) will just return the error message "client aborted" as the second return value (the first return value is surely `nil`).

When TCP keepalive is disabled, it is relying on the client side to close the socket gracefully (by sending a `FIN` packet or something like that). For (soft) real-time web applications, it is highly recommended to configure the [TCP keepalive](http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html) support in your system's TCP stack implementation in order to detect "half-open" TCP connections in time.

For example, on Linux, you can configure the standard [listen](http://nginx.org/en/docs/http/ngx_http_core_module.html#listen) directive in your `nginx.conf` file like this:

```nginx

 listen 80 so_keepalive=2s:2s:8;
```

On FreeBSD, you can only tune the system-wide configuration for TCP keepalive, for example:

    # sysctl net.inet.tcp.keepintvl=2000
    # sysctl net.inet.tcp.keepidle=2000

This directive was first introduced in the `v0.7.4` release.

See also [ngx.on_abort](#ngxon_abort).

[Back to TOC](#directives)

## lua_max_pending_timers

**syntax:** *lua_max_pending_timers &lt;count&gt;*

**default:** *lua_max_pending_timers 1024*

**context:** *http*

Controls the maximum number of pending timers allowed.

Pending timers are those timers that have not expired yet.

When exceeding this limit, the [ngx.timer.at](#ngxtimerat) call will immediately return `nil` and the error string "too many pending timers".

This directive was first introduced in the `v0.8.0` release.

[Back to TOC](#directives)

## lua_max_running_timers

**syntax:** *lua_max_running_timers &lt;count&gt;*

**default:** *lua_max_running_timers 256*

**context:** *http*

Controls the maximum number of "running timers" allowed.

Running timers are those timers whose user callback functions are still running or `lightthreads` spawned in callback functions are still running.

When exceeding this limit, Nginx will stop running the callbacks of newly expired timers and log an error message "N lua_max_running_timers are not enough" where "N" is the current value of this directive.

This directive was first introduced in the `v0.8.0` release.

[Back to TOC](#directives)

## lua_sa_restart

**syntax:** *lua_sa_restart on|off*

**default:** *lua_sa_restart on*

**context:** *http*

When enabled, this module will set the `SA_RESTART` flag on Nginx workers signal dispositions.

This allows Lua I/O primitives to not be interrupted by Nginx's handling of various signals.

This directive was first introduced in the `v0.10.14` release.

[Back to TOC](#directives)

## lua_worker_thread_vm_pool_size

**syntax:** *lua_worker_thread_vm_pool_size &lt;size&gt;*

**default:** *lua_worker_thread_vm_pool_size 100*

**context:** *http*

Specifies the size limit of the Lua VM pool (default 100) that will be used in the [ngx.run_worker_thread](#ngxrun_worker_thread) API.

Also, it is not allowed to create Lua VMs that exceeds the pool size limit.

The Lua VM in the VM pool is used to execute Lua code in separate thread.

The pool is global at Nginx worker level. And it is used to reuse Lua VMs between requests.

[Back to TOC](#directives)

## Nginx API for Lua

* [Introduction](#introduction)
* [ngx.arg](#ngxarg)
* [ngx.var.VARIABLE](#ngxvarvariable)
* [Core constants](#core-constants)
* [HTTP method constants](#http-method-constants)
* [HTTP status constants](#http-status-constants)
* [Nginx log level constants](#nginx-log-level-constants)
* [print](#print)
* [ngx.ctx](#ngxctx)
* [ngx.location.capture](#ngxlocationcapture)
* [ngx.location.capture_multi](#ngxlocationcapture_multi)
* [ngx.status](#ngxstatus)
* [ngx.header.HEADER](#ngxheaderheader)
* [ngx.resp.get_headers](#ngxrespget_headers)
* [ngx.req.is_internal](#ngxreqis_internal)
* [ngx.req.start_time](#ngxreqstart_time)
* [ngx.req.http_version](#ngxreqhttp_version)
* [ngx.req.raw_header](#ngxreqraw_header)
* [ngx.req.get_method](#ngxreqget_method)
* [ngx.req.set_method](#ngxreqset_method)
* [ngx.req.set_uri](#ngxreqset_uri)
* [ngx.req.set_uri_args](#ngxreqset_uri_args)
* [ngx.req.get_uri_args](#ngxreqget_uri_args)
* [ngx.req.get_post_args](#ngxreqget_post_args)
* [ngx.req.get_headers](#ngxreqget_headers)
* [ngx.req.set_header](#ngxreqset_header)
* [ngx.req.clear_header](#ngxreqclear_header)
* [ngx.req.read_body](#ngxreqread_body)
* [ngx.req.discard_body](#ngxreqdiscard_body)
* [ngx.req.get_body_data](#ngxreqget_body_data)
* [ngx.req.get_body_file](#ngxreqget_body_file)
* [ngx.req.set_body_data](#ngxreqset_body_data)
* [ngx.req.set_body_file](#ngxreqset_body_file)
* [ngx.req.init_body](#ngxreqinit_body)
* [ngx.req.append_body](#ngxreqappend_body)
* [ngx.req.finish_body](#ngxreqfinish_body)
* [ngx.req.socket](#ngxreqsocket)
* [ngx.exec](#ngxexec)
* [ngx.redirect](#ngxredirect)
* [ngx.send_headers](#ngxsend_headers)
* [ngx.headers_sent](#ngxheaders_sent)
* [ngx.print](#ngxprint)
* [ngx.say](#ngxsay)
* [ngx.log](#ngxlog)
* [ngx.flush](#ngxflush)
* [ngx.exit](#ngxexit)
* [ngx.eof](#ngxeof)
* [ngx.sleep](#ngxsleep)
* [ngx.escape_uri](#ngxescape_uri)
* [ngx.unescape_uri](#ngxunescape_uri)
* [ngx.encode_args](#ngxencode_args)
* [ngx.decode_args](#ngxdecode_args)
* [ngx.encode_base64](#ngxencode_base64)
* [ngx.decode_base64](#ngxdecode_base64)
* [ngx.crc32_short](#ngxcrc32_short)
* [ngx.crc32_long](#ngxcrc32_long)
* [ngx.hmac_sha1](#ngxhmac_sha1)
* [ngx.md5](#ngxmd5)
* [ngx.md5_bin](#ngxmd5_bin)
* [ngx.sha1_bin](#ngxsha1_bin)
* [ngx.quote_sql_str](#ngxquote_sql_str)
* [ngx.today](#ngxtoday)
* [ngx.time](#ngxtime)
* [ngx.now](#ngxnow)
* [ngx.update_time](#ngxupdate_time)
* [ngx.localtime](#ngxlocaltime)
* [ngx.utctime](#ngxutctime)
* [ngx.cookie_time](#ngxcookie_time)
* [ngx.http_time](#ngxhttp_time)
* [ngx.parse_http_time](#ngxparse_http_time)
* [ngx.is_subrequest](#ngxis_subrequest)
* [ngx.re.match](#ngxrematch)
* [ngx.re.find](#ngxrefind)
* [ngx.re.gmatch](#ngxregmatch)
* [ngx.re.sub](#ngxresub)
* [ngx.re.gsub](#ngxregsub)
* [ngx.shared.DICT](#ngxshareddict)
* [ngx.shared.DICT.get](#ngxshareddictget)
* [ngx.shared.DICT.get_stale](#ngxshareddictget_stale)
* [ngx.shared.DICT.set](#ngxshareddictset)
* [ngx.shared.DICT.safe_set](#ngxshareddictsafe_set)
* [ngx.shared.DICT.add](#ngxshareddictadd)
* [ngx.shared.DICT.safe_add](#ngxshareddictsafe_add)
* [ngx.shared.DICT.replace](#ngxshareddictreplace)
* [ngx.shared.DICT.delete](#ngxshareddictdelete)
* [ngx.shared.DICT.incr](#ngxshareddictincr)
* [ngx.shared.DICT.lpush](#ngxshareddictlpush)
* [ngx.shared.DICT.rpush](#ngxshareddictrpush)
* [ngx.shared.DICT.lpop](#ngxshareddictlpop)
* [ngx.shared.DICT.rpop](#ngxshareddictrpop)
* [ngx.shared.DICT.llen](#ngxshareddictllen)
* [ngx.shared.DICT.ttl](#ngxshareddictttl)
* [ngx.shared.DICT.expire](#ngxshareddictexpire)
* [ngx.shared.DICT.flush_all](#ngxshareddictflush_all)
* [ngx.shared.DICT.flush_expired](#ngxshareddictflush_expired)
* [ngx.shared.DICT.get_keys](#ngxshareddictget_keys)
* [ngx.shared.DICT.capacity](#ngxshareddictcapacity)
* [ngx.shared.DICT.free_space](#ngxshareddictfree_space)
* [ngx.socket.udp](#ngxsocketudp)
* [udpsock:setpeername](#udpsocksetpeername)
* [udpsock:send](#udpsocksend)
* [udpsock:receive](#udpsockreceive)
* [udpsock:close](#udpsockclose)
* [udpsock:settimeout](#udpsocksettimeout)
* [ngx.socket.stream](#ngxsocketstream)
* [ngx.socket.tcp](#ngxsockettcp)
* [tcpsock:bind](#tcpsockbind)
* [tcpsock:connect](#tcpsockconnect)
* [tcpsock:setclientcert](#tcpsocksetclientcert)
* [tcpsock:sslhandshake](#tcpsocksslhandshake)
* [tcpsock:send](#tcpsocksend)
* [tcpsock:receive](#tcpsockreceive)
* [tcpsock:receiveany](#tcpsockreceiveany)
* [tcpsock:receiveuntil](#tcpsockreceiveuntil)
* [tcpsock:close](#tcpsockclose)
* [tcpsock:settimeout](#tcpsocksettimeout)
* [tcpsock:settimeouts](#tcpsocksettimeouts)
* [tcpsock:setoption](#tcpsocksetoption)
* [tcpsock:setkeepalive](#tcpsocksetkeepalive)
* [tcpsock:getreusedtimes](#tcpsockgetreusedtimes)
* [ngx.socket.connect](#ngxsocketconnect)
* [ngx.get_phase](#ngxget_phase)
* [ngx.thread.spawn](#ngxthreadspawn)
* [ngx.thread.wait](#ngxthreadwait)
* [ngx.thread.kill](#ngxthreadkill)
* [ngx.on_abort](#ngxon_abort)
* [ngx.timer.at](#ngxtimerat)
* [ngx.timer.every](#ngxtimerevery)
* [ngx.timer.running_count](#ngxtimerrunning_count)
* [ngx.timer.pending_count](#ngxtimerpending_count)
* [ngx.config.subsystem](#ngxconfigsubsystem)
* [ngx.config.debug](#ngxconfigdebug)
* [ngx.config.prefix](#ngxconfigprefix)
* [ngx.config.nginx_version](#ngxconfignginx_version)
* [ngx.config.nginx_configure](#ngxconfignginx_configure)
* [ngx.config.ngx_lua_version](#ngxconfigngx_lua_version)
* [ngx.worker.exiting](#ngxworkerexiting)
* [ngx.worker.pid](#ngxworkerpid)
* [ngx.worker.pids](#ngxworkerpids)
* [ngx.worker.count](#ngxworkercount)
* [ngx.worker.id](#ngxworkerid)
* [ngx.semaphore](#ngxsemaphore)
* [ngx.balancer](#ngxbalancer)
* [ngx.ssl](#ngxssl)
* [ngx.ocsp](#ngxocsp)
* [ndk.set_var.DIRECTIVE](#ndkset_vardirective)
* [coroutine.create](#coroutinecreate)
* [coroutine.resume](#coroutineresume)
* [coroutine.yield](#coroutineyield)
* [coroutine.wrap](#coroutinewrap)
* [coroutine.running](#coroutinerunning)
* [coroutine.status](#coroutinestatus)
* [ngx.run_worker_thread](#ngxrun_worker_thread)



## Introduction

The various `*_by_lua`, `*_by_lua_block` and `*_by_lua_file` configuration directives serve as gateways to the Lua API within the `nginx.conf` file. The Nginx Lua API described below can only be called within the user Lua code run in the context of these configuration directives.

The API is exposed to Lua in the form of two standard packages `ngx` and `ndk`. These packages are in the default global scope within ngx_lua and are always available within ngx_lua directives.

The packages can be introduced into external Lua modules like this:

```lua

 local say = ngx.say

 local _M = {}

 function _M.foo(a)
     say(a)
 end

 return _M
```

Use of the [package.seeall](https://www.lua.org/manual/5.1/manual.html#pdf-package.seeall) flag is strongly discouraged due to its various bad side-effects.

It is also possible to directly require the packages in external Lua modules:

```lua

 local ngx = require "ngx"
 local ndk = require "ndk"
```

The ability to require these packages was introduced in the `v0.2.1rc19` release.

Network I/O operations in user code should only be done through the Nginx Lua API calls as the Nginx event loop may be blocked and performance drop off dramatically otherwise. Disk operations with relatively small amount of data can be done using the standard Lua `io` library but huge file reading and writing should be avoided wherever possible as they may block the Nginx process significantly. Delegating all network and disk I/O operations to Nginx's subrequests (via the [ngx.location.capture](#ngxlocationcapture) method and similar) is strongly recommended for maximum performance.

[Back to TOC](#nginx-api-for-lua)

## ngx.arg

**syntax:** *val = ngx.arg\[index\]*

**context:** *set_by_lua&#42;, body_filter_by_lua&#42;*

When this is used in the context of the [set_by_lua*](#set_by_lua) directives, this table is read-only and holds the input arguments to the config directives:

```lua

 value = ngx.arg[n]
```

Here is an example

```nginx

 location /foo {
     set $a 32;
     set $b 56;

     set_by_lua $sum
         'return tonumber(ngx.arg[1]) + tonumber(ngx.arg[2])'
         $a $b;

     echo $sum;
 }
```

that writes out `88`, the sum of `32` and `56`.

When this table is used in the context of [body_filter_by_lua*](#body_filter_by_lua), the first element holds the input data chunk to the output filter code and the second element holds the boolean flag for the "eof" flag indicating the end of the whole output data stream.

The data chunk and "eof" flag passed to the downstream Nginx output filters can also be overridden by assigning values directly to the corresponding table elements. When setting `nil` or an empty Lua string value to `ngx.arg[1]`, no data chunk will be passed to the downstream Nginx output filters at all.

[Back to TOC](#nginx-api-for-lua)

## ngx.var.VARIABLE

**syntax:** *ngx.var.VAR_NAME*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, balancer_by_lua&#42;*

Read and write Nginx variable values.

```nginx

 value = ngx.var.some_nginx_variable_name
 ngx.var.some_nginx_variable_name = value
```

Note that only already defined Nginx variables can be written to.
For example:

```nginx

 location /foo {
     set $my_var ''; # this line is required to create $my_var at config time
     content_by_lua_block {
         ngx.var.my_var = 123
         ...
     }
 }
```

That is, Nginx variables cannot be created on-the-fly. Here is a list of pre-defined
[Nginx variables](http://nginx.org/en/docs/varindex.html).

Some special Nginx variables like `$args` and `$limit_rate` can be assigned a value,
many others are not, like `$query_string`, `$arg_PARAMETER`, and `$http_NAME`.

Nginx regex group capturing variables `$1`, `$2`, `$3`, and etc, can be read by this
interface as well, by writing `ngx.var[1]`, `ngx.var[2]`, `ngx.var[3]`, and etc.

Setting `ngx.var.Foo` to a `nil` value will unset the `$Foo` Nginx variable.

```lua

 ngx.var.args = nil
```

**CAUTION** When reading from an Nginx variable, Nginx will allocate memory in the per-request memory pool which is freed only at request termination. So when you need to read from an Nginx variable repeatedly in your Lua code, cache the Nginx variable value to your own Lua variable, for example,

```lua

 local val = ngx.var.some_var
 --- use the val repeatedly later
```

to prevent (temporary) memory leaking within the current request's lifetime. Another way of caching the result is to use the [ngx.ctx](#ngxctx) table.

Undefined Nginx variables are evaluated to `nil` while uninitialized (but defined) Nginx variables are evaluated to an empty Lua string.

This API requires a relatively expensive metamethod call and it is recommended to avoid using it on hot code paths.

[Back to TOC](#nginx-api-for-lua)

## Core constants

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, &#42;log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

```lua

   ngx.OK (0)
   ngx.ERROR (-1)
   ngx.AGAIN (-2)
   ngx.DONE (-4)
   ngx.DECLINED (-5)
```

Note that only three of these constants are utilized by the [Nginx API for Lua](#nginx-api-for-lua) (i.e., [ngx.exit](#ngxexit) accepts `ngx.OK`, `ngx.ERROR`, and `ngx.DECLINED` as input).

```lua

   ngx.null
```

The `ngx.null` constant is a `NULL` light userdata usually used to represent nil values in Lua tables etc and is similar to the [lua-cjson](http://www.kyne.com.au/~mark/software/lua-cjson.php) library's `cjson.null` constant. This constant was first introduced in the `v0.5.0rc5` release.

The `ngx.DECLINED` constant was first introduced in the `v0.5.0rc19` release.

[Back to TOC](#nginx-api-for-lua)

## HTTP method constants

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*


      ngx.HTTP_GET
      ngx.HTTP_HEAD
      ngx.HTTP_PUT
      ngx.HTTP_POST
      ngx.HTTP_DELETE
      ngx.HTTP_OPTIONS   (added in the v0.5.0rc24 release)
      ngx.HTTP_MKCOL     (added in the v0.8.2 release)
      ngx.HTTP_COPY      (added in the v0.8.2 release)
      ngx.HTTP_MOVE      (added in the v0.8.2 release)
      ngx.HTTP_PROPFIND  (added in the v0.8.2 release)
      ngx.HTTP_PROPPATCH (added in the v0.8.2 release)
      ngx.HTTP_LOCK      (added in the v0.8.2 release)
      ngx.HTTP_UNLOCK    (added in the v0.8.2 release)
      ngx.HTTP_PATCH     (added in the v0.8.2 release)
      ngx.HTTP_TRACE     (added in the v0.8.2 release)


These constants are usually used in [ngx.location.capture](#ngxlocationcapture) and [ngx.location.capture_multi](#ngxlocationcapture_multi) method calls.

[Back to TOC](#nginx-api-for-lua)

## HTTP status constants

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

```nginx

   value = ngx.HTTP_CONTINUE (100) (first added in the v0.9.20 release)
   value = ngx.HTTP_SWITCHING_PROTOCOLS (101) (first added in the v0.9.20 release)
   value = ngx.HTTP_OK (200)
   value = ngx.HTTP_CREATED (201)
   value = ngx.HTTP_ACCEPTED (202) (first added in the v0.9.20 release)
   value = ngx.HTTP_NO_CONTENT (204) (first added in the v0.9.20 release)
   value = ngx.HTTP_PARTIAL_CONTENT (206) (first added in the v0.9.20 release)
   value = ngx.HTTP_SPECIAL_RESPONSE (300)
   value = ngx.HTTP_MOVED_PERMANENTLY (301)
   value = ngx.HTTP_MOVED_TEMPORARILY (302)
   value = ngx.HTTP_SEE_OTHER (303)
   value = ngx.HTTP_NOT_MODIFIED (304)
   value = ngx.HTTP_TEMPORARY_REDIRECT (307) (first added in the v0.9.20 release)
   value = ngx.HTTP_PERMANENT_REDIRECT (308)
   value = ngx.HTTP_BAD_REQUEST (400)
   value = ngx.HTTP_UNAUTHORIZED (401)
   value = ngx.HTTP_PAYMENT_REQUIRED (402) (first added in the v0.9.20 release)
   value = ngx.HTTP_FORBIDDEN (403)
   value = ngx.HTTP_NOT_FOUND (404)
   value = ngx.HTTP_NOT_ALLOWED (405)
   value = ngx.HTTP_NOT_ACCEPTABLE (406) (first added in the v0.9.20 release)
   value = ngx.HTTP_REQUEST_TIMEOUT (408) (first added in the v0.9.20 release)
   value = ngx.HTTP_CONFLICT (409) (first added in the v0.9.20 release)
   value = ngx.HTTP_GONE (410)
   value = ngx.HTTP_UPGRADE_REQUIRED (426) (first added in the v0.9.20 release)
   value = ngx.HTTP_TOO_MANY_REQUESTS (429) (first added in the v0.9.20 release)
   value = ngx.HTTP_CLOSE (444) (first added in the v0.9.20 release)
   value = ngx.HTTP_ILLEGAL (451) (first added in the v0.9.20 release)
   value = ngx.HTTP_INTERNAL_SERVER_ERROR (500)
   value = ngx.HTTP_NOT_IMPLEMENTED (501)
   value = ngx.HTTP_METHOD_NOT_IMPLEMENTED (501) (kept for compatibility)
   value = ngx.HTTP_BAD_GATEWAY (502) (first added in the v0.9.20 release)
   value = ngx.HTTP_SERVICE_UNAVAILABLE (503)
   value = ngx.HTTP_GATEWAY_TIMEOUT (504) (first added in the v0.3.1rc38 release)
   value = ngx.HTTP_VERSION_NOT_SUPPORTED (505) (first added in the v0.9.20 release)
   value = ngx.HTTP_INSUFFICIENT_STORAGE (507) (first added in the v0.9.20 release)
```

[Back to TOC](#nginx-api-for-lua)

## Nginx log level constants

**context:** *init_by_lua&#42;, init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

```lua

   ngx.STDERR
   ngx.EMERG
   ngx.ALERT
   ngx.CRIT
   ngx.ERR
   ngx.WARN
   ngx.NOTICE
   ngx.INFO
   ngx.DEBUG
```

These constants are usually used by the [ngx.log](#ngxlog) method.

[Back to TOC](#nginx-api-for-lua)

## print

**syntax:** *print(...)*

**context:** *init_by_lua&#42;, init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Writes argument values into the Nginx `error.log` file with the `ngx.NOTICE` log level.

It is equivalent to

```lua

 ngx.log(ngx.NOTICE, ...)
```

Lua `nil` arguments are accepted and result in literal `"nil"` strings while Lua booleans result in literal `"true"` or `"false"` strings. And the `ngx.null` constant will yield the `"null"` string output.

There is a hard coded `2048` byte limitation on error message lengths in the Nginx core. This limit includes trailing newlines and leading time stamps. If the message size exceeds this limit, Nginx will truncate the message text accordingly. This limit can be manually modified by editing the `NGX_MAX_ERROR_STR` macro definition in the `src/core/ngx_log.h` file in the Nginx source tree.

[Back to TOC](#nginx-api-for-lua)

## ngx.ctx

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, exit_worker_by_lua&#42;*

This table can be used to store per-request Lua context data and has a life time identical to the current request (as with the Nginx variables).

Consider the following example,

```nginx

 location /test {
     rewrite_by_lua_block {
         ngx.ctx.foo = 76
     }
     access_by_lua_block {
         ngx.ctx.foo = ngx.ctx.foo + 3
     }
     content_by_lua_block {
         ngx.say(ngx.ctx.foo)
     }
 }
```

Then `GET /test` will yield the output

```bash

 79
```

That is, the `ngx.ctx.foo` entry persists across the rewrite, access, and content phases of a request.

Every request, including subrequests, has its own copy of the table. For example:

```nginx

 location /sub {
     content_by_lua_block {
         ngx.say("sub pre: ", ngx.ctx.blah)
         ngx.ctx.blah = 32
         ngx.say("sub post: ", ngx.ctx.blah)
     }
 }

 location /main {
     content_by_lua_block {
         ngx.ctx.blah = 73
         ngx.say("main pre: ", ngx.ctx.blah)
         local res = ngx.location.capture("/sub")
         ngx.print(res.body)
         ngx.say("main post: ", ngx.ctx.blah)
     }
 }
```

Then `GET /main` will give the output

```bash

 main pre: 73
 sub pre: nil
 sub post: 32
 main post: 73
```

Here, modification of the `ngx.ctx.blah` entry in the subrequest does not affect the one in the parent request. This is because they have two separate versions of `ngx.ctx.blah`.

Internal redirects (triggered by nginx configuration directives like `error_page`, `try_files`, `index` and etc) will destroy the original request `ngx.ctx` data (if any) and the new request will have an empty `ngx.ctx` table. For instance,

```nginx

 location /new {
     content_by_lua_block {
         ngx.say(ngx.ctx.foo)
     }
 }

 location /orig {
     content_by_lua_block {
         ngx.ctx.foo = "hello"
         ngx.exec("/new")
     }
 }
```

Then `GET /orig` will give

```bash

 nil
```

rather than the original `"hello"` value.

Because HTTP request is created after SSL handshake, the `ngx.ctx` created
in [ssl_certificate_by_lua*](#ssl_certificate_by_lua), [ssl_session_store_by_lua*](#ssl_session_store_by_lua), [ssl_session_fetch_by_lua*](#ssl_session_fetch_by_lua) and [ssl_client_hello_by_lua*](#ssl_client_hello_by_lua)
is not available in the following phases like [rewrite_by_lua*](#rewrite_by_lua).

Since `v0.10.18`, the `ngx.ctx` created during a SSL handshake
will be inherited by the requests which share the same TCP connection established by the handshake.
Note that overwrite values in `ngx.ctx` in the http request phases (like `rewrite_by_lua*`) will only take affect in the current http request.

Arbitrary data values, including Lua closures and nested tables, can be inserted into this "magic" table. It also allows the registration of custom meta methods.

Overriding `ngx.ctx` with a new Lua table is also supported, for example,

```lua

 ngx.ctx = { foo = 32, bar = 54 }
```

When being used in the context of [init_worker_by_lua*](#init_worker_by_lua), this table just has the same lifetime of the current Lua handler.

The `ngx.ctx` lookup requires relatively expensive metamethod calls and it is much slower than explicitly passing per-request data along by your own function arguments. So do not abuse this API for saving your own function arguments because it usually has quite some performance impact.

Because of the metamethod magic, never "local" the `ngx.ctx` table outside your Lua function scope on the Lua module level due to [worker-level data sharing](#data-sharing-within-an-nginx-worker). For example, the following is bad:

```lua

 -- mymodule.lua
 local _M = {}

 -- the following line is bad since ngx.ctx is a per-request
 -- data while this <code>ctx</code> variable is on the Lua module level
 -- and thus is per-nginx-worker.
 local ctx = ngx.ctx

 function _M.main()
     ctx.foo = "bar"
 end

 return _M
```

Use the following instead:

```lua

 -- mymodule.lua
 local _M = {}

 function _M.main(ctx)
     ctx.foo = "bar"
 end

 return _M
```

That is, let the caller pass the `ctx` table explicitly via a function argument.

[Back to TOC](#nginx-api-for-lua)

## ngx.location.capture

**syntax:** *res = ngx.location.capture(uri, options?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Issues a synchronous but still non-blocking *Nginx Subrequest* using `uri`.

Nginx's subrequests provide a powerful way to make non-blocking internal requests to other locations configured with disk file directory or *any* other Nginx C modules like `ngx_proxy`, `ngx_fastcgi`, `ngx_memc`,
`ngx_postgres`, `ngx_drizzle`, and even ngx_lua itself and etc etc etc.

Also note that subrequests just mimic the HTTP interface but there is *no* extra HTTP/TCP traffic *nor* IPC involved. Everything works internally, efficiently, on the C level.

Subrequests are completely different from HTTP 301/302 redirection (via [ngx.redirect](#ngxredirect)) and internal redirection (via [ngx.exec](#ngxexec)).

You should always read the request body (by either calling [ngx.req.read_body](#ngxreqread_body) or configuring [lua_need_request_body](#lua_need_request_body) on) before initiating a subrequest.

This API function (as well as [ngx.location.capture_multi](#ngxlocationcapture_multi)) always buffers the whole response body of the subrequest in memory. Thus, you should use [cosockets](#ngxsockettcp)
and streaming processing instead if you have to handle large subrequest responses.

Here is a basic example:

```lua

 res = ngx.location.capture(uri)
```

Returns a Lua table with 4 slots: `res.status`, `res.header`, `res.body`, and `res.truncated`.

`res.status` holds the response status code for the subrequest response.

`res.header` holds all the response headers of the
subrequest and it is a normal Lua table. For multi-value response headers,
the value is a Lua (array) table that holds all the values in the order that
they appear. For instance, if the subrequest response headers contain the following
lines:

```bash

 Set-Cookie: a=3
 Set-Cookie: foo=bar
 Set-Cookie: baz=blah
```

Then `res.header["Set-Cookie"]` will be evaluated to the table value
`{"a=3", "foo=bar", "baz=blah"}`.

`res.body` holds the subrequest's response body data, which might be truncated. You always need to check the `res.truncated` boolean flag to see if `res.body` contains truncated data. The data truncation here can only be caused by those unrecoverable errors in your subrequests like the cases that the remote end aborts the connection prematurely in the middle of the response body data stream or a read timeout happens when your subrequest is receiving the response body data from the remote.

URI query strings can be concatenated to URI itself, for instance,

```lua

 res = ngx.location.capture('/foo/bar?a=3&b=4')
```

Named locations like `@foo` are not allowed due to a limitation in
the Nginx core. Use normal locations combined with the `internal` directive to
prepare internal-only locations.

An optional option table can be fed as the second
argument, which supports the options:

* `method`
	specify the subrequest's request method, which only accepts constants like `ngx.HTTP_POST`.
* `body`
	specify the subrequest's request body (string value only).
* `args`
	specify the subrequest's URI query arguments (both string value and Lua tables are accepted)
* `ctx`
	specify a Lua table to be the [ngx.ctx](#ngxctx) table for the subrequest. It can be the current request's [ngx.ctx](#ngxctx) table, which effectively makes the parent and its subrequest to share exactly the same context table. This option was first introduced in the `v0.3.1rc25` release.
* `vars`
	take a Lua table which holds the values to set the specified Nginx variables in the subrequest as this option's value. This option was first introduced in the `v0.3.1rc31` release.
* `copy_all_vars`
	specify whether to copy over all the Nginx variable values of the current request to the subrequest in question. modifications of the Nginx variables in the subrequest will not affect the current (parent) request. This option was first introduced in the `v0.3.1rc31` release.
* `share_all_vars`
	specify whether to share all the Nginx variables of the subrequest with the current (parent) request. modifications of the Nginx variables in the subrequest will affect the current (parent) request. Enabling this option may lead to hard-to-debug issues due to bad side-effects and is considered bad and harmful. Only enable this option when you completely know what you are doing.
* `always_forward_body`
	when set to true, the current (parent) request's request body will always be forwarded to the subrequest being created if the `body` option is not specified. The request body read by either [ngx.req.read_body()](#ngxreqread_body) or [lua_need_request_body on](#lua_need_request_body) will be directly forwarded to the subrequest without copying the whole request body data when creating the subrequest (no matter the request body data is buffered in memory buffers or temporary files). By default, this option is `false` and when the `body` option is not specified, the request body of the current (parent) request is only forwarded when the subrequest takes the `PUT` or `POST` request method.

Issuing a POST subrequest, for example, can be done as follows

```lua

 res = ngx.location.capture(
     '/foo/bar',
     { method = ngx.HTTP_POST, body = 'hello, world' }
 )
```

See HTTP method constants methods other than POST.
The `method` option is `ngx.HTTP_GET` by default.

The `args` option can specify extra URI arguments, for instance,

```lua

 ngx.location.capture('/foo?a=1',
     { args = { b = 3, c = ':' } }
 )
```

is equivalent to

```lua

 ngx.location.capture('/foo?a=1&b=3&c=%3a')
```

that is, this method will escape argument keys and values according to URI rules and
concatenate them together into a complete query string. The format for the Lua table passed as the `args` argument is identical to the format used in the [ngx.encode_args](#ngxencode_args) method.

The `args` option can also take plain query strings:

```lua

 ngx.location.capture('/foo?a=1',
     { args = 'b=3&c=%3a' }
 )
```

This is functionally identical to the previous examples.

The `share_all_vars` option controls whether to share Nginx variables among the current request and its subrequests.
If this option is set to `true`, then the current request and associated subrequests will share the same Nginx variable scope. Hence, changes to Nginx variables made by a subrequest will affect the current request.

Care should be taken in using this option as variable scope sharing can have unexpected side effects. The `args`, `vars`, or `copy_all_vars` options are generally preferable instead.

This option is set to `false` by default

```nginx

 location /other {
     set $dog "$dog world";
     echo "$uri dog: $dog";
 }

 location /lua {
     set $dog 'hello';
     content_by_lua_block {
         res = ngx.location.capture("/other",
             { share_all_vars = true })

         ngx.print(res.body)
         ngx.say(ngx.var.uri, ": ", ngx.var.dog)
     }
 }
```

Accessing location `/lua` gives


    /other dog: hello world
    /lua: hello world


The `copy_all_vars` option provides a copy of the parent request's Nginx variables to subrequests when such subrequests are issued. Changes made to these variables by such subrequests will not affect the parent request or any other subrequests sharing the parent request's variables.

```nginx

 location /other {
     set $dog "$dog world";
     echo "$uri dog: $dog";
 }

 location /lua {
     set $dog 'hello';
     content_by_lua_block {
         res = ngx.location.capture("/other",
             { copy_all_vars = true })

         ngx.print(res.body)
         ngx.say(ngx.var.uri, ": ", ngx.var.dog)
     }
 }
```

Request `GET /lua` will give the output


    /other dog: hello world
    /lua: hello


Note that if both `share_all_vars` and `copy_all_vars` are set to true, then `share_all_vars` takes precedence.

In addition to the two settings above, it is possible to specify
values for variables in the subrequest using the `vars` option. These
variables are set after the sharing or copying of variables has been
evaluated, and provides a more efficient method of passing specific
values to a subrequest over encoding them as URL arguments and
unescaping them in the Nginx config file.

```nginx

 location /other {
     content_by_lua_block {
         ngx.say("dog = ", ngx.var.dog)
         ngx.say("cat = ", ngx.var.cat)
     }
 }

 location /lua {
     set $dog '';
     set $cat '';
     content_by_lua_block {
         res = ngx.location.capture("/other",
             { vars = { dog = "hello", cat = 32 }})

         ngx.print(res.body)
     }
 }
```

Accessing `/lua` will yield the output


    dog = hello
    cat = 32


The `ctx` option can be used to specify a custom Lua table to serve as the [ngx.ctx](#ngxctx) table for the subrequest.

```nginx

 location /sub {
     content_by_lua_block {
         ngx.ctx.foo = "bar";
     }
 }
 location /lua {
     content_by_lua_block {
         local ctx = {}
         res = ngx.location.capture("/sub", { ctx = ctx })

         ngx.say(ctx.foo)
         ngx.say(ngx.ctx.foo)
     }
 }
```

Then request `GET /lua` gives


    bar
    nil


It is also possible to use this `ctx` option to share the same [ngx.ctx](#ngxctx) table between the current (parent) request and the subrequest:

```nginx

 location /sub {
     content_by_lua_block {
         ngx.ctx.foo = "bar"
     }
 }
 location /lua {
     content_by_lua_block {
         res = ngx.location.capture("/sub", { ctx = ngx.ctx })
         ngx.say(ngx.ctx.foo)
     }
 }
```

Request `GET /lua` yields the output


    bar


Note that subrequests issued by [ngx.location.capture](#ngxlocationcapture) inherit all the
request headers of the current request by default and that this may have unexpected side effects on the
subrequest responses. For example, when using the standard `ngx_proxy` module to serve
subrequests, an "Accept-Encoding: gzip" header in the main request may result
in gzipped responses that cannot be handled properly in Lua code. Original request headers should be ignored by setting
[proxy_pass_request_headers](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass_request_headers) to `off` in subrequest locations.

When the `body` option is not specified and the `always_forward_body` option is false (the default value), the `POST` and `PUT` subrequests will inherit the request bodies of the parent request (if any).

There is a hard-coded upper limit on the number of subrequests possible for every main request. In older versions of Nginx, the limit was `50` concurrent subrequests and in more recent versions, Nginx `1.9.5` onwards, the same limit is changed to limit the depth of recursive subrequests. When this limit is exceeded, the following error message is added to the `error.log` file:


    [error] 13983#0: *1 subrequests cycle while processing "/uri"


The limit can be manually modified if required by editing the definition of the `NGX_HTTP_MAX_SUBREQUESTS` macro in the `nginx/src/http/ngx_http_request.h` file in the Nginx source tree.

Please also refer to restrictions on capturing locations configured by [subrequest directives of other modules](#locations-configured-by-subrequest-directives-of-other-modules).

[Back to TOC](#nginx-api-for-lua)

## ngx.location.capture_multi

**syntax:** *res1, res2, ... = ngx.location.capture_multi({ {uri, options?}, {uri, options?}, ... })*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Just like [ngx.location.capture](#ngxlocationcapture), but supports multiple subrequests running in parallel.

This function issues several parallel subrequests specified by the input table and returns their results in the same order. For example,

```lua

 res1, res2, res3 = ngx.location.capture_multi{
     { "/foo", { args = "a=3&b=4" } },
     { "/bar" },
     { "/baz", { method = ngx.HTTP_POST, body = "hello" } },
 }

 if res1.status == ngx.HTTP_OK then
     ...
 end

 if res2.body == "BLAH" then
     ...
 end
```

This function will not return until all the subrequests terminate.
The total latency is the longest latency of the individual subrequests rather than the sum.

Lua tables can be used for both requests and responses when the number of subrequests to be issued is not known in advance:

```lua

 -- construct the requests table
 local reqs = {}
 table.insert(reqs, { "/mysql" })
 table.insert(reqs, { "/postgres" })
 table.insert(reqs, { "/redis" })
 table.insert(reqs, { "/memcached" })

 -- issue all the requests at once and wait until they all return
 local resps = {
     ngx.location.capture_multi(reqs)
 }

 -- loop over the responses table
 for i, resp in ipairs(resps) do
     -- process the response table "resp"
 end
```

The [ngx.location.capture](#ngxlocationcapture) function is just a special form
of this function. Logically speaking, the [ngx.location.capture](#ngxlocationcapture) can be implemented like this

```lua

 ngx.location.capture =
     function (uri, args)
         return ngx.location.capture_multi({ {uri, args} })
     end
```

Please also refer to restrictions on capturing locations configured by [subrequest directives of other modules](#locations-configured-by-subrequest-directives-of-other-modules).

[Back to TOC](#nginx-api-for-lua)

## ngx.status

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;*

Read and write the current request's response status. This should be called
before sending out the response headers.

```lua

 ngx.status = ngx.HTTP_CREATED
 status = ngx.status
```

Setting `ngx.status` after the response header is sent out has no effect but leaving an error message in your Nginx's error log file:


    attempt to set ngx.status after sending out response headers


[Back to TOC](#nginx-api-for-lua)

## ngx.header.HEADER

**syntax:** *ngx.header.HEADER = VALUE*

**syntax:** *value = ngx.header.HEADER*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;*

Set, add to, or clear the current request's `HEADER` response header that is to be sent.

Underscores (`_`) in the header names will be replaced by hyphens (`-`) by default. This transformation can be turned off via the [lua_transform_underscores_in_response_headers](#lua_transform_underscores_in_response_headers) directive.

The header names are matched case-insensitively.

```lua

 -- equivalent to ngx.header["Content-Type"] = 'text/plain'
 ngx.header.content_type = 'text/plain'

 ngx.header["X-My-Header"] = 'blah blah'
```

Multi-value headers can be set this way:

```lua

 ngx.header['Set-Cookie'] = {'a=32; path=/', 'b=4; path=/'}
```

will yield

```bash

 Set-Cookie: a=32; path=/
 Set-Cookie: b=4; path=/
```

in the response headers.

Only Lua tables are accepted (Only the last element in the table will take effect for standard headers such as `Content-Type` that only accept a single value).

```lua

 ngx.header.content_type = {'a', 'b'}
```

is equivalent to

```lua

 ngx.header.content_type = 'b'
```

Setting a slot to `nil` effectively removes it from the response headers:

```lua

 ngx.header["X-My-Header"] = nil
```

The same applies to assigning an empty table:

```lua

 ngx.header["X-My-Header"] = {}
```

Setting `ngx.header.HEADER` after sending out response headers (either explicitly with [ngx.send_headers](#ngxsend_headers) or implicitly with [ngx.print](#ngxprint) and similar) will log an error message.

Reading `ngx.header.HEADER` will return the value of the response header named `HEADER`.

Underscores (`_`) in the header names will also be replaced by dashes (`-`) and the header names will be matched case-insensitively. If the response header is not present at all, `nil` will be returned.

This is particularly useful in the context of [header_filter_by_lua*](#header_filter_by_lua), for example,

```nginx

 location /test {
     set $footer '';

     proxy_pass http://some-backend;

     header_filter_by_lua_block {
         if ngx.header["X-My-Header"] == "blah" then
             ngx.var.footer = "some value"
         end
     }

     echo_after_body $footer;
 }
```

For multi-value headers, all of the values of header will be collected in order and returned as a Lua table. For example, response headers


    Foo: bar
    Foo: baz


will result in

```lua

 {"bar", "baz"}
```

to be returned when reading `ngx.header.Foo`.

Note that `ngx.header` is not a normal Lua table and as such, it is not possible to iterate through it using the Lua `ipairs` function.

Note: this function throws a Lua error if `HEADER` or
`VALUE` contain unsafe characters (control characters).

For reading *request* headers, use the [ngx.req.get_headers](#ngxreqget_headers) function instead.

[Back to TOC](#nginx-api-for-lua)

## ngx.resp.get_headers

**syntax:** *headers, err = ngx.resp.get_headers(max_headers?, raw?)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, balancer_by_lua&#42;*

Returns a Lua table holding all the current response headers for the current request.

```lua

 local h, err = ngx.resp.get_headers()

 if err == "truncated" then
     -- one can choose to ignore or reject the current response here
 end

 for k, v in pairs(h) do
     ...
 end
```

This function has the same signature as [ngx.req.get_headers](#ngxreqget_headers) except getting response headers instead of request headers.

Note that a maximum of 100 response headers are parsed by default (including those with the same name) and that additional response headers are silently discarded to guard against potential denial of service attacks. Since `v0.10.13`, when the limit is exceeded, it will return a second value which is the string `"truncated"`.

This API was first introduced in the `v0.9.5` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.is_internal

**syntax:** *is_internal = ngx.req.is_internal()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;*

Returns a boolean indicating whether the current request is an "internal request", i.e.,
a request initiated from inside the current Nginx server instead of from the client side.

Subrequests are all internal requests and so are requests after internal redirects.

This API was first introduced in the `v0.9.20` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.start_time

**syntax:** *secs = ngx.req.start_time()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;*

Returns a floating-point number representing the timestamp (including milliseconds as the decimal part) when the current request was created.

The following example emulates the `$request_time` variable value (provided by [ngx_http_log_module](http://nginx.org/en/docs/http/ngx_http_log_module.html)) in pure Lua:

```lua

 local request_time = ngx.now() - ngx.req.start_time()
```

This function was first introduced in the `v0.7.7` release.

See also [ngx.now](#ngxnow) and [ngx.update_time](#ngxupdate_time).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.http_version

**syntax:** *num = ngx.req.http_version()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;*

Returns the HTTP version number for the current request as a Lua number.

Current possible values are 2.0, 1.0, 1.1, and 0.9. Returns `nil` for unrecognized values.

This method was first introduced in the `v0.7.17` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.raw_header

**syntax:** *str = ngx.req.raw_header(no_request_line?)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;*

Returns the original raw HTTP protocol header received by the Nginx server.

By default, the request line and trailing `CR LF` terminator will also be included. For example,

```lua

 ngx.print(ngx.req.raw_header())
```

gives something like this:


    GET /t HTTP/1.1
    Host: localhost
    Connection: close
    Foo: bar



You can specify the optional
`no_request_line` argument as a `true` value to exclude the request line from the result. For example,

```lua

 ngx.print(ngx.req.raw_header(true))
```

outputs something like this:


    Host: localhost
    Connection: close
    Foo: bar



This method was first introduced in the `v0.7.17` release.

This method does not work in HTTP/2 requests yet.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.get_method

**syntax:** *method_name = ngx.req.get_method()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, balancer_by_lua&#42;, log_by_lua&#42;*

Retrieves the current request's request method name. Strings like `"GET"` and `"POST"` are returned instead of numerical [method constants](#http-method-constants).

If the current request is an Nginx subrequest, then the subrequest's method name will be returned.

This method was first introduced in the `v0.5.6` release.

See also [ngx.req.set_method](#ngxreqset_method).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.set_method

**syntax:** *ngx.req.set_method(method_id)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;*

Overrides the current request's request method with the `method_id` argument. Currently only numerical [method constants](#http-method-constants) are supported, like `ngx.HTTP_POST` and `ngx.HTTP_GET`.

If the current request is an Nginx subrequest, then the subrequest's method will be overridden.

This method was first introduced in the `v0.5.6` release.

See also [ngx.req.get_method](#ngxreqget_method).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.set_uri

**syntax:** *ngx.req.set_uri(uri, jump?, binary?)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;*

Rewrite the current request's (parsed) URI by the `uri` argument. The `uri` argument must be a Lua string and cannot be of zero length, or a Lua exception will be thrown.

The optional boolean `jump` argument can trigger location rematch (or location jump) as [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html)'s [rewrite](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#rewrite) directive, that is, when `jump` is `true` (default to `false`), this function will never return and it will tell Nginx to try re-searching locations with the new URI value at the later `post-rewrite` phase and jumping to the new location.

Location jump will not be triggered otherwise, and only the current request's URI will be modified, which is also the default behavior. This function will return but with no returned values when the `jump` argument is `false` or absent altogether.

For example, the following Nginx config snippet

```nginx

 rewrite ^ /foo last;
```

can be coded in Lua like this:

```lua

 ngx.req.set_uri("/foo", true)
```

Similarly, Nginx config

```nginx

 rewrite ^ /foo break;
```

can be coded in Lua as

```lua

 ngx.req.set_uri("/foo", false)
```

or equivalently,

```lua

 ngx.req.set_uri("/foo")
```

The `jump` argument can only be set to `true` in [rewrite_by_lua*](#rewrite_by_lua). Use of jump in other contexts is prohibited and will throw out a Lua exception.

A more sophisticated example involving regex substitutions is as follows

```nginx

 location /test {
     rewrite_by_lua_block {
         local uri = ngx.re.sub(ngx.var.uri, "^/test/(.*)", "/$1", "o")
         ngx.req.set_uri(uri)
     }
     proxy_pass http://my_backend;
 }
```

which is functionally equivalent to

```nginx

 location /test {
     rewrite ^/test/(.*) /$1 break;
     proxy_pass http://my_backend;
 }
```

Note: this function throws a Lua error if the `uri` argument
contains unsafe characters (control characters).

Note that it is not possible to use this interface to rewrite URI arguments and that [ngx.req.set_uri_args](#ngxreqset_uri_args) should be used for this instead. For instance, Nginx config

```nginx

 rewrite ^ /foo?a=3? last;
```

can be coded as

```nginx

 ngx.req.set_uri_args("a=3")
 ngx.req.set_uri("/foo", true)
```

or

```nginx

 ngx.req.set_uri_args({a = 3})
 ngx.req.set_uri("/foo", true)
```

Starting from `0.10.16` of this module, this function accepts an
optional boolean `binary` argument to allow arbitrary binary URI
data. By default, this `binary` argument is false and this function
will throw out a Lua error such as the one below when the `uri`
argument contains any control characters (ASCII Code 0 ~ 0x08, 0x0A ~ 0x1F and 0x7F).


    [error] 23430#23430: *1 lua entry thread aborted: runtime error:
    content_by_lua(nginx.conf:44):3: ngx.req.set_uri unsafe byte "0x00"
    in "\x00foo" (maybe you want to set the 'binary' argument?)


This interface was first introduced in the `v0.3.1rc14` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.set_uri_args

**syntax:** *ngx.req.set_uri_args(args)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;*

Rewrite the current request's URI query arguments by the `args` argument. The `args` argument can be either a Lua string, as in

```lua

 ngx.req.set_uri_args("a=3&b=hello%20world")
```

or a Lua table holding the query arguments' key-value pairs, as in

```lua

 ngx.req.set_uri_args({ a = 3, b = "hello world" })
```

In the former case, i.e., when the whole query-string is provided directly,
the input Lua string should already be well-formed with the URI encoding.
For security considerations, this method will automatically escape any control and
whitespace characters (ASCII code 0x00 ~ 0x20 and 0x7F) in the Lua string.

In the latter case, this method will escape argument keys and values according to the URI escaping rule.

Multi-value arguments are also supported:

```lua

 ngx.req.set_uri_args({ a = 3, b = {5, 6} })
```

which will result in a query string like `a=3&b=5&b=6` or `b=5&b=6&a=3`.

**Note that when using Lua table as the `arg` argument, the order of the arguments in the result query string which change from time to time. If you would like to get an ordered result, you need to use Lua string as the `arg` argument.**

This interface was first introduced in the `v0.3.1rc13` release.

See also [ngx.req.set_uri](#ngxreqset_uri).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.get_uri_args

**syntax:** *args, err = ngx.req.get_uri_args(max_args?, tab?)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, balancer_by_lua&#42;*

Returns a Lua table holding all the current request URL query arguments. An optional `tab` argument
can be used to reuse the table returned by this method.

```nginx

 location = /test {
     content_by_lua_block {
         local args, err = ngx.req.get_uri_args()

         if err == "truncated" then
             -- one can choose to ignore or reject the current request here
         end

         for key, val in pairs(args) do
             if type(val) == "table" then
                 ngx.say(key, ": ", table.concat(val, ", "))
             else
                 ngx.say(key, ": ", val)
             end
         end
     }
 }
```

Then `GET /test?foo=bar&bar=baz&bar=blah` will yield the response body

```bash

 foo: bar
 bar: baz, blah
```

Multiple occurrences of an argument key will result in a table value holding all the values for that key in order.

Keys and values are unescaped according to URI escaping rules. In the settings above, `GET /test?a%20b=1%61+2` will yield:

```bash

 a b: 1a 2
```

Arguments without the `=<value>` parts are treated as boolean arguments. `GET /test?foo&bar` will yield:

```bash

 foo: true
 bar: true
```

That is, they will take Lua boolean values `true`. However, they are different from arguments taking empty string values. `GET /test?foo=&bar=` will give something like

```bash

 foo:
 bar:
```

Empty key arguments are discarded. `GET /test?=hello&=world` will yield an empty output for instance.

Updating query arguments via the Nginx variable `$args` (or `ngx.var.args` in Lua) at runtime is also supported:

```lua

 ngx.var.args = "a=3&b=42"
 local args, err = ngx.req.get_uri_args()
```

Here the `args` table will always look like

```lua

 {a = 3, b = 42}
```

regardless of the actual request query string.

Note that a maximum of 100 request arguments are parsed by default (including those with the same name) and that additional request arguments are silently discarded to guard against potential denial of service attacks. Since `v0.10.13`, when the limit is exceeded, it will return a second value which is the string `"truncated"`.

However, the optional `max_args` function argument can be used to override this limit:

```lua

 local args, err = ngx.req.get_uri_args(10)
 if err == "truncated" then
     -- one can choose to ignore or reject the current request here
 end
```

This argument can be set to zero to remove the limit and to process all request arguments received:

```lua

 local args, err = ngx.req.get_uri_args(0)
```

Removing the `max_args` cap is strongly discouraged.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.get_post_args

**syntax:** *args, err = ngx.req.get_post_args(max_args?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;*

Returns a Lua table holding all the current request POST query arguments (of the MIME type `application/x-www-form-urlencoded`). Call [ngx.req.read_body](#ngxreqread_body) to read the request body first or turn on the [lua_need_request_body](#lua_need_request_body) directive to avoid errors.

```nginx

 location = /test {
     content_by_lua_block {
         ngx.req.read_body()
         local args, err = ngx.req.get_post_args()

         if err == "truncated" then
             -- one can choose to ignore or reject the current request here
         end

         if not args then
             ngx.say("failed to get post args: ", err)
             return
         end
         for key, val in pairs(args) do
             if type(val) == "table" then
                 ngx.say(key, ": ", table.concat(val, ", "))
             else
                 ngx.say(key, ": ", val)
             end
         end
     }
 }
```

Then

```bash

 # Post request with the body 'foo=bar&bar=baz&bar=blah'
 $ curl --data 'foo=bar&bar=baz&bar=blah' localhost/test
```

will yield the response body like

```bash

 foo: bar
 bar: baz, blah
```

Multiple occurrences of an argument key will result in a table value holding all of the values for that key in order.

Keys and values will be unescaped according to URI escaping rules.

With the settings above,

```bash

 # POST request with body 'a%20b=1%61+2'
 $ curl -d 'a%20b=1%61+2' localhost/test
```

will yield:

```bash

 a b: 1a 2
```

Arguments without the `=<value>` parts are treated as boolean arguments. `POST /test` with the request body `foo&bar` will yield:

```bash

 foo: true
 bar: true
```

That is, they will take Lua boolean values `true`. However, they are different from arguments taking empty string values. `POST /test` with request body `foo=&bar=` will return something like

```bash

 foo:
 bar:
```

Empty key arguments are discarded. `POST /test` with body `=hello&=world` will yield empty outputs for instance.

Note that a maximum of 100 request arguments are parsed by default (including those with the same name) and that additional request arguments are silently discarded to guard against potential denial of service attacks. Since `v0.10.13`, when the limit is exceeded, it will return a second value which is the string `"truncated"`.

However, the optional `max_args` function argument can be used to override this limit:

```lua

 local args, err = ngx.req.get_post_args(10)
 if err == "truncated" then
     -- one can choose to ignore or reject the current request here
 end
```

This argument can be set to zero to remove the limit and to process all request arguments received:

```lua

 local args, err = ngx.req.get_post_args(0)
```

Removing the `max_args` cap is strongly discouraged.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.get_headers

**syntax:** *headers, err = ngx.req.get_headers(max_headers?, raw?)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;*

Returns a Lua table holding all the current request headers.

```lua

 local h, err = ngx.req.get_headers()

 if err == "truncated" then
     -- one can choose to ignore or reject the current request here
 end

 for k, v in pairs(h) do
     ...
 end
```

To read an individual header:

```lua

 ngx.say("Host: ", ngx.req.get_headers()["Host"])
```

Note that the [ngx.var.HEADER](#ngxvarvariable) API call, which uses core [$http_HEADER](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_http_) variables, may be more preferable for reading individual request headers.

For multiple instances of request headers such as:

```bash

 Foo: foo
 Foo: bar
 Foo: baz
```

the value of `ngx.req.get_headers()["Foo"]` will be a Lua (array) table such as:

```lua

 {"foo", "bar", "baz"}
```

Note that a maximum of 100 request headers are parsed by default (including those with the same name) and that additional request headers are silently discarded to guard against potential denial of service attacks. Since `v0.10.13`, when the limit is exceeded, it will return a second value which is the string `"truncated"`.

However, the optional `max_headers` function argument can be used to override this limit:

```lua

 local headers, err = ngx.req.get_headers(10)

 if err == "truncated" then
     -- one can choose to ignore or reject the current request here
 end
```

This argument can be set to zero to remove the limit and to process all request headers received:

```lua

 local headers, err = ngx.req.get_headers(0)
```

Removing the `max_headers` cap is strongly discouraged.

Since the `0.6.9` release, all the header names in the Lua table returned are converted to the pure lower-case form by default, unless the `raw` argument is set to `true` (default to `false`).

Also, by default, an `__index` metamethod is added to the resulting Lua table and will normalize the keys to a pure lowercase form with all underscores converted to dashes in case of a lookup miss. For example, if a request header `My-Foo-Header` is present, then the following invocations will all pick up the value of this header correctly:

```lua

 ngx.say(headers.my_foo_header)
 ngx.say(headers["My-Foo-Header"])
 ngx.say(headers["my-foo-header"])
```

The `__index` metamethod will not be added when the `raw` argument is set to `true`.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.set_header

**syntax:** *ngx.req.set_header(header_name, header_value)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;*

Set the current request's request header named `header_name` to value `header_value`, overriding any existing ones.

The input Lua string `header_name` and `header_value` should already be well-formed with the URI encoding.
For security considerations, this method will automatically escape " ", """, "(", ")", ",", "/", ":", ";", "?",
"<", "=", ">", "?", "@", "[", "]", "\", "{", "}", 0x00-0x1F, 0x7F-0xFF in `header_name` and automatically escape
"0x00-0x08, 0x0A-0x0F, 0x7F in `header_value`.

By default, all the subrequests subsequently initiated by [ngx.location.capture](#ngxlocationcapture) and [ngx.location.capture_multi](#ngxlocationcapture_multi) will inherit the new header.

It is not a Lua's equivalent of nginx `proxy_set_header` directive (same is true about [ngx.req.clear_header](#ngxreqclear_header)). `proxy_set_header` only affects the upstream request while `ngx.req.set_header` change the incoming request. Record the http headers in the access log file will show the difference. But you still can use it as an alternative of nginx `proxy_set_header` directive as long as you know the difference.

Here is an example of setting the `Content-Type` header:

```lua

 ngx.req.set_header("Content-Type", "text/css")
```

The `header_value` can take an array list of values,
for example,

```lua

 ngx.req.set_header("Foo", {"a", "abc"})
```

will produce two new request headers:

```bash

 Foo: a
 Foo: abc
```

and old `Foo` headers will be overridden if there is any.

When the `header_value` argument is `nil`, the request header will be removed. So

```lua

 ngx.req.set_header("X-Foo", nil)
```

is equivalent to

```lua

 ngx.req.clear_header("X-Foo")
```

Note: this function throws a Lua error if `header_name` or
`header_value` contain unsafe characters (control characters).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.clear_header

**syntax:** *ngx.req.clear_header(header_name)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;*

Clears the current request's request header named `header_name`. None of the current request's existing subrequests will be affected but subsequently initiated subrequests will inherit the change by default.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.read_body

**syntax:** *ngx.req.read_body()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Reads the client request body synchronously without blocking the Nginx event loop.

```lua

 ngx.req.read_body()
 local args = ngx.req.get_post_args()
```

If the request body is already read previously by turning on [lua_need_request_body](#lua_need_request_body) or by using other modules, then this function does not run and returns immediately.

If the request body has already been explicitly discarded, either by the [ngx.req.discard_body](#ngxreqdiscard_body) function or other modules, this function does not run and returns immediately.

In case of errors, such as connection errors while reading the data, this method will throw out a Lua exception *or* terminate the current request with a 500 status code immediately.

The request body data read using this function can be retrieved later via [ngx.req.get_body_data](#ngxreqget_body_data) or, alternatively, the temporary file name for the body data cached to disk using [ngx.req.get_body_file](#ngxreqget_body_file). This depends on

1. whether the current request body is already larger than the [client_body_buffer_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size),
1. and whether [client_body_in_file_only](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_in_file_only) has been switched on.

In cases where current request may have a request body and the request body data is not required, The [ngx.req.discard_body](#ngxreqdiscard_body) function must be used to explicitly discard the request body to avoid breaking things under HTTP 1.1 keepalive or HTTP 1.1 pipelining.

This function was first introduced in the `v0.3.1rc17` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.discard_body

**syntax:** *ngx.req.discard_body()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Explicitly discard the request body, i.e., read the data on the connection and throw it away immediately (without using the request body by any means).

This function is an asynchronous call and returns immediately.

If the request body has already been read, this function does nothing and returns immediately.

This function was first introduced in the `v0.3.1rc17` release.

See also [ngx.req.read_body](#ngxreqread_body).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.get_body_data

**syntax:** *data = ngx.req.get_body_data()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, log_by_lua&#42;*

Retrieves in-memory request body data. It returns a Lua string rather than a Lua table holding all the parsed query arguments. Use the [ngx.req.get_post_args](#ngxreqget_post_args) function instead if a Lua table is required.

This function returns `nil` if

1. the request body has not been read,
1. the request body has been read into disk temporary files,
1. or the request body has zero size.

If the request body has not been read yet, call [ngx.req.read_body](#ngxreqread_body) first (or turn on [lua_need_request_body](#lua_need_request_body) to force this module to read the request body. This is not recommended however).

If the request body has been read into disk files, try calling the [ngx.req.get_body_file](#ngxreqget_body_file) function instead.

To force in-memory request bodies, try setting [client_body_buffer_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size) to the same size value in [client_max_body_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size).

Note that calling this function instead of using `ngx.var.request_body` or `ngx.var.echo_request_body` is more efficient because it can save one dynamic memory allocation and one data copy.

This function was first introduced in the `v0.3.1rc17` release.

See also [ngx.req.get_body_file](#ngxreqget_body_file).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.get_body_file

**syntax:** *file_name = ngx.req.get_body_file()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Retrieves the file name for the in-file request body data. Returns `nil` if the request body has not been read or has been read into memory.

The returned file is read only and is usually cleaned up by Nginx's memory pool. It should not be manually modified, renamed, or removed in Lua code.

If the request body has not been read yet, call [ngx.req.read_body](#ngxreqread_body) first (or turn on [lua_need_request_body](#lua_need_request_body) to force this module to read the request body. This is not recommended however).

If the request body has been read into memory, try calling the [ngx.req.get_body_data](#ngxreqget_body_data) function instead.

To force in-file request bodies, try turning on [client_body_in_file_only](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_in_file_only).

This function was first introduced in the `v0.3.1rc17` release.

See also [ngx.req.get_body_data](#ngxreqget_body_data).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.set_body_data

**syntax:** *ngx.req.set_body_data(data)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Set the current request's request body using the in-memory data specified by the `data` argument.

If the request body has not been read yet, call [ngx.req.read_body](#ngxreqread_body) first (or turn on [lua_need_request_body](#lua_need_request_body) to force this module to read the request body. This is not recommended however). Additionally, the request body must not have been previously discarded by [ngx.req.discard_body](#ngxreqdiscard_body).

Whether the previous request body has been read into memory or buffered into a disk file, it will be freed or the disk file will be cleaned up immediately, respectively.

This function was first introduced in the `v0.3.1rc18` release.

See also [ngx.req.set_body_file](#ngxreqset_body_file).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.set_body_file

**syntax:** *ngx.req.set_body_file(file_name, auto_clean?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Set the current request's request body using the in-file data specified by the `file_name` argument.

If the request body has not been read yet, call [ngx.req.read_body](#ngxreqread_body) first (or turn on [lua_need_request_body](#lua_need_request_body) to force this module to read the request body. This is not recommended however). Additionally, the request body must not have been previously discarded by [ngx.req.discard_body](#ngxreqdiscard_body).

If the optional `auto_clean` argument is given a `true` value, then this file will be removed at request completion or the next time this function or [ngx.req.set_body_data](#ngxreqset_body_data) are called in the same request. The `auto_clean` is default to `false`.

Please ensure that the file specified by the `file_name` argument exists and is readable by an Nginx worker process by setting its permission properly to avoid Lua exception errors.

Whether the previous request body has been read into memory or buffered into a disk file, it will be freed or the disk file will be cleaned up immediately, respectively.

This function was first introduced in the `v0.3.1rc18` release.

See also [ngx.req.set_body_data](#ngxreqset_body_data).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.init_body

**syntax:** *ngx.req.init_body(buffer_size?)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Creates a new blank request body for the current request and initializes the buffer for later request body data writing via the [ngx.req.append_body](#ngxreqappend_body) and [ngx.req.finish_body](#ngxreqfinish_body) APIs.

If the `buffer_size` argument is specified, then its value will be used for the size of the memory buffer for body writing with [ngx.req.append_body](#ngxreqappend_body). If the argument is omitted, then the value specified by the standard [client_body_buffer_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size) directive will be used instead.

When the data can no longer be hold in the memory buffer for the request body, then the data will be flushed onto a temporary file just like the standard request body reader in the Nginx core.

It is important to always call the [ngx.req.finish_body](#ngxreqfinish_body) after all the data has been appended onto the current request body. Also, when this function is used together with [ngx.req.socket](#ngxreqsocket), it is required to call [ngx.req.socket](#ngxreqsocket) *before* this function, or you will get the "request body already exists" error message.

The usage of this function is often like this:

```lua

 ngx.req.init_body(128 * 1024)  -- buffer is 128KB
 for chunk in next_data_chunk() do
     ngx.req.append_body(chunk) -- each chunk can be 4KB
 end
 ngx.req.finish_body()
```

This function can be used with [ngx.req.append_body](#ngxreqappend_body), [ngx.req.finish_body](#ngxreqfinish_body), and [ngx.req.socket](#ngxreqsocket) to implement efficient input filters in pure Lua (in the context of [rewrite_by_lua*](#rewrite_by_lua) or [access_by_lua*](#access_by_lua)), which can be used with other Nginx content handler or upstream modules like [ngx_http_proxy_module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html) and [ngx_http_fastcgi_module](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html).

This function was first introduced in the `v0.5.11` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.req.append_body

**syntax:** *ngx.req.append_body(data_chunk)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Append new data chunk specified by the `data_chunk` argument onto the existing request body created by the [ngx.req.init_body](#ngxreqinit_body) call.

When the data can no longer be hold in the memory buffer for the request body, then the data will be flushed onto a temporary file just like the standard request body reader in the Nginx core.

It is important to always call the [ngx.req.finish_body](#ngxreqfinish_body) after all the data has been appended onto the current request body.

This function can be used with [ngx.req.init_body](#ngxreqinit_body), [ngx.req.finish_body](#ngxreqfinish_body), and [ngx.req.socket](#ngxreqsocket) to implement efficient input filters in pure Lua (in the context of [rewrite_by_lua*](#rewrite_by_lua) or [access_by_lua*](#access_by_lua)), which can be used with other Nginx content handler or upstream modules like [ngx_http_proxy_module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html) and [ngx_http_fastcgi_module](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html).

This function was first introduced in the `v0.5.11` release.

See also [ngx.req.init_body](#ngxreqinit_body).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.finish_body

**syntax:** *ngx.req.finish_body()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Completes the construction process of the new request body created by the [ngx.req.init_body](#ngxreqinit_body) and [ngx.req.append_body](#ngxreqappend_body) calls.

This function can be used with [ngx.req.init_body](#ngxreqinit_body), [ngx.req.append_body](#ngxreqappend_body), and [ngx.req.socket](#ngxreqsocket) to implement efficient input filters in pure Lua (in the context of [rewrite_by_lua*](#rewrite_by_lua) or [access_by_lua*](#access_by_lua)), which can be used with other Nginx content handler or upstream modules like [ngx_http_proxy_module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html) and [ngx_http_fastcgi_module](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html).

This function was first introduced in the `v0.5.11` release.

See also [ngx.req.init_body](#ngxreqinit_body).

[Back to TOC](#nginx-api-for-lua)

## ngx.req.socket

**syntax:** *tcpsock, err = ngx.req.socket()*

**syntax:** *tcpsock, err = ngx.req.socket(raw)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Returns a read-only cosocket object that wraps the downstream connection. Only [receive](#tcpsockreceive), [receiveany](#tcpsockreceiveany) and [receiveuntil](#tcpsockreceiveuntil) methods are supported on this object.

In case of error, `nil` will be returned as well as a string describing the error.

The socket object returned by this method is usually used to read the current request's body in a streaming fashion. Do not turn on the [lua_need_request_body](#lua_need_request_body) directive, and do not mix this call with [ngx.req.read_body](#ngxreqread_body) and [ngx.req.discard_body](#ngxreqdiscard_body).

If any request body data has been pre-read into the Nginx core request header buffer, the resulting cosocket object will take care of this to avoid potential data loss resulting from such pre-reading.
Chunked request bodies are not yet supported in this API.

Since the `v0.9.0` release, this function accepts an optional boolean `raw` argument. When this argument is `true`, this function returns a full-duplex cosocket object wrapping around the raw downstream connection socket, upon which you can call the [receive](#tcpsockreceive), [receiveany](#tcpsockreceiveany), [receiveuntil](#tcpsockreceiveuntil), and [send](#tcpsocksend) methods.

When the `raw` argument is `true`, it is required that no pending data from any previous [ngx.say](#ngxsay), [ngx.print](#ngxprint), or [ngx.send_headers](#ngxsend_headers) calls exists. So if you have these downstream output calls previously, you should call [ngx.flush(true)](#ngxflush) before calling `ngx.req.socket(true)` to ensure that there is no pending output data. If the request body has not been read yet, then this "raw socket" can also be used to read the request body.

You can use the "raw request socket" returned by `ngx.req.socket(true)` to implement fancy protocols like [WebSocket](https://en.wikipedia.org/wiki/WebSocket), or just emit your own raw HTTP response header or body data. You can refer to the [lua-resty-websocket library](https://github.com/openresty/lua-resty-websocket) for a real world example.

This function was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.exec

**syntax:** *ngx.exec(uri, args?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Does an internal redirect to `uri` with `args` and is similar to the [echo_exec](http://github.com/openresty/echo-nginx-module#echo_exec) directive of the [echo-nginx-module](http://github.com/openresty/echo-nginx-module).

```lua

 ngx.exec('/some-location')
 ngx.exec('/some-location', 'a=3&b=5&c=6')
 ngx.exec('/some-location?a=3&b=5', 'c=6')
```

The optional second `args` can be used to specify extra URI query arguments, for example:

```lua

 ngx.exec("/foo", "a=3&b=hello%20world")
```

Alternatively, a Lua table can be passed for the `args` argument for ngx_lua to carry out URI escaping and string concatenation.

```lua

 ngx.exec("/foo", { a = 3, b = "hello world" })
```

The result is exactly the same as the previous example.

The format for the Lua table passed as the `args` argument is identical to the format used in the [ngx.encode_args](#ngxencode_args) method.

Named locations are also supported but the second `args` argument will be ignored if present and the querystring for the new target is inherited from the referring location (if any).

`GET /foo/file.php?a=hello` will return "hello" and not "goodbye" in the example below

```nginx

 location /foo {
     content_by_lua_block {
         ngx.exec("@bar", "a=goodbye")
     }
 }

 location @bar {
     content_by_lua_block {
         local args = ngx.req.get_uri_args()
         for key, val in pairs(args) do
             if key == "a" then
                 ngx.say(val)
             end
         end
     }
 }
```

Note that the `ngx.exec` method is different from [ngx.redirect](#ngxredirect) in that
it is purely an internal redirect and that no new external HTTP traffic is involved.

Also note that this method call terminates the processing of the current request and that it *must* be called before [ngx.send_headers](#ngxsend_headers) or explicit response body
outputs by either [ngx.print](#ngxprint) or [ngx.say](#ngxsay).

It is recommended that a coding style that combines this method call with the `return` statement, i.e., `return ngx.exec(...)` be adopted when this method call is used in contexts other than [header_filter_by_lua*](#header_filter_by_lua) to reinforce the fact that the request processing is being terminated.

[Back to TOC](#nginx-api-for-lua)

## ngx.redirect

**syntax:** *ngx.redirect(uri, status?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Issue an `HTTP 301` or `302` redirection to `uri`.

Note: this function throws a Lua error if the `uri` argument
contains unsafe characters (control characters).

The optional `status` parameter specifies the HTTP status code to be used. The following status codes are supported right now:

* `301`
* `302` (default)
* `303`
* `307`
* `308`

It is `302` (`ngx.HTTP_MOVED_TEMPORARILY`) by default.

Here is an example assuming the current server name is `localhost` and that it is listening on port 1984:

```lua

 return ngx.redirect("/foo")
```

which is equivalent to

```lua

 return ngx.redirect("/foo", ngx.HTTP_MOVED_TEMPORARILY)
```

Redirecting arbitrary external URLs is also supported, for example:

```lua

 return ngx.redirect("http://www.google.com")
```

We can also use the numerical code directly as the second `status` argument:

```lua

 return ngx.redirect("/foo", 301)
```

This method is similar to the [rewrite](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#rewrite) directive with the `redirect` modifier in the standard
[ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html), for example, this `nginx.conf` snippet

```nginx

 rewrite ^ /foo? redirect;  # nginx config
```

is equivalent to the following Lua code

```lua

 return ngx.redirect('/foo')  -- Lua code
```

while

```nginx

 rewrite ^ /foo? permanent;  # nginx config
```

is equivalent to

```lua

 return ngx.redirect('/foo', ngx.HTTP_MOVED_PERMANENTLY)  -- Lua code
```

URI arguments can be specified as well, for example:

```lua

 return ngx.redirect('/foo?a=3&b=4')
```

Note that this method call terminates the processing of the current request and that it *must* be called before [ngx.send_headers](#ngxsend_headers) or explicit response body
outputs by either [ngx.print](#ngxprint) or [ngx.say](#ngxsay).

It is recommended that a coding style that combines this method call with the `return` statement, i.e., `return ngx.redirect(...)` be adopted when this method call is used in contexts other than [header_filter_by_lua*](#header_filter_by_lua) to reinforce the fact that the request processing is being terminated.

[Back to TOC](#nginx-api-for-lua)

## ngx.send_headers

**syntax:** *ok, err = ngx.send_headers()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Explicitly send out the response headers.

Since `v0.8.3` this function returns `1` on success, or returns `nil` and a string describing the error otherwise.

Note that there is normally no need to manually send out response headers as ngx_lua will automatically send headers out
before content is output with [ngx.say](#ngxsay) or [ngx.print](#ngxprint) or when [content_by_lua*](#content_by_lua) exits normally.

[Back to TOC](#nginx-api-for-lua)

## ngx.headers_sent

**syntax:** *value = ngx.headers_sent*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Returns `true` if the response headers have been sent (by ngx_lua), and `false` otherwise.

This API was first introduced in ngx_lua v0.3.1rc6.

[Back to TOC](#nginx-api-for-lua)

## ngx.print

**syntax:** *ok, err = ngx.print(...)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Emits arguments concatenated to the HTTP client (as response body). If response headers have not been sent, this function will send headers out first and then output body data.

Since `v0.8.3` this function returns `1` on success, or returns `nil` and a string describing the error otherwise.

Lua `nil` values will output `"nil"` strings and Lua boolean values will output `"true"` and `"false"` literal strings respectively.

Nested arrays of strings are permitted and the elements in the arrays will be sent one by one:

```lua

 local table = {
     "hello, ",
     {"world: ", true, " or ", false,
         {": ", nil}}
 }
 ngx.print(table)
```

will yield the output

```bash

 hello, world: true or false: nil
```

Non-array table arguments will cause a Lua exception to be thrown.

The `ngx.null` constant will yield the `"null"` string output.

This is an asynchronous call and will return immediately without waiting for all the data to be written into the system send buffer. To run in synchronous mode, call `ngx.flush(true)` after calling `ngx.print`. This can be particularly useful for streaming output. See [ngx.flush](#ngxflush) for more details.

Please note that both `ngx.print` and [ngx.say](#ngxsay) will always invoke the whole Nginx output body filter chain, which is an expensive operation. So be careful when calling either of these two in a tight loop; buffer the data yourself in Lua and save the calls.

[Back to TOC](#nginx-api-for-lua)

## ngx.say

**syntax:** *ok, err = ngx.say(...)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Just as [ngx.print](#ngxprint) but also emit a trailing newline.

[Back to TOC](#nginx-api-for-lua)

## ngx.log

**syntax:** *ngx.log(log_level, ...)*

**context:** *init_by_lua&#42;, init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Log arguments concatenated to error.log with the given logging level.

Lua `nil` arguments are accepted and result in literal `"nil"` string while Lua booleans result in literal `"true"` or `"false"` string outputs. And the `ngx.null` constant will yield the `"null"` string output.

The `log_level` argument can take constants like `ngx.ERR` and `ngx.WARN`. Check out [Nginx log level constants](#nginx-log-level-constants) for details.

There is a hard coded `2048` byte limitation on error message lengths in the Nginx core. This limit includes trailing newlines and leading time stamps. If the message size exceeds this limit, Nginx will truncate the message text accordingly. This limit can be manually modified by editing the `NGX_MAX_ERROR_STR` macro definition in the `src/core/ngx_log.h` file in the Nginx source tree.

[Back to TOC](#nginx-api-for-lua)

## ngx.flush

**syntax:** *ok, err = ngx.flush(wait?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Flushes response output to the client.

`ngx.flush` accepts an optional boolean `wait` argument (Default: `false`) first introduced in the `v0.3.1rc34` release. When called with the default argument, it issues an asynchronous call (Returns immediately without waiting for output data to be written into the system send buffer). Calling the function with the `wait` argument set to `true` switches to synchronous mode.

In synchronous mode, the function will not return until all output data has been written into the system send buffer or until the [send_timeout](http://nginx.org/en/docs/http/ngx_http_core_module.html#send_timeout) setting has expired. Note that using the Lua coroutine mechanism means that this function does not block the Nginx event loop even in the synchronous mode.

When `ngx.flush(true)` is called immediately after [ngx.print](#ngxprint) or [ngx.say](#ngxsay), it causes the latter functions to run in synchronous mode. This can be particularly useful for streaming output.

Note that `ngx.flush` is not functional when in the HTTP 1.0 output buffering mode. See [HTTP 1.0 support](#http-10-support).

Since `v0.8.3` this function returns `1` on success, or returns `nil` and a string describing the error otherwise.

[Back to TOC](#nginx-api-for-lua)

## ngx.exit

**syntax:** *ngx.exit(status)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

When `status >= 200` (i.e., `ngx.HTTP_OK` and above), it will interrupt the execution of the current request and return status code to Nginx.

When `status == 0` (i.e., `ngx.OK`), it will only quit the current phase handler (or the content handler if the [content_by_lua*](#content_by_lua) directive is used) and continue to run later phases (if any) for the current request.

The `status` argument can be `ngx.OK`, `ngx.ERROR`, `ngx.HTTP_NOT_FOUND`,
`ngx.HTTP_MOVED_TEMPORARILY`, or other [HTTP status constants](#http-status-constants).

To return an error page with custom contents, use code snippets like this:

```lua

 ngx.status = ngx.HTTP_GONE
 ngx.say("This is our own content")
 -- to cause quit the whole request rather than the current phase handler
 ngx.exit(ngx.HTTP_OK)
```

The effect in action:

```bash

 $ curl -i http://localhost/test
 HTTP/1.1 410 Gone
 Server: nginx/1.0.6
 Date: Thu, 15 Sep 2011 00:51:48 GMT
 Content-Type: text/plain
 Transfer-Encoding: chunked
 Connection: keep-alive

 This is our own content
```

Number literals can be used directly as the argument, for instance,

```lua

 ngx.exit(501)
```

Note that while this method accepts all [HTTP status constants](#http-status-constants) as input, it only accepts `ngx.OK` and `ngx.ERROR` of the [core constants](#core-constants).

Also note that this method call terminates the processing of the current request and that it is recommended that a coding style that combines this method call with the `return` statement, i.e., `return ngx.exit(...)` be used to reinforce the fact that the request processing is being terminated.

When being used in the contexts of [header_filter_by_lua*](#header_filter_by_lua), [balancer_by_lua*](#balancer_by_lua_block), and
[ssl_session_store_by_lua*](#ssl_session_store_by_lua_block), `ngx.exit()` is
an asynchronous operation and will return immediately. This behavior may change in future and it is recommended that users always use `return` in combination as suggested above.

[Back to TOC](#nginx-api-for-lua)

## ngx.eof

**syntax:** *ok, err = ngx.eof()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Explicitly specify the end of the response output stream. In the case of HTTP 1.1 chunked encoded output, it will just trigger the Nginx core to send out the "last chunk".

When you disable the HTTP 1.1 keep-alive feature for your downstream connections, you can rely on well written HTTP clients to close the connection actively for you when you call this method. This trick can be used do back-ground jobs without letting the HTTP clients to wait on the connection, as in the following example:

```nginx

 location = /async {
     keepalive_timeout 0;
     content_by_lua_block {
         ngx.say("got the task!")
         ngx.eof()  -- well written HTTP clients will close the connection at this point
         -- access MySQL, PostgreSQL, Redis, Memcached, and etc here...
     }
 }
```

But if you create subrequests to access other locations configured by Nginx upstream modules, then you should configure those upstream modules to ignore client connection abortions if they are not by default. For example, by default the standard [ngx_http_proxy_module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html) will terminate both the subrequest and the main request as soon as the client closes the connection, so it is important to turn on the [proxy_ignore_client_abort](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_ignore_client_abort) directive in your location block configured by [ngx_http_proxy_module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html):

```nginx

 proxy_ignore_client_abort on;
```

A better way to do background jobs is to use the [ngx.timer.at](#ngxtimerat) API.

Since `v0.8.3` this function returns `1` on success, or returns `nil` and a string describing the error otherwise.

[Back to TOC](#nginx-api-for-lua)

## ngx.sleep

**syntax:** *ngx.sleep(seconds)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Sleeps for the specified seconds without blocking. One can specify time resolution up to 0.001 seconds (i.e., one millisecond).

Behind the scene, this method makes use of the Nginx timers.

Since the `0.7.20` release, The `0` time argument can also be specified.

This method was introduced in the `0.5.0rc30` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.escape_uri

**syntax:** *newstr = ngx.escape_uri(str, type?)*

**context:** *init_by_lua&#42;, init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Since `v0.10.16`, this function accepts an optional `type` argument.
It accepts the following values (defaults to `2`):

* `0`: escapes `str` as a full URI. And the characters
` ` (space), `#`, `%`,
`?`, 0x00 ~ 0x1F, 0x7F ~ 0xFF will be escaped.
* `2`: escape `str` as a URI component. All characters except
alphabetic characters, digits, `-`, `.`, `_`,
`~` will be encoded as `%XX`.

[Back to TOC](#nginx-api-for-lua)

## ngx.unescape_uri

**syntax:** *newstr = ngx.unescape_uri(str)*

**context:** *init_by_lua&#42;, init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Unescape `str` as an escaped URI component.

For example,

```lua

 ngx.say(ngx.unescape_uri("b%20r56+7"))
```

gives the output


    b r56 7


Invalid escaping sequences are handled in a conventional way: `%`s are left unchanged. Also, characters that should not appear in escaped string are simply left unchanged.

For example,

```lua

 ngx.say(ngx.unescape_uri("try %search%%20%again%"))
```

gives the output


    try %search% %again%


(Note that `%20` following `%` got unescaped, even it can be considered a part of invalid sequence.)

[Back to TOC](#nginx-api-for-lua)

## ngx.encode_args

**syntax:** *str = ngx.encode_args(table)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Encode the Lua table to a query args string according to the URI encoded rules.

For example,

```lua

 ngx.encode_args({foo = 3, ["b r"] = "hello world"})
```

yields


    foo=3&b%20r=hello%20world


The table keys must be Lua strings.

Multi-value query args are also supported. Just use a Lua table for the argument's value, for example:

```lua

 ngx.encode_args({baz = {32, "hello"}})
```

gives


    baz=32&baz=hello


If the value table is empty and the effect is equivalent to the `nil` value.

Boolean argument values are also supported, for instance,

```lua

 ngx.encode_args({a = true, b = 1})
```

yields


    a&b=1


If the argument value is `false`, then the effect is equivalent to the `nil` value.

This method was first introduced in the `v0.3.1rc27` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.decode_args

**syntax:** *table, err = ngx.decode_args(str, max_args?)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Decodes a URI encoded query-string into a Lua table. This is the inverse function of [ngx.encode_args](#ngxencode_args).

The optional `max_args` argument can be used to specify the maximum number of arguments parsed from the `str` argument. By default, a maximum of 100 request arguments are parsed (including those with the same name) and that additional URI arguments are silently discarded to guard against potential denial of service attacks. Since `v0.10.13`, when the limit is exceeded, it will return a second value which is the string `"truncated"`.

This argument can be set to zero to remove the limit and to process all request arguments received:

```lua

 local args = ngx.decode_args(str, 0)
```

Removing the `max_args` cap is strongly discouraged.

This method was introduced in the `v0.5.0rc29`.

[Back to TOC](#nginx-api-for-lua)

## ngx.encode_base64

**syntax:** *newstr = ngx.encode_base64(str, no_padding?)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Encodes `str` to a base64 digest.

Since the `0.9.16` release, an optional boolean-typed `no_padding` argument can be specified to control whether the base64 padding should be appended to the resulting digest (default to `false`, i.e., with padding enabled).

[Back to TOC](#nginx-api-for-lua)

## ngx.decode_base64

**syntax:** *newstr = ngx.decode_base64(str)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Decodes the `str` argument as a base64 digest to the raw form. Returns `nil` if `str` is not well formed.

[Back to TOC](#nginx-api-for-lua)

## ngx.crc32_short

**syntax:** *intval = ngx.crc32_short(str)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Calculates the CRC-32 (Cyclic Redundancy Code) digest for the `str` argument.

This method performs better on relatively short `str` inputs (i.e., less than 30 ~ 60 bytes), as compared to [ngx.crc32_long](#ngxcrc32_long). The result is exactly the same as [ngx.crc32_long](#ngxcrc32_long).

Behind the scene, it is just a thin wrapper around the `ngx_crc32_short` function defined in the Nginx core.

This API was first introduced in the `v0.3.1rc8` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.crc32_long

**syntax:** *intval = ngx.crc32_long(str)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Calculates the CRC-32 (Cyclic Redundancy Code) digest for the `str` argument.

This method performs better on relatively long `str` inputs (i.e., longer than 30 ~ 60 bytes), as compared to [ngx.crc32_short](#ngxcrc32_short).  The result is exactly the same as [ngx.crc32_short](#ngxcrc32_short).

Behind the scene, it is just a thin wrapper around the `ngx_crc32_long` function defined in the Nginx core.

This API was first introduced in the `v0.3.1rc8` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.hmac_sha1

**syntax:** *digest = ngx.hmac_sha1(secret_key, str)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Computes the [HMAC-SHA1](https://en.wikipedia.org/wiki/HMAC) digest of the argument `str` and turns the result using the secret key `<secret_key>`.

The raw binary form of the `HMAC-SHA1` digest will be generated, use [ngx.encode_base64](#ngxencode_base64), for example, to encode the result to a textual representation if desired.

For example,

```lua

 local key = "thisisverysecretstuff"
 local src = "some string we want to sign"
 local digest = ngx.hmac_sha1(key, src)
 ngx.say(ngx.encode_base64(digest))
```

yields the output


    R/pvxzHC4NLtj7S+kXFg/NePTmk=


This API requires the OpenSSL library enabled in the Nginx build (usually by passing the `--with-http_ssl_module` option to the `./configure` script).

This function was first introduced in the `v0.3.1rc29` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.md5

**syntax:** *digest = ngx.md5(str)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the hexadecimal representation of the MD5 digest of the `str` argument.

For example,

```nginx

 location = /md5 {
     content_by_lua_block {
         ngx.say(ngx.md5("hello"))
     }
 }
```

yields the output


    5d41402abc4b2a76b9719d911017c592


See [ngx.md5_bin](#ngxmd5_bin) if the raw binary MD5 digest is required.

[Back to TOC](#nginx-api-for-lua)

## ngx.md5_bin

**syntax:** *digest = ngx.md5_bin(str)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the binary form of the MD5 digest of the `str` argument.

See [ngx.md5](#ngxmd5) if the hexadecimal form of the MD5 digest is required.

[Back to TOC](#nginx-api-for-lua)

## ngx.sha1_bin

**syntax:** *digest = ngx.sha1_bin(str)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the binary form of the SHA-1 digest of the `str` argument.

This function requires SHA-1 support in the Nginx build. (This usually just means OpenSSL should be installed while building Nginx).

This function was first introduced in the `v0.5.0rc6`.

[Back to TOC](#nginx-api-for-lua)

## ngx.quote_sql_str

**syntax:** *quoted_value = ngx.quote_sql_str(raw_value)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns a quoted SQL string literal according to the MySQL quoting rules.

[Back to TOC](#nginx-api-for-lua)

## ngx.today

**syntax:** *str = ngx.today()*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns current date (in the format `yyyy-mm-dd`) from the Nginx cached time (no syscall involved unlike Lua's date library).

This is the local time.

[Back to TOC](#nginx-api-for-lua)

## ngx.time

**syntax:** *secs = ngx.time()*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the elapsed seconds from the epoch for the current time stamp from the Nginx cached time (no syscall involved unlike Lua's date library).

Updates of the Nginx time cache can be forced by calling [ngx.update_time](#ngxupdate_time) first.

[Back to TOC](#nginx-api-for-lua)

## ngx.now

**syntax:** *secs = ngx.now()*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns a floating-point number for the elapsed time in seconds (including milliseconds as the decimal part) from the epoch for the current time stamp from the Nginx cached time (no syscall involved unlike Lua's date library).

You can forcibly update the Nginx time cache by calling [ngx.update_time](#ngxupdate_time) first.

This API was first introduced in `v0.3.1rc32`.

[Back to TOC](#nginx-api-for-lua)

## ngx.update_time

**syntax:** *ngx.update_time()*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Forcibly updates the Nginx current time cache. This call involves a syscall and thus has some overhead, so do not abuse it.

This API was first introduced in `v0.3.1rc32`.

[Back to TOC](#nginx-api-for-lua)

## ngx.localtime

**syntax:** *str = ngx.localtime()*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the current time stamp (in the format `yyyy-mm-dd hh:mm:ss`) of the Nginx cached time (no syscall involved unlike Lua's [os.date](https://www.lua.org/manual/5.1/manual.html#pdf-os.date) function).

This is the local time.

[Back to TOC](#nginx-api-for-lua)

## ngx.utctime

**syntax:** *str = ngx.utctime()*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the current time stamp (in the format `yyyy-mm-dd hh:mm:ss`) of the Nginx cached time (no syscall involved unlike Lua's [os.date](https://www.lua.org/manual/5.1/manual.html#pdf-os.date) function).

This is the UTC time.

[Back to TOC](#nginx-api-for-lua)

## ngx.cookie_time

**syntax:** *str = ngx.cookie_time(sec)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns a formatted string can be used as the cookie expiration time. The parameter `sec` is the time stamp in seconds (like those returned from [ngx.time](#ngxtime)).

```nginx

 ngx.say(ngx.cookie_time(1290079655))
     -- yields "Thu, 18-Nov-10 11:27:35 GMT"
```

[Back to TOC](#nginx-api-for-lua)

## ngx.http_time

**syntax:** *str = ngx.http_time(sec)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns a formated string can be used as the http header time (for example, being used in `Last-Modified` header). The parameter `sec` is the time stamp in seconds (like those returned from [ngx.time](#ngxtime)).

```nginx

 ngx.say(ngx.http_time(1290079655))
     -- yields "Thu, 18 Nov 2010 11:27:35 GMT"
```

[Back to TOC](#nginx-api-for-lua)

## ngx.parse_http_time

**syntax:** *sec = ngx.parse_http_time(str)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Parse the http time string (as returned by [ngx.http_time](#ngxhttp_time)) into seconds. Returns the seconds or `nil` if the input string is in bad forms.

```nginx

 local time = ngx.parse_http_time("Thu, 18 Nov 2010 11:27:35 GMT")
 if time == nil then
     ...
 end
```

[Back to TOC](#nginx-api-for-lua)

## ngx.is_subrequest

**syntax:** *value = ngx.is_subrequest*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;*

Returns `true` if the current request is an Nginx subrequest, or `false` otherwise.

[Back to TOC](#nginx-api-for-lua)

## ngx.re.match

**syntax:** *captures, err = ngx.re.match(subject, regex, options?, ctx?, res_table?)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Matches the `subject` string using the Perl compatible regular expression `regex` with the optional `options`.

Only the first occurrence of the match is returned, or `nil` if no match is found. In case of errors, like seeing a bad regular expression or exceeding the PCRE stack limit, `nil` and a string describing the error will be returned.

When a match is found, a Lua table `captures` is returned, where `captures[0]` holds the whole substring being matched, and `captures[1]` holds the first parenthesized sub-pattern's capturing, `captures[2]` the second, and so on.

```lua

 local m, err = ngx.re.match("hello, 1234", "[0-9]+")
 if m then
     -- m[0] == "1234"

 else
     if err then
         ngx.log(ngx.ERR, "error: ", err)
         return
     end

     ngx.say("match not found")
 end
```

```lua

 local m, err = ngx.re.match("hello, 1234", "([0-9])[0-9]+")
 -- m[0] == "1234"
 -- m[1] == "1"
```

Named captures are also supported since the `v0.7.14` release
and are returned in the same Lua table as key-value pairs as the numbered captures.

```lua

 local m, err = ngx.re.match("hello, 1234", "([0-9])(?<remaining>[0-9]+)")
 -- m[0] == "1234"
 -- m[1] == "1"
 -- m[2] == "234"
 -- m["remaining"] == "234"
```

Unmatched subpatterns will have `false` values in their `captures` table fields.

```lua

 local m, err = ngx.re.match("hello, world", "(world)|(hello)|(?<named>howdy)")
 -- m[0] == "hello"
 -- m[1] == false
 -- m[2] == "hello"
 -- m[3] == false
 -- m["named"] == false
```

Specify `options` to control how the match operation will be performed. The following option characters are supported:


    a             anchored mode (only match from the beginning)

    d             enable the DFA mode (or the longest token match semantics).
                  this requires PCRE 6.0+ or else a Lua exception will be thrown.
                  first introduced in ngx_lua v0.3.1rc30.

    D             enable duplicate named pattern support. This allows named
                  subpattern names to be repeated, returning the captures in
                  an array-like Lua table. for example,
                    local m = ngx.re.match("hello, world",
                                           "(?<named>\w+), (?<named>\w+)",
                                           "D")
                    -- m["named"] == {"hello", "world"}
                  this option was first introduced in the v0.7.14 release.
                  this option requires at least PCRE 8.12.

    i             case insensitive mode (similar to Perl's /i modifier)

    j             enable PCRE JIT compilation, this requires PCRE 8.21+ which
                  must be built with the --enable-jit option. for optimum performance,
                  this option should always be used together with the 'o' option.
                  first introduced in ngx_lua v0.3.1rc30.

    J             enable the PCRE Javascript compatible mode. this option was
                  first introduced in the v0.7.14 release. this option requires
                  at least PCRE 8.12.

    m             multi-line mode (similar to Perl's /m modifier)

    o             compile-once mode (similar to Perl's /o modifier),
                  to enable the worker-process-level compiled-regex cache

    s             single-line mode (similar to Perl's /s modifier)

    u             UTF-8 mode. this requires PCRE to be built with
                  the --enable-utf8 option or else a Lua exception will be thrown.

    U             similar to "u" but disables PCRE's UTF-8 validity check on
                  the subject string. first introduced in ngx_lua v0.8.1.

    x             extended mode (similar to Perl's /x modifier)


These options can be combined:

```nginx

 local m, err = ngx.re.match("hello, world", "HEL LO", "ix")
 -- m[0] == "hello"
```

```nginx

 local m, err = ngx.re.match("hello, 美好生活", "HELLO, (.{2})", "iu")
 -- m[0] == "hello, 美好"
 -- m[1] == "美好"
```

The `o` option is useful for performance tuning, because the regex pattern in question will only be compiled once, cached in the worker-process level, and shared among all requests in the current Nginx worker process. The upper limit of the regex cache can be tuned via the [lua_regex_cache_max_entries](#lua_regex_cache_max_entries) directive.

The optional fourth argument, `ctx`, can be a Lua table holding an optional `pos` field. When the `pos` field in the `ctx` table argument is specified, `ngx.re.match` will start matching from that offset (starting from 1). Regardless of the presence of the `pos` field in the `ctx` table, `ngx.re.match` will always set this `pos` field to the position *after* the substring matched by the whole pattern in case of a successful match. When match fails, the `ctx` table will be left intact.

```lua

 local ctx = {}
 local m, err = ngx.re.match("1234, hello", "[0-9]+", "", ctx)
      -- m[0] = "1234"
      -- ctx.pos == 5
```

```lua

 local ctx = { pos = 2 }
 local m, err = ngx.re.match("1234, hello", "[0-9]+", "", ctx)
      -- m[0] = "234"
      -- ctx.pos == 5
```

The `ctx` table argument combined with the `a` regex modifier can be used to construct a lexer atop `ngx.re.match`.

Note that, the `options` argument is not optional when the `ctx` argument is specified and that the empty Lua string (`""`) must be used as placeholder for `options` if no meaningful regex options are required.

This method requires the PCRE library enabled in Nginx ([Known Issue With Special Escaping Sequences](#special-escaping-sequences)).

To confirm that PCRE JIT is enabled, activate the Nginx debug log by adding the `--with-debug` option to Nginx or OpenResty's `./configure` script. Then, enable the "debug" error log level in `error_log` directive. The following message will be generated if PCRE JIT is enabled:


    pcre JIT compiling result: 1


Starting from the `0.9.4` release, this function also accepts a 5th argument, `res_table`, for letting the caller supply the Lua table used to hold all the capturing results. Starting from `0.9.6`, it is the caller's responsibility to ensure this table is empty. This is very useful for recycling Lua tables and saving GC and table allocation overhead.

This feature was introduced in the `v0.2.1rc11` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.re.find

**syntax:** *from, to, err = ngx.re.find(subject, regex, options?, ctx?, nth?)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Similar to [ngx.re.match](#ngxrematch) but only returns the beginning index (`from`) and end index (`to`) of the matched substring. The returned indexes are 1-based and can be fed directly into the [string.sub](https://www.lua.org/manual/5.1/manual.html#pdf-string.sub) API function to obtain the matched substring.

In case of errors (like bad regexes or any PCRE runtime errors), this API function returns two `nil` values followed by a string describing the error.

If no match is found, this function just returns a `nil` value.

Below is an example:

```lua

 local s = "hello, 1234"
 local from, to, err = ngx.re.find(s, "([0-9]+)", "jo")
 if from then
     ngx.say("from: ", from)
     ngx.say("to: ", to)
     ngx.say("matched: ", string.sub(s, from, to))
 else
     if err then
         ngx.say("error: ", err)
         return
     end
     ngx.say("not matched!")
 end
```

This example produces the output

    from: 8
    to: 11
    matched: 1234

Because this API function does not create new Lua strings nor new Lua tables, it is much faster than [ngx.re.match](#ngxrematch). It should be used wherever possible.

Since the `0.9.3` release, an optional 5th argument, `nth`, is supported to specify which (submatch) capture's indexes to return. When `nth` is 0 (which is the default), the indexes for the whole matched substring is returned; when `nth` is 1, then the 1st submatch capture's indexes are returned; when `nth` is 2, then the 2nd submatch capture is returned, and so on. When the specified submatch does not have a match, then two `nil` values will be returned. Below is an example for this:

```lua

 local str = "hello, 1234"
 local from, to = ngx.re.find(str, "([0-9])([0-9]+)", "jo", nil, 2)
 if from then
     ngx.say("matched 2nd submatch: ", string.sub(str, from, to))  -- yields "234"
 end
```

This API function was first introduced in the `v0.9.2` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.re.gmatch

**syntax:** *iterator, err = ngx.re.gmatch(subject, regex, options?)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Similar to [ngx.re.match](#ngxrematch), but returns a Lua iterator instead, so as to let the user programmer iterate all the matches over the `<subject>` string argument with the PCRE `regex`.

In case of errors, like seeing an ill-formed regular expression, `nil` and a string describing the error will be returned.

Here is a small example to demonstrate its basic usage:

```lua

 local iterator, err = ngx.re.gmatch("hello, world!", "([a-z]+)", "i")
 if not iterator then
     ngx.log(ngx.ERR, "error: ", err)
     return
 end

 local m
 m, err = iterator()    -- m[0] == m[1] == "hello"
 if err then
     ngx.log(ngx.ERR, "error: ", err)
     return
 end

 m, err = iterator()    -- m[0] == m[1] == "world"
 if err then
     ngx.log(ngx.ERR, "error: ", err)
     return
 end

 m, err = iterator()    -- m == nil
 if err then
     ngx.log(ngx.ERR, "error: ", err)
     return
 end
```

More often we just put it into a Lua loop:

```lua

 local it, err = ngx.re.gmatch("hello, world!", "([a-z]+)", "i")
 if not it then
     ngx.log(ngx.ERR, "error: ", err)
     return
 end

 while true do
     local m, err = it()
     if err then
         ngx.log(ngx.ERR, "error: ", err)
         return
     end

     if not m then
         -- no match found (any more)
         break
     end

     -- found a match
     ngx.say(m[0])
     ngx.say(m[1])
 end
```

The optional `options` argument takes exactly the same semantics as the [ngx.re.match](#ngxrematch) method.

The current implementation requires that the iterator returned should only be used in a single request. That is, one should *not* assign it to a variable belonging to persistent namespace like a Lua package.

This method requires the PCRE library enabled in Nginx ([Known Issue With Special Escaping Sequences](#special-escaping-sequences)).

This feature was first introduced in the `v0.2.1rc12` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.re.sub

**syntax:** *newstr, n, err = ngx.re.sub(subject, regex, replace, options?)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Substitutes the first match of the Perl compatible regular expression `regex` on the `subject` argument string with the string or function argument `replace`. The optional `options` argument has exactly the same meaning as in [ngx.re.match](#ngxrematch).

This method returns the resulting new string as well as the number of successful substitutions. In case of failures, like syntax errors in the regular expressions or the `<replace>` string argument, it will return `nil` and a string describing the error.

When the `replace` is a string, then it is treated as a special template for string replacement. For example,

```lua

 local newstr, n, err = ngx.re.sub("hello, 1234", "([0-9])[0-9]", "[$0][$1]")
 if not newstr then
     ngx.log(ngx.ERR, "error: ", err)
     return
 end

 -- newstr == "hello, [12][1]34"
 -- n == 1
```

where `$0` referring to the whole substring matched by the pattern and `$1` referring to the first parenthesized capturing substring.

Curly braces can also be used to disambiguate variable names from the background string literals:

```lua

 local newstr, n, err = ngx.re.sub("hello, 1234", "[0-9]", "${0}00")
 -- newstr == "hello, 100234"
 -- n == 1
```

Literal dollar sign characters (`$`) in the `replace` string argument can be escaped by another dollar sign, for instance,

```lua

 local newstr, n, err = ngx.re.sub("hello, 1234", "[0-9]", "$$")
 -- newstr == "hello, $234"
 -- n == 1
```

Do not use backlashes to escape dollar signs; it will not work as expected.

When the `replace` argument is of type "function", then it will be invoked with the "match table" as the argument to generate the replace string literal for substitution. The "match table" fed into the `replace` function is exactly the same as the return value of [ngx.re.match](#ngxrematch). Here is an example:

```lua

 local func = function (m)
     return "[" .. m[0] .. "][" .. m[1] .. "]"
 end

 local newstr, n, err = ngx.re.sub("hello, 1234", "( [0-9] ) [0-9]", func, "x")
 -- newstr == "hello, [12][1]34"
 -- n == 1
```

The dollar sign characters in the return value of the `replace` function argument are not special at all.

This method requires the PCRE library enabled in Nginx ([Known Issue With Special Escaping Sequences](#special-escaping-sequences)).

This feature was first introduced in the `v0.2.1rc13` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.re.gsub

**syntax:** *newstr, n, err = ngx.re.gsub(subject, regex, replace, options?)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Just like [ngx.re.sub](#ngxresub), but does global substitution.

Here is some examples:

```lua

 local newstr, n, err = ngx.re.gsub("hello, world", "([a-z])[a-z]+", "[$0,$1]", "i")
 if not newstr then
     ngx.log(ngx.ERR, "error: ", err)
     return
 end

 -- newstr == "[hello,h], [world,w]"
 -- n == 2
```

```lua

 local func = function (m)
     return "[" .. m[0] .. "," .. m[1] .. "]"
 end
 local newstr, n, err = ngx.re.gsub("hello, world", "([a-z])[a-z]+", func, "i")
 -- newstr == "[hello,h], [world,w]"
 -- n == 2
```

This method requires the PCRE library enabled in Nginx ([Known Issue With Special Escaping Sequences](#special-escaping-sequences)).

This feature was first introduced in the `v0.2.1rc15` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT

**syntax:** *dict = ngx.shared.DICT*

**syntax:** *dict = ngx.shared\[name_var\]*

**context:** *init_by_lua&#42;, init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Fetching the shm-based Lua dictionary object for the shared memory zone named `DICT` defined by the [lua_shared_dict](#lua_shared_dict) directive.

Shared memory zones are always shared by all the Nginx worker processes in the current Nginx server instance.

The resulting object `dict` has the following methods:

* [get](#ngxshareddictget)
* [get_stale](#ngxshareddictget_stale)
* [set](#ngxshareddictset)
* [safe_set](#ngxshareddictsafe_set)
* [add](#ngxshareddictadd)
* [safe_add](#ngxshareddictsafe_add)
* [replace](#ngxshareddictreplace)
* [delete](#ngxshareddictdelete)
* [incr](#ngxshareddictincr)
* [lpush](#ngxshareddictlpush)
* [rpush](#ngxshareddictrpush)
* [lpop](#ngxshareddictlpop)
* [rpop](#ngxshareddictrpop)
* [llen](#ngxshareddictllen)
* [ttl](#ngxshareddictttl)
* [expire](#ngxshareddictexpire)
* [flush_all](#ngxshareddictflush_all)
* [flush_expired](#ngxshareddictflush_expired)
* [get_keys](#ngxshareddictget_keys)
* [capacity](#ngxshareddictcapacity)
* [free_space](#ngxshareddictfree_space)

All these methods are *atomic* operations, that is, safe from concurrent accesses from multiple Nginx worker processes for the same `lua_shared_dict` zone.

Here is an example:

```nginx

 http {
     lua_shared_dict dogs 10m;
     server {
         location /set {
             content_by_lua_block {
                 local dogs = ngx.shared.dogs
                 dogs:set("Jim", 8)
                 ngx.say("STORED")
             }
         }
         location /get {
             content_by_lua_block {
                 local dogs = ngx.shared.dogs
                 ngx.say(dogs:get("Jim"))
             }
         }
     }
 }
```

Let us test it:

```bash

 $ curl localhost/set
 STORED

 $ curl localhost/get
 8

 $ curl localhost/get
 8
```

The number `8` will be consistently output when accessing `/get` regardless of how many Nginx workers there are because the `dogs` dictionary resides in the shared memory and visible to *all* of the worker processes.

The shared dictionary will retain its contents through a server config reload (either by sending the `HUP` signal to the Nginx process or by using the `-s reload` command-line option).

The contents in the dictionary storage will be lost, however, when the Nginx server quits.

This feature was first introduced in the `v0.3.1rc22` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.get

**syntax:** *value, flags = ngx.shared.DICT:get(key)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Retrieving the value in the dictionary [ngx.shared.DICT](#ngxshareddict) for the key `key`. If the key does not exist or has expired, then `nil` will be returned.

In case of errors, `nil` and a string describing the error will be returned.

The value returned will have the original data type when they were inserted into the dictionary, for example, Lua booleans, numbers, or strings.

The first argument to this method must be the dictionary object itself, for example,

```lua

 local cats = ngx.shared.cats
 local value, flags = cats.get(cats, "Marry")
```

or use Lua's syntactic sugar for method calls:

```lua

 local cats = ngx.shared.cats
 local value, flags = cats:get("Marry")
```

These two forms are fundamentally equivalent.

If the user flags is `0` (the default), then no flags value will be returned.

This feature was first introduced in the `v0.3.1rc22` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.get_stale

**syntax:** *value, flags, stale = ngx.shared.DICT:get_stale(key)*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Similar to the [get](#ngxshareddictget) method but returns the value even if the key has already expired.

Returns a 3rd value, `stale`, indicating whether the key has expired or not.

Note that the value of an expired key is not guaranteed to be available so one should never rely on the availability of expired items.

This method was first introduced in the `0.8.6` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.set

**syntax:** *success, err, forcible = ngx.shared.DICT:set(key, value, exptime?, flags?)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Unconditionally sets a key-value pair into the shm-based dictionary [ngx.shared.DICT](#ngxshareddict). Returns three values:

* `success`: boolean value to indicate whether the key-value pair is stored or not.
* `err`: textual error message, can be `"no memory"`.
* `forcible`: a boolean value to indicate whether other valid items have been removed forcibly when out of storage in the shared memory zone.

The `value` argument inserted can be Lua booleans, numbers, strings, or `nil`. Their value type will also be stored into the dictionary and the same data type can be retrieved later via the [get](#ngxshareddictget) method.

The optional `exptime` argument specifies expiration time (in seconds) for the inserted key-value pair. The time resolution is `0.001` seconds. If the `exptime` takes the value `0` (which is the default), then the item will never expire.

The optional `flags` argument specifies a user flags value associated with the entry to be stored. It can also be retrieved later with the value. The user flags is stored as an unsigned 32-bit integer internally. Defaults to `0`. The user flags argument was first introduced in the `v0.5.0rc2` release.

When it fails to allocate memory for the current key-value item, then `set` will try removing existing items in the storage according to the Least-Recently Used (LRU) algorithm. Note that, LRU takes priority over expiration time here. If up to tens of existing items have been removed and the storage left is still insufficient (either due to the total capacity limit specified by [lua_shared_dict](#lua_shared_dict) or memory segmentation), then the `err` return value will be `no memory` and `success` will be `false`.

If the sizes of items in the dictionary are not multiples or even powers of a certain value (like 2), it is easier to encounter `no memory` error because of memory fragmentation. It is recommended to use different dictionaries for different sizes of items.

When you encounter `no memory` error, you can also evict more least-recently-used items by retrying this method call more times to to make room for the current item.

If this method succeeds in storing the current item by forcibly removing other not-yet-expired items in the dictionary via LRU, the `forcible` return value will be `true`. If it stores the item without forcibly removing other valid items, then the return value `forcible` will be `false`.

The first argument to this method must be the dictionary object itself, for example,

```lua

 local cats = ngx.shared.cats
 local succ, err, forcible = cats.set(cats, "Marry", "it is a nice cat!")
```

or use Lua's syntactic sugar for method calls:

```lua

 local cats = ngx.shared.cats
 local succ, err, forcible = cats:set("Marry", "it is a nice cat!")
```

These two forms are fundamentally equivalent.

This feature was first introduced in the `v0.3.1rc22` release.

Please note that while internally the key-value pair is set atomically, the atomicity does not go across the method call boundary.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.safe_set

**syntax:** *ok, err = ngx.shared.DICT:safe_set(key, value, exptime?, flags?)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Similar to the [set](#ngxshareddictset) method, but never overrides the (least recently used) unexpired items in the store when running out of storage in the shared memory zone. In this case, it will immediately return `nil` and the string "no memory".

This feature was first introduced in the `v0.7.18` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.add

**syntax:** *success, err, forcible = ngx.shared.DICT:add(key, value, exptime?, flags?)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Just like the [set](#ngxshareddictset) method, but only stores the key-value pair into the dictionary [ngx.shared.DICT](#ngxshareddict) if the key does *not* exist.

If the `key` argument already exists in the dictionary (and not expired for sure), the `success` return value will be `false` and the `err` return value will be `"exists"`.

This feature was first introduced in the `v0.3.1rc22` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.safe_add

**syntax:** *ok, err = ngx.shared.DICT:safe_add(key, value, exptime?, flags?)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Similar to the [add](#ngxshareddictadd) method, but never overrides the (least recently used) unexpired items in the store when running out of storage in the shared memory zone. In this case, it will immediately return `nil` and the string "no memory".

This feature was first introduced in the `v0.7.18` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.replace

**syntax:** *success, err, forcible = ngx.shared.DICT:replace(key, value, exptime?, flags?)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Just like the [set](#ngxshareddictset) method, but only stores the key-value pair into the dictionary [ngx.shared.DICT](#ngxshareddict) if the key *does* exist.

If the `key` argument does *not* exist in the dictionary (or expired already), the `success` return value will be `false` and the `err` return value will be `"not found"`.

This feature was first introduced in the `v0.3.1rc22` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.delete

**syntax:** *ngx.shared.DICT:delete(key)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Unconditionally removes the key-value pair from the shm-based dictionary [ngx.shared.DICT](#ngxshareddict).

It is equivalent to `ngx.shared.DICT:set(key, nil)`.

This feature was first introduced in the `v0.3.1rc22` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.incr

**syntax:** *newval, err, forcible? = ngx.shared.DICT:incr(key, value, init?, init_ttl?)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

**optional requirement:** `resty.core.shdict` or `resty.core`

Increments the (numerical) value for `key` in the shm-based dictionary [ngx.shared.DICT](#ngxshareddict) by the step value `value`. Returns the new resulting number if the operation is successfully completed or `nil` and an error message otherwise.

When the key does not exist or has already expired in the shared dictionary,

1. if the `init` argument is not specified or takes the value `nil`, this method will return `nil` and the error string `"not found"`, or
1. if the `init` argument takes a number value, this method will create a new `key` with the value `init + value`.

Like the [add](#ngxshareddictadd) method, it also overrides the (least recently used) unexpired items in the store when running out of storage in the shared memory zone.

The optional `init_ttl` argument specifies expiration time (in seconds) of the value when it is initialized via the `init` argument. The time resolution is `0.001` seconds. If `init_ttl` takes the value `0` (which is the default), then the item will never expire. This argument cannot be provided without providing the `init` argument as well, and has no effect if the value already exists (e.g., if it was previously inserted via [set](#ngxshareddictset) or the likes).

**Note:** Usage of the `init_ttl` argument requires the `resty.core.shdict` or `resty.core` modules from the [lua-resty-core](https://github.com/openresty/lua-resty-core) library. Example:

```lua

 require "resty.core"

 local cats = ngx.shared.cats
 local newval, err = cats:incr("black_cats", 1, 0, 0.1)

 print(newval) -- 1

 ngx.sleep(0.2)

 local val, err = cats:get("black_cats")
 print(val) -- nil
```

The `forcible` return value will always be `nil` when the `init` argument is not specified.

If this method succeeds in storing the current item by forcibly removing other not-yet-expired items in the dictionary via LRU, the `forcible` return value will be `true`. If it stores the item without forcibly removing other valid items, then the return value `forcible` will be `false`.

If the original value is not a valid Lua number in the dictionary, it will return `nil` and `"not a number"`.

The `value` argument and `init` argument can be any valid Lua numbers, like negative numbers or floating-point numbers.

This method was first introduced in the `v0.3.1rc22` release.

The optional `init` parameter was first added in the `v0.10.6` release.

The optional `init_ttl` parameter was introduced in the `v0.10.12rc2` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.lpush

**syntax:** *length, err = ngx.shared.DICT:lpush(key, value)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Inserts the specified (numerical or string) `value` at the head of the list named `key` in the shm-based dictionary [ngx.shared.DICT](#ngxshareddict). Returns the number of elements in the list after the push operation.

If `key` does not exist, it is created as an empty list before performing the push operation. When the `key` already takes a value that is not a list, it will return `nil` and `"value not a list"`.

It never overrides the (least recently used) unexpired items in the store when running out of storage in the shared memory zone. In this case, it will immediately return `nil` and the string "no memory".

This feature was first introduced in the `v0.10.6` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.rpush

**syntax:** *length, err = ngx.shared.DICT:rpush(key, value)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Similar to the [lpush](#ngxshareddictlpush) method, but inserts the specified (numerical or string) `value` at the tail of the list named `key`.

This feature was first introduced in the `v0.10.6` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.lpop

**syntax:** *val, err = ngx.shared.DICT:lpop(key)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Removes and returns the first element of the list named `key` in the shm-based dictionary [ngx.shared.DICT](#ngxshareddict).

If `key` does not exist, it will return `nil`. When the `key` already takes a value that is not a list, it will return `nil` and `"value not a list"`.

This feature was first introduced in the `v0.10.6` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.rpop

**syntax:** *val, err = ngx.shared.DICT:rpop(key)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Removes and returns the last element of the list named `key` in the shm-based dictionary [ngx.shared.DICT](#ngxshareddict).

If `key` does not exist, it will return `nil`. When the `key` already takes a value that is not a list, it will return `nil` and `"value not a list"`.

This feature was first introduced in the `v0.10.6` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.llen

**syntax:** *len, err = ngx.shared.DICT:llen(key)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the number of elements in the list named `key` in the shm-based dictionary [ngx.shared.DICT](#ngxshareddict).

If key does not exist, it is interpreted as an empty list and 0 is returned. When the `key` already takes a value that is not a list, it will return `nil` and `"value not a list"`.

This feature was first introduced in the `v0.10.6` release.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.ttl

**syntax:** *ttl, err = ngx.shared.DICT:ttl(key)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

**requires:** `resty.core.shdict` or `resty.core`

Retrieves the remaining TTL (time-to-live in seconds) of a key-value pair in the shm-based dictionary [ngx.shared.DICT](#ngxshareddict). Returns the TTL as a number if the operation is successfully completed or `nil` and an error message otherwise.

If the key does not exist (or has already expired), this method will return `nil` and the error string `"not found"`.

The TTL is originally determined by the `exptime` argument of the [set](#ngxshareddictset), [add](#ngxshareddictadd), [replace](#ngxshareddictreplace) (and the likes) methods. It has a time resolution of `0.001` seconds. A value of `0` means that the item will never expire.

Example:

```lua

 require "resty.core"

 local cats = ngx.shared.cats
 local succ, err = cats:set("Marry", "a nice cat", 0.5)

 ngx.sleep(0.2)

 local ttl, err = cats:ttl("Marry")
 ngx.say(ttl) -- 0.3
```

This feature was first introduced in the `v0.10.11` release.

**Note:** This method requires the `resty.core.shdict` or `resty.core` modules from the [lua-resty-core](https://github.com/openresty/lua-resty-core) library.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.expire

**syntax:** *success, err = ngx.shared.DICT:expire(key, exptime)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

**requires:** `resty.core.shdict` or `resty.core`

Updates the `exptime` (in second) of a key-value pair in the shm-based dictionary [ngx.shared.DICT](#ngxshareddict). Returns a boolean indicating success if the operation completes or `nil` and an error message otherwise.

If the key does not exist, this method will return `nil` and the error string `"not found"`.

The `exptime` argument has a resolution of `0.001` seconds. If `exptime` is `0`, then the item will never expire.

Example:

```lua

 require "resty.core"

 local cats = ngx.shared.cats
 local succ, err = cats:set("Marry", "a nice cat", 0.1)

 succ, err = cats:expire("Marry", 0.5)

 ngx.sleep(0.2)

 local val, err = cats:get("Marry")
 ngx.say(val) -- "a nice cat"
```

This feature was first introduced in the `v0.10.11` release.

**Note:** This method requires the `resty.core.shdict` or `resty.core` modules from the [lua-resty-core](https://github.com/openresty/lua-resty-core) library.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.flush_all

**syntax:** *ngx.shared.DICT:flush_all()*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Flushes out all the items in the dictionary. This method does not actually free up all the memory blocks in the dictionary but just marks all the existing items as expired.

This feature was first introduced in the `v0.5.0rc17` release.

See also [ngx.shared.DICT.flush_expired](#ngxshareddictflush_expired) and [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.flush_expired

**syntax:** *flushed = ngx.shared.DICT:flush_expired(max_count?)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Flushes out the expired items in the dictionary, up to the maximal number specified by the optional `max_count` argument. When the `max_count` argument is given `0` or not given at all, then it means unlimited. Returns the number of items that have actually been flushed.

Unlike the [flush_all](#ngxshareddictflush_all) method, this method actually frees up the memory used by the expired items.

This feature was first introduced in the `v0.6.3` release.

See also [ngx.shared.DICT.flush_all](#ngxshareddictflush_all) and [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.get_keys

**syntax:** *keys = ngx.shared.DICT:get_keys(max_count?)*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Fetch a list of the keys from the dictionary, up to `<max_count>`.

By default, only the first 1024 keys (if any) are returned. When the `<max_count>` argument is given the value `0`, then all the keys will be returned even there is more than 1024 keys in the dictionary.

**CAUTION** Avoid calling this method on dictionaries with a very large number of keys as it may lock the dictionary for significant amount of time and block Nginx worker processes trying to access the dictionary.

This feature was first introduced in the `v0.7.3` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.capacity

**syntax:** *capacity_bytes = ngx.shared.DICT:capacity()*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

**requires:** `resty.core.shdict` or `resty.core`

Retrieves the capacity in bytes for the shm-based dictionary [ngx.shared.DICT](#ngxshareddict) declared with
the [lua_shared_dict](#lua_shared_dict) directive.

Example:

```lua

 require "resty.core.shdict"

 local cats = ngx.shared.cats
 local capacity_bytes = cats:capacity()
```

This feature was first introduced in the `v0.10.11` release.

**Note:** This method requires the `resty.core.shdict` or `resty.core` modules from the [lua-resty-core](https://github.com/openresty/lua-resty-core) library.

This feature requires at least Nginx core version `0.7.3`.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.shared.DICT.free_space

**syntax:** *free_page_bytes = ngx.shared.DICT:free_space()*

**context:** *init_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

**requires:** `resty.core.shdict` or `resty.core`

Retrieves the free page size in bytes for the shm-based dictionary [ngx.shared.DICT](#ngxshareddict).

**Note:** The memory for ngx.shared.DICT is allocated via the Nginx slab allocator which has each slot for
data size ranges like \~8, 9\~16, 17\~32, ..., 1025\~2048, 2048\~ bytes. And pages are assigned to a slot if there
is no room in already assigned pages for the slot.

So even if the return value of the `free_space` method is zero, there may be room in already assigned pages, so
you may successfully set a new key value pair to the shared dict without getting `true` for `forcible` or
non nil `err` from the `ngx.shared.DICT.set`.

On the other hand, if already assigned pages for a slot are full and a new key value pair is added to the
slot and there is no free page, you may get `true` for `forcible` or non nil `err` from the
`ngx.shared.DICT.set` method.

Example:

```lua

 require "resty.core.shdict"

 local cats = ngx.shared.cats
 local free_page_bytes = cats:free_space()
```

This feature was first introduced in the `v0.10.11` release.

**Note:** This method requires the `resty.core.shdict` or `resty.core` modules from the [lua-resty-core](https://github.com/openresty/lua-resty-core) library.

This feature requires at least Nginx core version `1.11.7`.

See also [ngx.shared.DICT](#ngxshareddict).

[Back to TOC](#nginx-api-for-lua)

## ngx.socket.udp

**syntax:** *udpsock = ngx.socket.udp()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Creates and returns a UDP or datagram-oriented unix domain socket object (also known as one type of the "cosocket" objects). The following methods are supported on this object:

* [setpeername](#udpsocksetpeername)
* [send](#udpsocksend)
* [receive](#udpsockreceive)
* [close](#udpsockclose)
* [settimeout](#udpsocksettimeout)

It is intended to be compatible with the UDP API of the [LuaSocket](http://w3.impa.br/~diego/software/luasocket/udp.html) library but is 100% nonblocking out of the box.

This feature was first introduced in the `v0.5.7` release.

See also [ngx.socket.tcp](#ngxsockettcp).

[Back to TOC](#nginx-api-for-lua)

## udpsock:setpeername

**syntax:** *ok, err = udpsock:setpeername(host, port)*

**syntax:** *ok, err = udpsock:setpeername("unix:/path/to/unix-domain.socket")*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Attempts to connect a UDP socket object to a remote server or to a datagram unix domain socket file. Because the datagram protocol is actually connection-less, this method does not really establish a "connection", but only just set the name of the remote peer for subsequent read/write operations.

Both IP addresses and domain names can be specified as the `host` argument. In case of domain names, this method will use Nginx core's dynamic resolver to parse the domain name without blocking and it is required to configure the [resolver](http://nginx.org/en/docs/http/ngx_http_core_module.html#resolver) directive in the `nginx.conf` file like this:

```nginx

 resolver 8.8.8.8;  # use Google's public DNS nameserver
```

If the nameserver returns multiple IP addresses for the host name, this method will pick up one randomly.

In case of error, the method returns `nil` followed by a string describing the error. In case of success, the method returns `1`.

Here is an example for connecting to a UDP (memcached) server:

```nginx

 location /test {
     resolver 8.8.8.8;

     content_by_lua_block {
         local sock = ngx.socket.udp()
         local ok, err = sock:setpeername("my.memcached.server.domain", 11211)
         if not ok then
             ngx.say("failed to connect to memcached: ", err)
             return
         end
         ngx.say("successfully connected to memcached!")
         sock:close()
     }
 }
```

Since the `v0.7.18` release, connecting to a datagram unix domain socket file is also possible on Linux:

```lua

 local sock = ngx.socket.udp()
 local ok, err = sock:setpeername("unix:/tmp/some-datagram-service.sock")
 if not ok then
     ngx.say("failed to connect to the datagram unix domain socket: ", err)
     return
 end

 -- do something after connect
 -- such as sock:send or sock:receive
```

assuming the datagram service is listening on the unix domain socket file `/tmp/some-datagram-service.sock` and the client socket will use the "autobind" feature on Linux.

Calling this method on an already connected socket object will cause the original connection to be closed first.

This method was first introduced in the `v0.5.7` release.

[Back to TOC](#nginx-api-for-lua)

## udpsock:send

**syntax:** *ok, err = udpsock:send(data)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Sends data on the current UDP or datagram unix domain socket object.

In case of success, it returns `1`. Otherwise, it returns `nil` and a string describing the error.

The input argument `data` can either be a Lua string or a (nested) Lua table holding string fragments. In case of table arguments, this method will copy all the string elements piece by piece to the underlying Nginx socket send buffers, which is usually optimal than doing string concatenation operations on the Lua land.

This feature was first introduced in the `v0.5.7` release.

[Back to TOC](#nginx-api-for-lua)

## udpsock:receive

**syntax:** *data, err = udpsock:receive(size?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Receives data from the UDP or datagram unix domain socket object with an optional receive buffer size argument, `size`.

This method is a synchronous operation and is 100% nonblocking.

In case of success, it returns the data received; in case of error, it returns `nil` with a string describing the error.

If the `size` argument is specified, then this method will use this size as the receive buffer size. But when this size is greater than `8192`, then `8192` will be used instead.

If no argument is specified, then the maximal buffer size, `8192` is assumed.

Timeout for the reading operation is controlled by the [lua_socket_read_timeout](#lua_socket_read_timeout) config directive and the [settimeout](#udpsocksettimeout) method. And the latter takes priority. For example:

```lua

 sock:settimeout(1000)  -- one second timeout
 local data, err = sock:receive()
 if not data then
     ngx.say("failed to read a packet: ", err)
     return
 end
 ngx.say("successfully read a packet: ", data)
```

It is important here to call the [settimeout](#udpsocksettimeout) method *before* calling this method.

This feature was first introduced in the `v0.5.7` release.

[Back to TOC](#nginx-api-for-lua)

## udpsock:close

**syntax:** *ok, err = udpsock:close()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Closes the current UDP or datagram unix domain socket. It returns the `1` in case of success and returns `nil` with a string describing the error otherwise.

Socket objects that have not invoked this method (and associated connections) will be closed when the socket object is released by the Lua GC (Garbage Collector) or the current client HTTP request finishes processing.

This feature was first introduced in the `v0.5.7` release.

[Back to TOC](#nginx-api-for-lua)

## udpsock:settimeout

**syntax:** *udpsock:settimeout(time)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Set the timeout value in milliseconds for subsequent socket operations (like [receive](#udpsockreceive)).

Settings done by this method takes priority over those config directives, like [lua_socket_read_timeout](#lua_socket_read_timeout).

This feature was first introduced in the `v0.5.7` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.socket.stream

Just an alias to [ngx.socket.tcp](#ngxsockettcp). If the stream-typed cosocket may also connect to a unix domain
socket, then this API name is preferred.

This API function was first added to the `v0.10.1` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.socket.tcp

**syntax:** *tcpsock = ngx.socket.tcp()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Creates and returns a TCP or stream-oriented unix domain socket object (also known as one type of the "cosocket" objects). The following methods are supported on this object:

* [bind](#tcpsockbind)
* [connect](#tcpsockconnect)
* [setclientcert](#tcpsocksetclientcert)
* [sslhandshake](#tcpsocksslhandshake)
* [send](#tcpsocksend)
* [receive](#tcpsockreceive)
* [close](#tcpsockclose)
* [settimeout](#tcpsocksettimeout)
* [settimeouts](#tcpsocksettimeouts)
* [setoption](#tcpsocksetoption)
* [receiveany](#tcpsockreceiveany)
* [receiveuntil](#tcpsockreceiveuntil)
* [setkeepalive](#tcpsocksetkeepalive)
* [getreusedtimes](#tcpsockgetreusedtimes)

It is intended to be compatible with the TCP API of the [LuaSocket](http://w3.impa.br/~diego/software/luasocket/tcp.html) library but is 100% nonblocking out of the box. Also, we introduce some new APIs to provide more functionalities.

The cosocket object created by this API function has exactly the same lifetime as the Lua handler creating it. So never pass the cosocket object to any other Lua handler (including ngx.timer callback functions) and never share the cosocket object between different Nginx requests.

For every cosocket object's underlying connection, if you do not
explicitly close it (via [close](#tcpsockclose)) or put it back to the connection
pool (via [setkeepalive](#tcpsocksetkeepalive)), then it is automatically closed when one of
the following two events happens:

* the current request handler completes, or
* the Lua cosocket object value gets collected by the Lua GC.

Fatal errors in cosocket operations always automatically close the current
connection (note that, read timeout error is the only error that is
not fatal), and if you call [close](#tcpsockclose) on a closed connection, you will get
the "closed" error.

Starting from the `0.9.9` release, the cosocket object here is full-duplex, that is, a reader "light thread" and a writer "light thread" can operate on a single cosocket object simultaneously (both "light threads" must belong to the same Lua handler though, see reasons above). But you cannot have two "light threads" both reading (or writing or connecting) the same cosocket, otherwise you might get an error like "socket busy reading" when calling the methods of the cosocket object.

This feature was first introduced in the `v0.5.0rc1` release.

See also [ngx.socket.udp](#ngxsocketudp).

[Back to TOC](#nginx-api-for-lua)

## tcpsock:bind
**syntax:** *ok, err = tcpsock:bind(address)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;,ssl_session_fetch_by_lua&#42;,ssl_client_hello_by_lua&#42;*

Just like the standard [proxy_bind](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_bind) directive, this api makes the outgoing connection to a upstream server originate from the specified local IP address.

Only IP addresses can be specified as the `address` argument.

Here is an example for connecting to a TCP server from the specified local IP address:

```nginx

 location /test {
     content_by_lua_block {
         local sock = ngx.socket.tcp()
         -- assume "192.168.1.10" is the local ip address
         local ok, err = sock:bind("192.168.1.10")
         if not ok then
             ngx.say("failed to bind")
             return
         end
         local ok, err = sock:connect("192.168.1.67", 80)
         if not ok then
             ngx.say("failed to connect server: ", err)
             return
         end
         ngx.say("successfully connected!")
         sock:close()
     }
 }
```

[Back to TOC](#nginx-api-for-lua)

## tcpsock:connect

**syntax:** *ok, err = tcpsock:connect(host, port, options_table?)*

**syntax:** *ok, err = tcpsock:connect("unix:/path/to/unix-domain.socket", options_table?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Attempts to connect a TCP socket object to a remote server or to a stream unix domain socket file without blocking.

Before actually resolving the host name and connecting to the remote backend, this method will always look up the connection pool for matched idle connections created by previous calls of this method (or the [ngx.socket.connect](#ngxsocketconnect) function).

Both IP addresses and domain names can be specified as the `host` argument. In case of domain names, this method will use Nginx core's dynamic resolver to parse the domain name without blocking and it is required to configure the [resolver](http://nginx.org/en/docs/http/ngx_http_core_module.html#resolver) directive in the `nginx.conf` file like this:

```nginx

 resolver 8.8.8.8;  # use Google's public DNS nameserver
```

If the nameserver returns multiple IP addresses for the host name, this method will pick up one randomly.

In case of error, the method returns `nil` followed by a string describing the error. In case of success, the method returns `1`.

Here is an example for connecting to a TCP server:

```nginx

 location /test {
     resolver 8.8.8.8;

     content_by_lua_block {
         local sock = ngx.socket.tcp()
         local ok, err = sock:connect("www.google.com", 80)
         if not ok then
             ngx.say("failed to connect to google: ", err)
             return
         end
         ngx.say("successfully connected to google!")
         sock:close()
     }
 }
```

Connecting to a Unix Domain Socket file is also possible:

```lua

 local sock = ngx.socket.tcp()
 local ok, err = sock:connect("unix:/tmp/memcached.sock")
 if not ok then
     ngx.say("failed to connect to the memcached unix domain socket: ", err)
     return
 end

 -- do something after connect
 -- such as sock:send or sock:receive
```

assuming memcached (or something else) is listening on the unix domain socket file `/tmp/memcached.sock`.

Timeout for the connecting operation is controlled by the [lua_socket_connect_timeout](#lua_socket_connect_timeout) config directive and the [settimeout](#tcpsocksettimeout) method. And the latter takes priority. For example:

```lua

 local sock = ngx.socket.tcp()
 sock:settimeout(1000)  -- one second timeout
 local ok, err = sock:connect(host, port)
```

It is important here to call the [settimeout](#tcpsocksettimeout) method *before* calling this method.

Calling this method on an already connected socket object will cause the original connection to be closed first.

An optional Lua table can be specified as the last argument to this method to specify various connect options:

* `pool`
	specify a custom name for the connection pool being used. If omitted, then the connection pool name will be generated from the string template `"<host>:<port>"` or `"<unix-socket-path>"`.

* `pool_size`
	specify the size of the connection pool. If omitted and no
	`backlog` option was provided, no pool will be created. If omitted
	but `backlog` was provided, the pool will be created with a default
	size equal to the value of the [lua_socket_pool_size](#lua_socket_pool_size)
	directive.
	The connection pool holds up to `pool_size` alive connections
	ready to be reused by subsequent calls to [connect](#tcpsockconnect), but
	note that there is no upper limit to the total number of opened connections
	outside of the pool. If you need to restrict the total number of opened
	connections, specify the `backlog` option.
	When the connection pool would exceed its size limit, the least recently used
	(kept-alive) connection already in the pool will be closed to make room for
	the current connection.
	Note that the cosocket connection pool is per Nginx worker process rather
	than per Nginx server instance, so the size limit specified here also applies
	to every single Nginx worker process. Also note that the size of the connection
	pool cannot be changed once it has been created.
	This option was first introduced in the `v0.10.14` release.

* `backlog`
	if specified, this module will limit the total number of opened connections
	for this pool. No more connections than `pool_size` can be opened
	for this pool at any time. If the connection pool is full, subsequent
	connect operations will be queued into a queue equal to this option's
	value (the "backlog" queue).
	If the number of queued connect operations is equal to `backlog`,
	subsequent connect operations will fail and return `nil` plus the
	error string `"too many waiting connect operations"`.
	The queued connect operations will be resumed once the number of connections
	in the pool is less than `pool_size`.
	The queued connect operation will abort once they have been queued for more
	than `connect_timeout`, controlled by
	[settimeouts](#tcpsocksettimeouts), and will return `nil` plus
	the error string `"timeout"`.
	This option was first introduced in the `v0.10.14` release.

The support for the options table argument was first introduced in the `v0.5.7` release.

This method was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:setclientcert

**syntax:** *ok, err = tcpsock:setclientcert(cert, pkey)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Set client certificate chain and corresponding private key to the TCP socket object.
The certificate chain and private key provided will be used later by the [tcpsock:sslhandshake](#tcpsocksslhandshake) method.

* `cert` specify a client certificate chain cdata object that will be used while handshaking with
remote server. These objects can be created using [ngx.ssl.parse\_pem\_cert](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl.md#parse_pem_cert)
function provided by lua-resty-core. Note that specifying the `cert` option requires
corresponding `pkey` be provided too. See below.
* `pkey` specify a private key corresponds to the `cert` option above.
These objects can be created using [ngx.ssl.parse\_pem\_priv\_key](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl.md#parse_pem_priv_key)
function provided by lua-resty-core.

If both of `cert` and `pkey` are `nil`, this method will clear any existing client certificate and private key
that was previously set on the cosocket object.

This method was first introduced in the `v0.10.22` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:sslhandshake

**syntax:** *session, err = tcpsock:sslhandshake(reused_session?, server_name?, ssl_verify?, send_status_req?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Does SSL/TLS handshake on the currently established connection.

The optional `reused_session` argument can take a former SSL
session userdata returned by a previous `sslhandshake`
call for exactly the same target. For short-lived connections, reusing SSL
sessions can usually speed up the handshake by one order by magnitude but it
is not so useful if the connection pool is enabled. This argument defaults to
`nil`. If this argument takes the boolean `false` value, no SSL session
userdata would return by this call and only a Lua boolean will be returned as
the first return value; otherwise the current SSL session will
always be returned as the first argument in case of successes.

The optional `server_name` argument is used to specify the server
name for the new TLS extension Server Name Indication (SNI). Use of SNI can
make different servers share the same IP address on the server side. Also,
when SSL verification is enabled, this `server_name` argument is
also used to validate the server name specified in the server certificate sent from
the remote.

The optional `ssl_verify` argument takes a Lua boolean value to
control whether to perform SSL verification. When set to `true`, the server
certificate will be verified according to the CA certificates specified by
the [lua_ssl_trusted_certificate](#lua_ssl_trusted_certificate) directive.
You may also need to adjust the [lua_ssl_verify_depth](#lua_ssl_verify_depth)
directive to control how deep we should follow along the certificate chain.
Also, when the `ssl_verify` argument is true and the
`server_name` argument is also specified, the latter will be used
to validate the server name in the server certificate.

The optional `send_status_req` argument takes a boolean that controls whether to send
the OCSP status request in the SSL handshake request (which is for requesting OCSP stapling).

For connections that have already done SSL/TLS handshake, this method returns
immediately.

This method was first introduced in the `v0.9.11` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:send

**syntax:** *bytes, err = tcpsock:send(data)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Sends data without blocking on the current TCP or Unix Domain Socket connection.

This method is a synchronous operation that will not return until *all* the data has been flushed into the system socket send buffer or an error occurs.

In case of success, it returns the total number of bytes that have been sent. Otherwise, it returns `nil` and a string describing the error.

The input argument `data` can either be a Lua string or a (nested) Lua table holding string fragments. In case of table arguments, this method will copy all the string elements piece by piece to the underlying Nginx socket send buffers, which is usually optimal than doing string concatenation operations on the Lua land.

Timeout for the sending operation is controlled by the [lua_socket_send_timeout](#lua_socket_send_timeout) config directive and the [settimeout](#tcpsocksettimeout) method. And the latter takes priority. For example:

```lua

 sock:settimeout(1000)  -- one second timeout
 local bytes, err = sock:send(request)
```

It is important here to call the [settimeout](#tcpsocksettimeout) method *before* calling this method.

In case of any connection errors, this method always automatically closes the current connection.

This feature was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:receive

**syntax:** *data, err, partial = tcpsock:receive(size)*

**syntax:** *data, err, partial = tcpsock:receive(pattern?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Receives data from the connected socket according to the reading pattern or size.

This method is a synchronous operation just like the [send](#tcpsocksend) method and is 100% nonblocking.

In case of success, it returns the data received; in case of error, it returns `nil` with a string describing the error and the partial data received so far.

If a number-like argument is specified (including strings that look like numbers), then it is interpreted as a size. This method will not return until it reads exactly this size of data or an error occurs.

If a non-number-like string argument is specified, then it is interpreted as a "pattern". The following patterns are supported:

* `'*a'`: reads from the socket until the connection is closed. No end-of-line translation is performed;
* `'*l'`: reads a line of text from the socket. The line is terminated by a `Line Feed` (LF) character (ASCII 10), optionally preceded by a `Carriage Return` (CR) character (ASCII 13). The CR and LF characters are not included in the returned line. In fact, all CR characters are ignored by the pattern.

If no argument is specified, then it is assumed to be the pattern `'*l'`, that is, the line reading pattern.

Timeout for the reading operation is controlled by the [lua_socket_read_timeout](#lua_socket_read_timeout) config directive and the [settimeout](#tcpsocksettimeout) method. And the latter takes priority. For example:

```lua

 sock:settimeout(1000)  -- one second timeout
 local line, err, partial = sock:receive()
 if not line then
     ngx.say("failed to read a line: ", err)
     return
 end
 ngx.say("successfully read a line: ", line)
```

It is important here to call the [settimeout](#tcpsocksettimeout) method *before* calling this method.

Since the `v0.8.8` release, this method no longer automatically closes the current connection when the read timeout error happens. For other connection errors, this method always automatically closes the connection.

This feature was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:receiveany

**syntax:** *data, err = tcpsock:receiveany(max)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns any data received by the connected socket, at most `max` bytes.

This method is a synchronous operation just like the [send](#tcpsocksend) method and is 100% nonblocking.

In case of success, it returns the data received; in case of error, it returns `nil` with a string describing the error.

If the received data is more than this size, this method will return with exactly this size of data.
The remaining data in the underlying receive buffer could be returned in the next reading operation.

Timeout for the reading operation is controlled by the [lua_socket_read_timeout](#lua_socket_read_timeout) config directive and the [settimeouts](#tcpsocksettimeouts) method. And the latter takes priority. For example:

```lua

 sock:settimeouts(1000, 1000, 1000)  -- one second timeout for connect/read/write
 local data, err = sock:receiveany(10 * 1024) -- read any data, at most 10K
 if not data then
     ngx.say("failed to read any data: ", err)
     return
 end
 ngx.say("successfully read: ", data)
```

This method doesn't automatically close the current connection when the read timeout error occurs. For other connection errors, this method always automatically closes the connection.

This feature was first introduced in the `v0.10.14` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:receiveuntil

**syntax:** *iterator = tcpsock:receiveuntil(pattern, options?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

This method returns an iterator Lua function that can be called to read the data stream until it sees the specified pattern or an error occurs.

Here is an example for using this method to read a data stream with the boundary sequence `--abcedhb`:

```lua

 local reader = sock:receiveuntil("\r\n--abcedhb")
 local data, err, partial = reader()
 if not data then
     ngx.say("failed to read the data stream: ", err)
 end
 ngx.say("read the data stream: ", data)
```

When called without any argument, the iterator function returns the received data right *before* the specified pattern string in the incoming data stream. So for the example above, if the incoming data stream is `'hello, world! -agentzh\r\n--abcedhb blah blah'`, then the string `'hello, world! -agentzh'` will be returned.

In case of error, the iterator function will return `nil` along with a string describing the error and the partial data bytes that have been read so far.

The iterator function can be called multiple times and can be mixed safely with other cosocket method calls or other iterator function calls.

The iterator function behaves differently (i.e., like a real iterator) when it is called with a `size` argument. That is, it will read that `size` of data on each invocation and will return `nil` at the last invocation (either sees the boundary pattern or meets an error). For the last successful invocation of the iterator function, the `err` return value will be `nil` too. The iterator function will be reset after the last successful invocation that returns `nil` data and `nil` error. Consider the following example:

```lua

 local reader = sock:receiveuntil("\r\n--abcedhb")

 while true do
     local data, err, partial = reader(4)
     if not data then
         if err then
             ngx.say("failed to read the data stream: ", err)
             break
         end

         ngx.say("read done")
         break
     end
     ngx.say("read chunk: [", data, "]")
 end
```

Then for the incoming data stream `'hello, world! -agentzh\r\n--abcedhb blah blah'`, we shall get the following output from the sample code above:


    read chunk: [hell]
    read chunk: [o, w]
    read chunk: [orld]
    read chunk: [! -a]
    read chunk: [gent]
    read chunk: [zh]
    read done


Note that, the actual data returned *might* be a little longer than the size limit specified by the `size` argument when the boundary pattern has ambiguity for streaming parsing. Near the boundary of the data stream, the data string actually returned could also be shorter than the size limit.

Timeout for the iterator function's reading operation is controlled by the [lua_socket_read_timeout](#lua_socket_read_timeout) config directive and the [settimeout](#tcpsocksettimeout) method. And the latter takes priority. For example:

```lua

 local readline = sock:receiveuntil("\r\n")

 sock:settimeout(1000)  -- one second timeout
 line, err, partial = readline()
 if not line then
     ngx.say("failed to read a line: ", err)
     return
 end
 ngx.say("successfully read a line: ", line)
```

It is important here to call the [settimeout](#tcpsocksettimeout) method *before* calling the iterator function (note that the `receiveuntil` call is irrelevant here).

As from the `v0.5.1` release, this method also takes an optional `options` table argument to control the behavior. The following options are supported:

* `inclusive`

The `inclusive` takes a boolean value to control whether to include the pattern string in the returned data string. Default to `false`. For example,

```lua

 local reader = tcpsock:receiveuntil("_END_", { inclusive = true })
 local data = reader()
 ngx.say(data)
```

Then for the input data stream `"hello world _END_ blah blah blah"`, then the example above will output `hello world _END_`, including the pattern string `_END_` itself.

Since the `v0.8.8` release, this method no longer automatically closes the current connection when the read timeout error happens. For other connection errors, this method always automatically closes the connection.

This method was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:close

**syntax:** *ok, err = tcpsock:close()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Closes the current TCP or stream unix domain socket. It returns the `1` in case of success and returns `nil` with a string describing the error otherwise.

Note that there is no need to call this method on socket objects that have invoked the [setkeepalive](#tcpsocksetkeepalive) method because the socket object is already closed (and the current connection is saved into the built-in connection pool).

Socket objects that have not invoked this method (and associated connections) will be closed when the socket object is released by the Lua GC (Garbage Collector) or the current client HTTP request finishes processing.

This feature was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:settimeout

**syntax:** *tcpsock:settimeout(time)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Set the timeout value in milliseconds for subsequent socket operations ([connect](#tcpsockconnect), [receive](#tcpsockreceive), and iterators returned from [receiveuntil](#tcpsockreceiveuntil)).

Settings done by this method take priority over those specified via config directives (i.e. [lua_socket_connect_timeout](#lua_socket_connect_timeout), [lua_socket_send_timeout](#lua_socket_send_timeout), and [lua_socket_read_timeout](#lua_socket_read_timeout)).

Note that this method does *not* affect the [lua_socket_keepalive_timeout](#lua_socket_keepalive_timeout) setting; the `timeout` argument to the [setkeepalive](#tcpsocksetkeepalive) method should be used for this purpose instead.

This feature was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:settimeouts

**syntax:** *tcpsock:settimeouts(connect_timeout, send_timeout, read_timeout)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Respectively sets the connect, send, and read timeout thresholds (in milliseconds) for subsequent socket
operations ([connect](#tcpsockconnect), [send](#tcpsocksend), [receive](#tcpsockreceive), and iterators returned from [receiveuntil](#tcpsockreceiveuntil)).

Settings done by this method take priority over those specified via config directives (i.e. [lua_socket_connect_timeout](#lua_socket_connect_timeout), [lua_socket_send_timeout](#lua_socket_send_timeout), and [lua_socket_read_timeout](#lua_socket_read_timeout)).

It is recommended to use [settimeouts](#tcpsocksettimeouts) instead of [settimeout](#tcpsocksettimeout).

Note that this method does *not* affect the [lua_socket_keepalive_timeout](#lua_socket_keepalive_timeout) setting; the `timeout` argument to the [setkeepalive](#tcpsocksetkeepalive) method should be used for this purpose instead.

This feature was first introduced in the `v0.10.7` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:setoption

**syntax:** *ok, err = tcpsock:setoption(option, value?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

This function is added for [LuaSocket](http://w3.impa.br/~diego/software/luasocket/tcp.html) API compatibility, its functionality is implemented `v0.10.18`.

This feature was first introduced in the `v0.5.0rc1` release.

In case of success, it returns `true`. Otherwise, it returns nil and a string describing the error.

The `option` is a string with the option name, and the value depends on the option being set:

* `keepalive`

	Setting this option to true enables sending of keep-alive messages on
	connection-oriented sockets. Make sure the `connect` function
	had been called before, for example,

    ```lua

    local ok, err = tcpsock:setoption("keepalive", true)
    if not ok then
        ngx.say("setoption keepalive failed: ", err)
    end
    ```
* `reuseaddr`

	Enabling this option indicates that the rules used in validating addresses
	supplied in a call to bind should allow reuse of local addresses. Make sure
	the `connect` function had been called before, for example,

    ```lua

    local ok, err = tcpsock:setoption("reuseaddr", 0)
    if not ok then
        ngx.say("setoption reuseaddr failed: ", err)
    end
    ```
* `tcp-nodelay`

	Setting this option to true disables the Nagle's algorithm for the connection.
	Make sure the `connect` function had been called before, for example,

    ```lua

    local ok, err = tcpsock:setoption("tcp-nodelay", true)
    if not ok then
        ngx.say("setoption tcp-nodelay failed: ", err)
    end
    ```
* `sndbuf`

	Sets the maximum socket send buffer in bytes. The kernel doubles this value
	(to allow space for bookkeeping overhead) when it is set using setsockopt().
	Make sure the `connect` function had been called before, for example,

    ```lua

    local ok, err = tcpsock:setoption("sndbuf", 1024 * 10)
    if not ok then
        ngx.say("setoption sndbuf failed: ", err)
    end
    ```
* `rcvbuf`

	Sets the maximum socket receive buffer in bytes. The kernel doubles this value
	(to allow space for bookkeeping overhead) when it is set using setsockopt. Make
	sure the `connect` function had been called before, for example,

    ```lua

    local ok, err = tcpsock:setoption("rcvbuf", 1024 * 10)
    if not ok then
        ngx.say("setoption rcvbuf failed: ", err)
    end
    ```

NOTE: Once the option is set, it will become effective until the connection is closed. If you know the connection is from the connection pool and all the in-pool connections already have called the setoption() method with the desired socket option state, then you can just skip calling setoption() again to avoid the overhead of repeated calls, for example,

```lua

 local count, err = tcpsock:getreusedtimes()
 if not count then
     ngx.say("getreusedtimes failed: ", err)
     return
 end

 if count == 0 then
     local ok, err = tcpsock:setoption("rcvbuf", 1024 * 10)
     if not ok then
         ngx.say("setoption rcvbuf failed: ", err)
         return
     end
 end
```

These options described above are supported in `v0.10.18`, and more options will be implemented in future.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:setkeepalive

**syntax:** *ok, err = tcpsock:setkeepalive(timeout?, size?)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Puts the current socket's connection immediately into the cosocket built-in connection pool and keep it alive until other [connect](#tcpsockconnect) method calls request it or the associated maximal idle timeout is expired.

The first optional argument, `timeout`, can be used to specify the maximal idle timeout (in milliseconds) for the current connection. If omitted, the default setting in the [lua_socket_keepalive_timeout](#lua_socket_keepalive_timeout) config directive will be used. If the `0` value is given, then the timeout interval is unlimited.

The second optional argument `size` is considered deprecated since
the `v0.10.14` release of this module, in favor of the
`pool_size` option of the [connect](#tcpsockconnect) method.
Since the `v0.10.14` release, this option will only take effect if
the call to [connect](#tcpsockconnect) did not already create a connection
pool.
When this option takes effect (no connection pool was previously created by
[connect](#tcpsockconnect)), it will specify the size of the connection pool,
and create it.
If omitted (and no pool was previously created), the default size is the value
of the [lua_socket_pool_size](#lua_socket_pool_size) directive.
The connection pool holds up to `size` alive connections ready to be
reused by subsequent calls to [connect](#tcpsockconnect), but note that there
is no upper limit to the total number of opened connections outside of the
pool.
When the connection pool would exceed its size limit, the least recently used
(kept-alive) connection already in the pool will be closed to make room for
the current connection.
Note that the cosocket connection pool is per Nginx worker process rather
than per Nginx server instance, so the size limit specified here also applies
to every single Nginx worker process. Also note that the size of the connection
pool cannot be changed once it has been created.
If you need to restrict the total number of opened connections, specify both
the `pool_size` and `backlog` option in the call to
[connect](#tcpsockconnect).

In case of success, this method returns `1`; otherwise, it returns `nil` and a string describing the error.

When the system receive buffer for the current connection has unread data, then this method will return the "connection in dubious state" error message (as the second return value) because the previous session has unread data left behind for the next session and the connection is not safe to be reused.

This method also makes the current cosocket object enter the "closed" state, so there is no need to manually call the [close](#tcpsockclose) method on it afterwards.

This feature was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## tcpsock:getreusedtimes

**syntax:** *count, err = tcpsock:getreusedtimes()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

This method returns the (successfully) reused times for the current connection. In case of error, it returns `nil` and a string describing the error.

If the current connection does not come from the built-in connection pool, then this method always returns `0`, that is, the connection has never been reused (yet). If the connection comes from the connection pool, then the return value is always non-zero. So this method can also be used to determine if the current connection comes from the pool.

This feature was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.socket.connect

**syntax:** *tcpsock, err = ngx.socket.connect(host, port)*

**syntax:** *tcpsock, err = ngx.socket.connect("unix:/path/to/unix-domain.socket")*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;*

This function is a shortcut for combining [ngx.socket.tcp()](#ngxsockettcp) and the [connect()](#tcpsockconnect) method call in a single operation. It is actually implemented like this:

```lua

 local sock = ngx.socket.tcp()
 local ok, err = sock:connect(...)
 if not ok then
     return nil, err
 end
 return sock
```

There is no way to use the [settimeout](#tcpsocksettimeout) method to specify connecting timeout for this method and the [lua_socket_connect_timeout](#lua_socket_connect_timeout) directive must be set at configure time instead.

This feature was first introduced in the `v0.5.0rc1` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.get_phase

**syntax:** *str = ngx.get_phase()*

**context:** *init_by_lua&#42;, init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Retrieves the current running phase name. Possible return values are

* `init`
	for the context of [init_by_lua*](#init_by_lua).
* `init_worker`
	for the context of [init_worker_by_lua*](#init_worker_by_lua).
* `ssl_cert`
	for the context of [ssl_certificate_by_lua*](#ssl_certificate_by_lua_block).
* `ssl_session_fetch`
	for the context of [ssl_session_fetch_by_lua*](#ssl_session_fetch_by_lua_block).
* `ssl_session_store`
	for the context of [ssl_session_store_by_lua*](#ssl_session_store_by_lua_block).
* `ssl_client_hello`
	for the context of [ssl_client_hello_by_lua*](#ssl_client_hello_by_lua_block).
* `set`
	for the context of [set_by_lua*](#set_by_lua).
* `rewrite`
	for the context of [rewrite_by_lua*](#rewrite_by_lua).
* `balancer`
	for the context of [balancer_by_lua*](#balancer_by_lua_block).
* `access`
	for the context of [access_by_lua*](#access_by_lua).
* `content`
	for the context of [content_by_lua*](#content_by_lua).
* `header_filter`
	for the context of [header_filter_by_lua*](#header_filter_by_lua).
* `body_filter`
	for the context of [body_filter_by_lua*](#body_filter_by_lua).
* `log`
	for the context of [log_by_lua*](#log_by_lua).
* `timer`
	for the context of user callback functions for [ngx.timer.*](#ngxtimerat).
* `exit_worker`
	for the context of [exit_worker_by_lua*](#exit_worker_by_lua).

This API was first introduced in the `v0.5.10` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.thread.spawn

**syntax:** *co = ngx.thread.spawn(func, arg1, arg2, ...)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Spawns a new user "light thread" with the Lua function `func` as well as those optional arguments `arg1`, `arg2`, and etc. Returns a Lua thread (or Lua coroutine) object represents this "light thread".

"Light threads" are just a special kind of Lua coroutines that are scheduled by the ngx_lua module.

Before `ngx.thread.spawn` returns, the `func` will be called with those optional arguments until it returns, aborts with an error, or gets yielded due to I/O operations via the [Nginx API for Lua](#nginx-api-for-lua) (like [tcpsock:receive](#tcpsockreceive)).

After `ngx.thread.spawn` returns, the newly-created "light thread" will keep running asynchronously usually at various I/O events.

All the Lua code chunks running by [rewrite_by_lua](#rewrite_by_lua), [access_by_lua](#access_by_lua), and [content_by_lua](#content_by_lua) are in a boilerplate "light thread" created automatically by ngx_lua. Such boilerplate "light thread" are also called "entry threads".

By default, the corresponding Nginx handler (e.g., [rewrite_by_lua](#rewrite_by_lua) handler) will not terminate until

1. both the "entry thread" and all the user "light threads" terminates,
1. a "light thread" (either the "entry thread" or a user "light thread") aborts by calling [ngx.exit](#ngxexit), [ngx.exec](#ngxexec), [ngx.redirect](#ngxredirect), or [ngx.req.set_uri(uri, true)](#ngxreqset_uri), or
1. the "entry thread" terminates with a Lua error.

When the user "light thread" terminates with a Lua error, however, it will not abort other running "light threads" like the "entry thread" does.

Due to the limitation in the Nginx subrequest model, it is not allowed to abort a running Nginx subrequest in general. So it is also prohibited to abort a running "light thread" that is pending on one ore more Nginx subrequests. You must call [ngx.thread.wait](#ngxthreadwait) to wait for those "light thread" to terminate before quitting the "world". A notable exception here is that you can abort pending subrequests by calling [ngx.exit](#ngxexit) with and only with the status code `ngx.ERROR` (-1), `408`, `444`, or `499`.

The "light threads" are not scheduled in a pre-emptive way. In other words, no time-slicing is performed automatically. A "light thread" will keep running exclusively on the CPU until

1. a (nonblocking) I/O operation cannot be completed in a single run,
1. it calls [coroutine.yield](#coroutineyield) to actively give up execution, or
1. it is aborted by a Lua error or an invocation of [ngx.exit](#ngxexit), [ngx.exec](#ngxexec), [ngx.redirect](#ngxredirect), or [ngx.req.set_uri(uri, true)](#ngxreqset_uri).

For the first two cases, the "light thread" will usually be resumed later by the ngx_lua scheduler unless a "stop-the-world" event happens.

User "light threads" can create "light threads" themselves. And normal user coroutines created by [coroutine.create](#coroutinecreate) can also create "light threads". The coroutine (be it a normal Lua coroutine or a "light thread") that directly spawns the "light thread" is called the "parent coroutine" for the "light thread" newly spawned.

The "parent coroutine" can call [ngx.thread.wait](#ngxthreadwait) to wait on the termination of its child "light thread".

You can call coroutine.status() and coroutine.yield() on the "light thread" coroutines.

The status of the "light thread" coroutine can be "zombie" if

1. the current "light thread" already terminates (either successfully or with an error),
1. its parent coroutine is still alive, and
1. its parent coroutine is not waiting on it with [ngx.thread.wait](#ngxthreadwait).

The following example demonstrates the use of coroutine.yield() in the "light thread" coroutines
to do manual time-slicing:

```lua

 local yield = coroutine.yield

 function f()
     local self = coroutine.running()
     ngx.say("f 1")
     yield(self)
     ngx.say("f 2")
     yield(self)
     ngx.say("f 3")
 end

 local self = coroutine.running()
 ngx.say("0")
 yield(self)

 ngx.say("1")
 ngx.thread.spawn(f)

 ngx.say("2")
 yield(self)

 ngx.say("3")
 yield(self)

 ngx.say("4")
```

Then it will generate the output


    0
    1
    f 1
    2
    f 2
    3
    f 3
    4


"Light threads" are mostly useful for making concurrent upstream requests in a single Nginx request handler, much like a generalized version of [ngx.location.capture_multi](#ngxlocationcapture_multi) that can work with all the [Nginx API for Lua](#nginx-api-for-lua). The following example demonstrates parallel requests to MySQL, Memcached, and upstream HTTP services in a single Lua handler, and outputting the results in the order that they actually return (similar to Facebook's BigPipe model):

```lua

 -- query mysql, memcached, and a remote http service at the same time,
 -- output the results in the order that they
 -- actually return the results.

 local mysql = require "resty.mysql"
 local memcached = require "resty.memcached"

 local function query_mysql()
     local db = mysql:new()
     db:connect{
                 host = "127.0.0.1",
                 port = 3306,
                 database = "test",
                 user = "monty",
                 password = "mypass"
               }
     local res, err, errno, sqlstate =
             db:query("select * from cats order by id asc")
     db:set_keepalive(0, 100)
     ngx.say("mysql done: ", cjson.encode(res))
 end

 local function query_memcached()
     local memc = memcached:new()
     memc:connect("127.0.0.1", 11211)
     local res, err = memc:get("some_key")
     ngx.say("memcached done: ", res)
 end

 local function query_http()
     local res = ngx.location.capture("/my-http-proxy")
     ngx.say("http done: ", res.body)
 end

 ngx.thread.spawn(query_mysql)      -- create thread 1
 ngx.thread.spawn(query_memcached)  -- create thread 2
 ngx.thread.spawn(query_http)       -- create thread 3
```

This API was first enabled in the `v0.7.0` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.thread.wait

**syntax:** *ok, res1, res2, ... = ngx.thread.wait(thread1, thread2, ...)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Waits on one or more child "light threads" and returns the results of the first "light thread" that terminates (either successfully or with an error).

The arguments `thread1`, `thread2`, and etc are the Lua thread objects returned by earlier calls of [ngx.thread.spawn](#ngxthreadspawn).

The return values have exactly the same meaning as [coroutine.resume](#coroutineresume), that is, the first value returned is a boolean value indicating whether the "light thread" terminates successfully or not, and subsequent values returned are the return values of the user Lua function that was used to spawn the "light thread" (in case of success) or the error object (in case of failure).

Only the direct "parent coroutine" can wait on its child "light thread", otherwise a Lua exception will be raised.

The following example demonstrates the use of `ngx.thread.wait` and [ngx.location.capture](#ngxlocationcapture) to emulate [ngx.location.capture_multi](#ngxlocationcapture_multi):

```lua

 local capture = ngx.location.capture
 local spawn = ngx.thread.spawn
 local wait = ngx.thread.wait
 local say = ngx.say

 local function fetch(uri)
     return capture(uri)
 end

 local threads = {
     spawn(fetch, "/foo"),
     spawn(fetch, "/bar"),
     spawn(fetch, "/baz")
 }

 for i = 1, #threads do
     local ok, res = wait(threads[i])
     if not ok then
         say(i, ": failed to run: ", res)
     else
         say(i, ": status: ", res.status)
         say(i, ": body: ", res.body)
     end
 end
```

Here it essentially implements the "wait all" model.

And below is an example demonstrating the "wait any" model:

```lua

 function f()
     ngx.sleep(0.2)
     ngx.say("f: hello")
     return "f done"
 end

 function g()
     ngx.sleep(0.1)
     ngx.say("g: hello")
     return "g done"
 end

 local tf, err = ngx.thread.spawn(f)
 if not tf then
     ngx.say("failed to spawn thread f: ", err)
     return
 end

 ngx.say("f thread created: ", coroutine.status(tf))

 local tg, err = ngx.thread.spawn(g)
 if not tg then
     ngx.say("failed to spawn thread g: ", err)
     return
 end

 ngx.say("g thread created: ", coroutine.status(tg))

 ok, res = ngx.thread.wait(tf, tg)
 if not ok then
     ngx.say("failed to wait: ", res)
     return
 end

 ngx.say("res: ", res)

 -- stop the "world", aborting other running threads
 ngx.exit(ngx.OK)
```

And it will generate the following output:


    f thread created: running
    g thread created: running
    g: hello
    res: g done


This API was first enabled in the `v0.7.0` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.thread.kill

**syntax:** *ok, err = ngx.thread.kill(thread)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Kills a running "light thread" created by [ngx.thread.spawn](#ngxthreadspawn). Returns a true value when successful or `nil` and a string describing the error otherwise.

According to the current implementation, only the parent coroutine (or "light thread") can kill a thread. Also, a running "light thread" with pending Nginx subrequests (initiated by [ngx.location.capture](#ngxlocationcapture) for example) cannot be killed due to a limitation in the Nginx core.

This API was first enabled in the `v0.9.9` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.on_abort

**syntax:** *ok, err = ngx.on_abort(callback)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

Registers a user Lua function as the callback which gets called automatically when the client closes the (downstream) connection prematurely.

Returns `1` if the callback is registered successfully or returns `nil` and a string describing the error otherwise.

All the [Nginx API for Lua](#nginx-api-for-lua) can be used in the callback function because the function is run in a special "light thread", just as those "light threads" created by [ngx.thread.spawn](#ngxthreadspawn).

The callback function can decide what to do with the client abortion event all by itself. For example, it can simply ignore the event by doing nothing and the current Lua request handler will continue executing without interruptions. And the callback function can also decide to terminate everything by calling [ngx.exit](#ngxexit), for example,

```lua

 local function my_cleanup()
     -- custom cleanup work goes here, like cancelling a pending DB transaction

     -- now abort all the "light threads" running in the current request handler
     ngx.exit(499)
 end

 local ok, err = ngx.on_abort(my_cleanup)
 if not ok then
     ngx.log(ngx.ERR, "failed to register the on_abort callback: ", err)
     ngx.exit(500)
 end
```

When [lua_check_client_abort](#lua_check_client_abort) is set to `off` (which is the default), then this function call will always return the error message "lua_check_client_abort is off".

According to the current implementation, this function can only be called once in a single request handler; subsequent calls will return the error message "duplicate call".

This API was first introduced in the `v0.7.4` release.

See also [lua_check_client_abort](#lua_check_client_abort).

[Back to TOC](#nginx-api-for-lua)

## ngx.timer.at

**syntax:** *hdl, err = ngx.timer.at(delay, callback, user_arg1, user_arg2, ...)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Creates an Nginx timer with a user callback function as well as optional user arguments.

The first argument, `delay`, specifies the delay for the timer,
in seconds. One can specify fractional seconds like `0.001` to mean 1
millisecond here. `0` delay can also be specified, in which case the
timer will immediately expire when the current handler yields
execution.

The second argument, `callback`, can
be any Lua function, which will be invoked later in a background
"light thread" after the delay specified. The user callback will be
called automatically by the Nginx core with the arguments `premature`,
`user_arg1`, `user_arg2`, and etc, where the `premature`
argument takes a boolean value indicating whether it is a premature timer
expiration or not(for the `0` delay timer it is always `false`), and `user_arg1`, `user_arg2`, and etc, are
those (extra) user arguments specified when calling `ngx.timer.at`
as the remaining arguments.

Premature timer expiration happens when the Nginx worker process is
trying to shut down, as in an Nginx configuration reload triggered by
the `HUP` signal or in an Nginx server shutdown. When the Nginx worker
is trying to shut down, one can no longer call `ngx.timer.at` to
create new timers with nonzero delays and in that case `ngx.timer.at` will return a "conditional false" value and
a string describing the error, that is, "process exiting".

Starting from the `v0.9.3` release, it is allowed to create zero-delay timers even when the Nginx worker process starts shutting down.

When a timer expires, the user Lua code in the timer callback is
running in a "light thread" detached completely from the original
request creating the timer. So objects with the same lifetime as the
request creating them, like [cosockets](#ngxsockettcp), cannot be shared between the
original request and the timer user callback function.

Here is a simple example:

```nginx

 location / {
     ...
     log_by_lua_block {
         local function push_data(premature, uri, args, status)
             -- push the data uri, args, and status to the remote
             -- via ngx.socket.tcp or ngx.socket.udp
             -- (one may want to buffer the data in Lua a bit to
             -- save I/O operations)
         end
         local ok, err = ngx.timer.at(0, push_data,
                                      ngx.var.uri, ngx.var.args, ngx.header.status)
         if not ok then
             ngx.log(ngx.ERR, "failed to create timer: ", err)
             return
         end

         -- other job in log_by_lua_block
     }
 }
```

One can also create infinite re-occurring timers, for instance, a timer getting triggered every `5` seconds, by calling `ngx.timer.at` recursively in the timer callback function. Here is such an example,

```lua

 local delay = 5
 local handler
 handler = function (premature)
     -- do some routine job in Lua just like a cron job
     if premature then
         return
     end
     local ok, err = ngx.timer.at(delay, handler)
     if not ok then
         ngx.log(ngx.ERR, "failed to create the timer: ", err)
         return
     end

     -- do something in timer
 end

 local ok, err = ngx.timer.at(delay, handler)
 if not ok then
     ngx.log(ngx.ERR, "failed to create the timer: ", err)
     return
 end

 -- do other jobs
```

It is recommended, however, to use the [ngx.timer.every](#ngxtimerevery) API function
instead for creating recurring timers since it is more robust.

Because timer callbacks run in the background and their running time
will not add to any client request's response time, they can easily
accumulate in the server and exhaust system resources due to either
Lua programming mistakes or just too much client traffic. To prevent
extreme consequences like crashing the Nginx server, there are
built-in limitations on both the number of "pending timers" and the
number of "running timers" in an Nginx worker process. The "pending
timers" here mean timers that have not yet been expired and "running
timers" are those whose user callbacks are currently running.

The maximal number of pending timers allowed in an Nginx
worker is controlled by the [lua_max_pending_timers](#lua_max_pending_timers)
directive. The maximal number of running timers is controlled by the
[lua_max_running_timers](#lua_max_running_timers) directive.

According to the current implementation, each "running timer" will
take one (fake) connection record from the global connection record
list configured by the standard [worker_connections](http://nginx.org/en/docs/ngx_core_module.html#worker_connections) directive in
`nginx.conf`. So ensure that the
[worker_connections](http://nginx.org/en/docs/ngx_core_module.html#worker_connections) directive is set to
a large enough value that takes into account both the real connections
and fake connections required by timer callbacks (as limited by the
[lua_max_running_timers](#lua_max_running_timers) directive).

A lot of the Lua APIs for Nginx are enabled in the context of the timer
callbacks, like stream/datagram cosockets ([ngx.socket.tcp](#ngxsockettcp) and [ngx.socket.udp](#ngxsocketudp)), shared
memory dictionaries ([ngx.shared.DICT](#ngxshareddict)), user coroutines ([coroutine.*](#coroutinecreate)),
user "light threads" ([ngx.thread.*](#ngxthreadspawn)), [ngx.exit](#ngxexit), [ngx.now](#ngxnow)/[ngx.time](#ngxtime),
[ngx.md5](#ngxmd5)/[ngx.sha1_bin](#ngxsha1_bin), are all allowed. But the subrequest API (like
[ngx.location.capture](#ngxlocationcapture)), the [ngx.req.*](#ngxreqstart_time) API, the downstream output API
(like [ngx.say](#ngxsay), [ngx.print](#ngxprint), and [ngx.flush](#ngxflush)) are explicitly disabled in
this context.

You must notice that each timer will be based on a fake request (this fake request is also based on a fake connection). Because Nginx's memory release is based on the connection closure, if you run a lot of APIs that apply for memory resources in a timer, such as [tcpsock:connect](#tcpsockconnect), will cause the accumulation of memory resources. So it is recommended to create a new timer after running several times to release memory resources.

You can pass most of the standard Lua values (nils, booleans, numbers, strings, tables, closures, file handles, and etc) into the timer callback, either explicitly as user arguments or implicitly as upvalues for the callback closure. There are several exceptions, however: you *cannot* pass any thread objects returned by [coroutine.create](#coroutinecreate) and [ngx.thread.spawn](#ngxthreadspawn) or any cosocket objects returned by [ngx.socket.tcp](#ngxsockettcp), [ngx.socket.udp](#ngxsocketudp), and [ngx.req.socket](#ngxreqsocket) because these objects' lifetime is bound to the request context creating them while the timer callback is detached from the creating request's context (by design) and runs in its own (fake) request context. If you try to share the thread or cosocket objects across the boundary of the creating request, then you will get the "no co ctx found" error (for threads) or "bad request" (for cosockets). It is fine, however, to create all these objects inside your timer callback.

Please note that the timer Lua handler has its own copy of the `ngx.ctx` magic
table. It won't share the same `ngx.ctx` with the Lua handler creating the timer.
If you need to pass data from the timer creator to the timer handler, please
use the extra parameters of `ngx.timer.at()`.

This API was first introduced in the `v0.8.0` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.timer.every

**syntax:** *hdl, err = ngx.timer.every(delay, callback, user_arg1, user_arg2, ...)*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Similar to the [ngx.timer.at](#ngxtimerat) API function, but

1. `delay` *cannot* be zero,
1. timer will be created every `delay` seconds until the current Nginx worker process starts exiting.

Like [ngx.timer.at](#ngxtimerat), the `callback` argument will be called
automatically with the arguments `premature`, `user_arg1`, `user_arg2`, etc.

When success, returns a "conditional true" value (but not a `true`). Otherwise, returns a "conditional false" value and a string describing the error.

This API also respect the [lua_max_pending_timers](#lua_max_pending_timers) and [lua_max_running_timers](#lua_max_running_timers).

This API was first introduced in the `v0.10.9` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.timer.running_count

**syntax:** *count = ngx.timer.running_count()*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the number of timers currently running.

This directive was first introduced in the `v0.9.20` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.timer.pending_count

**syntax:** *count = ngx.timer.pending_count()*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Returns the number of pending timers.

This directive was first introduced in the `v0.9.20` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.config.subsystem

**syntax:** *subsystem = ngx.config.subsystem*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;, init_worker_by_lua&#42;, exit_worker_by_lua&#42;*

This string field indicates the Nginx subsystem the current Lua environment is based on. For this module, this field always takes the string value `"http"`. For
[ngx_stream_lua_module](https://github.com/openresty/stream-lua-nginx-module#readme), however, this field takes the value `"stream"`.

This field was first introduced in the `0.10.1`.

[Back to TOC](#nginx-api-for-lua)

## ngx.config.debug

**syntax:** *debug = ngx.config.debug*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;, init_worker_by_lua&#42;, exit_worker_by_lua&#42;*

This boolean field indicates whether the current Nginx is a debug build, i.e., being built by the `./configure` option `--with-debug`.

This field was first introduced in the `0.8.7`.

[Back to TOC](#nginx-api-for-lua)

## ngx.config.prefix

**syntax:** *prefix = ngx.config.prefix()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;, init_worker_by_lua&#42;, exit_worker_by_lua&#42;*

Returns the Nginx server "prefix" path, as determined by the `-p` command-line option when running the Nginx executable, or the path specified by the `--prefix` command-line option when building Nginx with the `./configure` script.

This function was first introduced in the `0.9.2`.

[Back to TOC](#nginx-api-for-lua)

## ngx.config.nginx_version

**syntax:** *ver = ngx.config.nginx_version*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;, init_worker_by_lua&#42;, exit_worker_by_lua&#42;*

This field take an integral value indicating the version number of the current Nginx core being used. For example, the version number `1.4.3` results in the Lua number 1004003.

This API was first introduced in the `0.9.3` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.config.nginx_configure

**syntax:** *str = ngx.config.nginx_configure()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;*

This function returns a string for the Nginx `./configure` command's arguments string.

This API was first introduced in the `0.9.5` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.config.ngx_lua_version

**syntax:** *ver = ngx.config.ngx_lua_version*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;*

This field take an integral value indicating the version number of the current `ngx_lua` module being used. For example, the version number `0.9.3` results in the Lua number 9003.

This API was first introduced in the `0.9.3` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.worker.exiting

**syntax:** *exiting = ngx.worker.exiting()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;, init_worker_by_lua&#42;, exit_worker_by_lua&#42;*

This function returns a boolean value indicating whether the current Nginx worker process already starts exiting. Nginx worker process exiting happens on Nginx server quit or configuration reload (aka HUP reload).

This API was first introduced in the `0.9.3` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.worker.pid

**syntax:** *pid = ngx.worker.pid()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;, init_worker_by_lua&#42;, exit_worker_by_lua&#42;*

This function returns a Lua number for the process ID (PID) of the current Nginx worker process. This API is more efficient than `ngx.var.pid` and can be used in contexts where the [ngx.var.VARIABLE](#ngxvarvariable) API cannot be used (like [init_worker_by_lua](#init_worker_by_lua)).

This API was first introduced in the `0.9.5` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.worker.pids

**syntax:** *pids = ngx.worker.pids()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, exit_worker_by_lua&#42;*

This function returns a Lua table for all Nginx worker process IDs (PIDs). Nginx uses channel to send the current worker PID to another worker in the worker process start or restart. So this API can get all current worker PIDs. Windows does not have this API.

This API was first introduced in the `0.10.23` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.worker.count

**syntax:** *count = ngx.worker.count()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_by_lua&#42;, init_worker_by_lua&#42;, exit_worker_by_lua&#42;*

Returns the total number of the Nginx worker processes (i.e., the value configured
by the [worker_processes](https://nginx.org/en/docs/ngx_core_module.html#worker_processes)
directive in `nginx.conf`).

This API was first introduced in the `0.9.20` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.worker.id

**syntax:** *id = ngx.worker.id()*

**context:** *set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, init_worker_by_lua&#42;, exit_worker_by_lua&#42;*

Returns the ordinal number of the current Nginx worker processes (starting from number 0).

So if the total number of workers is `N`, then this method may return a number between 0
and `N - 1` (inclusive).

This function returns meaningful values only for Nginx 1.9.1+. With earlier versions of Nginx, it
always returns `nil`.

See also [ngx.worker.count](#ngxworkercount).

This API was first introduced in the `0.9.20` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.semaphore

**syntax:** *local semaphore = require "ngx.semaphore"*

This is a Lua module that implements a classic-style semaphore API for efficient synchronizations among
different "light threads". Sharing the same semaphore among different "light threads" created in different (request)
contexts are also supported as long as the "light threads" reside in the same Nginx worker process
and the [lua_code_cache](#lua_code_cache) directive is turned on (which is the default).

This Lua module does not ship with this ngx_lua module itself rather it is shipped with
the
[lua-resty-core](https://github.com/openresty/lua-resty-core) library.

Please refer to the [documentation](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/semaphore.md)
for this `ngx.semaphore` Lua module in [lua-resty-core](https://github.com/openresty/lua-resty-core)
for more details.

This feature requires at least ngx_lua `v0.10.0`.

[Back to TOC](#nginx-api-for-lua)

## ngx.balancer

**syntax:** *local balancer = require "ngx.balancer"*

This is a Lua module that provides a Lua API to allow defining completely dynamic load balancers
in pure Lua.

This Lua module does not ship with this ngx_lua module itself rather it is shipped with
the
[lua-resty-core](https://github.com/openresty/lua-resty-core) library.

Please refer to the [documentation](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/balancer.md)
for this `ngx.balancer` Lua module in [lua-resty-core](https://github.com/openresty/lua-resty-core)
for more details.

This feature requires at least ngx_lua `v0.10.0`.

[Back to TOC](#nginx-api-for-lua)

## ngx.ssl

**syntax:** *local ssl = require "ngx.ssl"*

This Lua module provides API functions to control the SSL handshake process in contexts like
[ssl_certificate_by_lua*](#ssl_certificate_by_lua_block).

This Lua module does not ship with this ngx_lua module itself rather it is shipped with
the
[lua-resty-core](https://github.com/openresty/lua-resty-core) library.

Please refer to the [documentation](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl.md)
for this `ngx.ssl` Lua module for more details.

This feature requires at least ngx_lua `v0.10.0`.

[Back to TOC](#nginx-api-for-lua)

## ngx.ocsp

**syntax:** *local ocsp = require "ngx.ocsp"*

This Lua module provides API to perform OCSP queries, OCSP response validations, and
OCSP stapling planting.

Usually, this module is used together with the [ngx.ssl](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ssl.md)
module in the
context of [ssl_certificate_by_lua*](#ssl_certificate_by_lua_block).

This Lua module does not ship with this ngx_lua module itself rather it is shipped with
the
[lua-resty-core](https://github.com/openresty/lua-resty-core) library.

Please refer to the [documentation](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/ocsp.md)
for this `ngx.ocsp` Lua module for more details.

This feature requires at least ngx_lua `v0.10.0`.

[Back to TOC](#nginx-api-for-lua)

## ndk.set_var.DIRECTIVE

**syntax:** *res = ndk.set_var.DIRECTIVE_NAME*

**context:** *init_worker_by_lua&#42;, set_by_lua&#42;, rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, log_by_lua&#42;, ngx.timer.&#42;, balancer_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, exit_worker_by_lua&#42;, ssl_client_hello_by_lua&#42;*

This mechanism allows calling other Nginx C modules' directives that are implemented by [Nginx Devel Kit](https://github.com/simplresty/ngx_devel_kit) (NDK)'s set_var submodule's `ndk_set_var_value`.

For example, the following [set-misc-nginx-module](http://github.com/openresty/set-misc-nginx-module) directives can be invoked this way:

* [set_quote_sql_str](http://github.com/openresty/set-misc-nginx-module#set_quote_sql_str)
* [set_quote_pgsql_str](http://github.com/openresty/set-misc-nginx-module#set_quote_pgsql_str)
* [set_quote_json_str](http://github.com/openresty/set-misc-nginx-module#set_quote_json_str)
* [set_unescape_uri](http://github.com/openresty/set-misc-nginx-module#set_unescape_uri)
* [set_escape_uri](http://github.com/openresty/set-misc-nginx-module#set_escape_uri)
* [set_encode_base32](http://github.com/openresty/set-misc-nginx-module#set_encode_base32)
* [set_decode_base32](http://github.com/openresty/set-misc-nginx-module#set_decode_base32)
* [set_encode_base64](http://github.com/openresty/set-misc-nginx-module#set_encode_base64)
* [set_decode_base64](http://github.com/openresty/set-misc-nginx-module#set_decode_base64)
* [set_encode_hex](http://github.com/openresty/set-misc-nginx-module#set_encode_base64)
* [set_decode_hex](http://github.com/openresty/set-misc-nginx-module#set_decode_base64)
* [set_sha1](http://github.com/openresty/set-misc-nginx-module#set_encode_base64)
* [set_md5](http://github.com/openresty/set-misc-nginx-module#set_decode_base64)

For instance,

```lua

 local res = ndk.set_var.set_escape_uri('a/b')
 -- now res == 'a%2fb'
```

Similarly, the following directives provided by [encrypted-session-nginx-module](http://github.com/openresty/encrypted-session-nginx-module) can be invoked from within Lua too:

* [set_encrypt_session](http://github.com/openresty/encrypted-session-nginx-module#set_encrypt_session)
* [set_decrypt_session](http://github.com/openresty/encrypted-session-nginx-module#set_decrypt_session)

This feature requires the [ngx_devel_kit](https://github.com/simplresty/ngx_devel_kit) module.

[Back to TOC](#nginx-api-for-lua)

## coroutine.create

**syntax:** *co = coroutine.create(f)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, init_by_lua&#42;, ngx.timer.&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Creates a user Lua coroutines with a Lua function, and returns a coroutine object.

Similar to the standard Lua [coroutine.create](https://www.lua.org/manual/5.1/manual.html#pdf-coroutine.create) API, but works in the context of the Lua coroutines created by ngx_lua.

This API was first usable in the context of [init_by_lua*](#init_by_lua) since the `0.9.2`.

This API was first introduced in the `v0.6.0` release.

[Back to TOC](#nginx-api-for-lua)

## coroutine.resume

**syntax:** *ok, ... = coroutine.resume(co, ...)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, init_by_lua&#42;, ngx.timer.&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Resumes the execution of a user Lua coroutine object previously yielded or just created.

Similar to the standard Lua [coroutine.resume](https://www.lua.org/manual/5.1/manual.html#pdf-coroutine.resume) API, but works in the context of the Lua coroutines created by ngx_lua.

This API was first usable in the context of [init_by_lua*](#init_by_lua) since the `0.9.2`.

This API was first introduced in the `v0.6.0` release.

[Back to TOC](#nginx-api-for-lua)

## coroutine.yield

**syntax:** *... = coroutine.yield(...)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, init_by_lua&#42;, ngx.timer.&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Yields the execution of the current user Lua coroutine.

Similar to the standard Lua [coroutine.yield](https://www.lua.org/manual/5.1/manual.html#pdf-coroutine.yield) API, but works in the context of the Lua coroutines created by ngx_lua.

This API was first usable in the context of [init_by_lua*](#init_by_lua) since the `0.9.2`.

This API was first introduced in the `v0.6.0` release.

[Back to TOC](#nginx-api-for-lua)

## coroutine.wrap

**syntax:** *co = coroutine.wrap(f)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, init_by_lua&#42;, ngx.timer.&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Similar to the standard Lua [coroutine.wrap](https://www.lua.org/manual/5.1/manual.html#pdf-coroutine.wrap) API, but works in the context of the Lua coroutines created by ngx_lua.

This API was first usable in the context of [init_by_lua*](#init_by_lua) since the `0.9.2`.

This API was first introduced in the `v0.6.0` release.

[Back to TOC](#nginx-api-for-lua)

## coroutine.running

**syntax:** *co = coroutine.running()*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, init_by_lua&#42;, ngx.timer.&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Identical to the standard Lua [coroutine.running](https://www.lua.org/manual/5.1/manual.html#pdf-coroutine.running) API.

This API was first usable in the context of [init_by_lua*](#init_by_lua) since the `0.9.2`.

This API was first enabled in the `v0.6.0` release.

[Back to TOC](#nginx-api-for-lua)

## coroutine.status

**syntax:** *status = coroutine.status(co)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;, init_by_lua&#42;, ngx.timer.&#42;, header_filter_by_lua&#42;, body_filter_by_lua&#42;, ssl_certificate_by_lua&#42;, ssl_session_fetch_by_lua&#42;, ssl_session_store_by_lua&#42;, ssl_client_hello_by_lua&#42;*

Identical to the standard Lua [coroutine.status](https://www.lua.org/manual/5.1/manual.html#pdf-coroutine.status) API.

This API was first usable in the context of [init_by_lua*](#init_by_lua) since the `0.9.2`.

This API was first enabled in the `v0.6.0` release.

[Back to TOC](#nginx-api-for-lua)

## ngx.run_worker_thread

**syntax:** *ok, res1, res2, ... = ngx.run_worker_thread(threadpool, module_name, func_name, arg1, arg2, ...)*

**context:** *rewrite_by_lua&#42;, access_by_lua&#42;, content_by_lua&#42;*

**This API is still experimental and may change in the future without notice.**

**This API is available only for Linux.**

Wrap the [nginx worker thread](http://nginx.org/en/docs/dev/development_guide.html#threads) to execute lua function. The caller coroutine would yield until the function returns.

Only the following ngx_lua APIs could be used in `function_name` function of the `module` module:

* `ngx.encode_base64`
* `ngx.decode_base64`

* `ngx.hmac_sha1`
* `ngx.encode_args`
* `ngx.decode_args`
* `ngx.quote_sql_str`

* `ngx.re.match`
* `ngx.re.find`
* `ngx.re.gmatch`
* `ngx.re.sub`
* `ngx.re.gsub`

* `ngx.crc32_short`
* `ngx.crc32_long`
* `ngx.hmac_sha1`
* `ngx.md5_bin`
* `ngx.md5`

* `ngx.config.subsystem`
* `ngx.config.debug`
* `ngx.config.prefix`
* `ngx.config.nginx_version`
* `ngx.config.nginx_configure`
* `ngx.config.ngx_lua_version`

* `ngx.shared.DICT`

The first argument `threadpool` specifies the Nginx thread pool name defined by [thread_pool](https://nginx.org/en/docs/ngx_core_module.html#thread_pool).

The second argument `module_name` specifies the lua module name to execute in the worker thread, which would return a lua table. The module must be inside the package path, e.g.

```nginx

```

The third argument `func_name` specifies the function field in the module table as the second argument.

The type of `arg`s must be one of type below:

* boolean
* number
* string
* nil
* table (the table may be recursive, and contains members of types above.)

The `ok` is in boolean type, which indicate the C land error (failed to get thread from thread pool, pcall the module function failed, .etc). If `ok` is `false`, the `res1` is the error string.

The return values (res1, ...) are returned by invocation of the module function. Normally, the `res1` should be in boolean type, so that the caller could inspect the error.

This API is useful when you need to execute the below types of tasks:

* CPU bound task, e.g. do md5 calculation
* File I/O task
* Call `os.execute()` or blocking C API via `ffi`
* Call external Lua library not based on cosocket or nginx

Example1: do md5 calculation.

```nginx

 location /calc_md5 {
     default_type 'text/plain';

     content_by_lua_block {
         local ok, md5_or_err = ngx.run_worker_thread("testpool", "md5", "md5")
         ngx.say(ok, " : ", md5_or_err)
     }
 }
```

`md5.lua`

```lua
local function md5()
    return ngx.md5("hello")
end

return { md5=md5, }
```

Example2: write logs into the log file.

```nginx

 location /write_log_file {
     default_type 'text/plain';

     content_by_lua_block {
         local ok, err = ngx.run_worker_thread("testpool", "write_log_file", "log", ngx.var.arg_str)
         if not ok then
             ngx.say(ok, " : ", err)
             return
         end
         ngx.say(ok)
     }
 }
```

`write_log_file.lua`

```lua

 local function log(str)
     local file, err = io.open("/tmp/tmp.log", "a")
     if not file then
         return false, err
     end
     file:write(str)
     file:flush()
     file:close()
     return true
 end
 return {log=log}
```

[Back to TOC](#nginx-api-for-lua)

## Obsolete Sections

This section is just holding obsolete documentation sections that have been either renamed or removed so that existing links over the web are still valid.


## Special PCRE Sequences

This section has been renamed to [Special Escaping Sequences](#special-escaping-sequences).


## Lua/LuaJIT bytecode support

This section has been renamed to
[LuaJIT bytecode support](#luajit-bytecode-support). As of version
`v0.10.16` of this module, the standard Lua interpreter (also known
as "PUC-Rio Lua") is not supported anymore.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-lua](https://github.com/openresty/lua-nginx-module){target=_blank}.

# *markdown*: Markdown-to-html NGINX module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-markdown
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_markdown_filter_module.so;
```


This document describes nginx-module-markdown [v0.1.2](https://github.com/ukarim/ngx_markdown_filter_module/releases/tag/0.1.2){target=_blank} 
released on Apr 23 2023.

<hr />

The `ngx_markdown_filter_module` module is a filter that transforms markdown files to html format.

This module utilizes the [cmark](https://github.com/commonmark/cmark) library.

### Example configuration

```nginx
location ~ \.md {
    markdown_filter on;
    markdown_template html/template.html;
}
```

This works on proxy locations as well.

### Directives

```
Syntax:  markdown_filter on;
Context: location
```

```
Syntax:  markdown_template html/template.html;
Context: location
```

### Build with cmark-gfm (tables support)

Original cmark library doesn't support tables. But there is [cmark-gfm](https://github.com/github/cmark-gfm)
fork with table extension, supported by Github.

1. Clone this repo

2. Rename `config_gfm` to `config`

2. Install `cmark-gfm` lib

3. Download [nginx src archive](http://nginx.org/en/download.html) and unpack it

4. Run `configure` script (see nginx src) and build nginx

```
> ./configure --add-module=/path/to/ngx_markdown_filter_module --with-cc-opt=-DWITH_CMARK_GFM=1
> make
```

5. Apply markdown directives to nginx conf and run it

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-markdown](https://github.com/ukarim/ngx_markdown_filter_module){target=_blank}.

# *memc*: Extended version of the standard NGINX memcached module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-memc
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_memc_module.so;
```


This document describes nginx-module-memc [v0.19](https://github.com/openresty/memc-nginx-module/releases/tag/v0.19){target=_blank} 
released on Apr 19 2018.

<hr />

**ngx_memc** - An extended version of the standard memcached module that supports set, add, delete, and many more memcached commands.


## Synopsis

```nginx

 # GET /foo?key=dog
 #
 # POST /foo?key=cat
 # Cat's value...
 #
 # PUT /foo?key=bird
 # Bird's value...
 #
 # DELETE /foo?key=Tiger
 location /foo {
     set $memc_key $arg_key;

     # $memc_cmd defaults to get for GET,
     #   add for POST, set for PUT, and
     #   delete for the DELETE request method.

     memc_pass 127.0.0.1:11211;
 }
```

```nginx

 # GET /bar?cmd=get&key=cat
 #
 # POST /bar?cmd=set&key=dog
 # My value for the "dog" key...
 #
 # DELETE /bar?cmd=delete&key=dog
 # GET /bar?cmd=delete&key=dog
 location /bar {
     set $memc_cmd $arg_cmd;
     set $memc_key $arg_key;
     set $memc_flags $arg_flags; # defaults to 0
     set $memc_exptime $arg_exptime; # defaults to 0

     memc_pass 127.0.0.1:11211;
 }
```

```nginx

 # GET /bar?cmd=get&key=cat
 # GET /bar?cmd=set&key=dog&val=animal&flags=1234&exptime=2
 # GET /bar?cmd=delete&key=dog
 # GET /bar?cmd=flush_all
 location /bar {
     set $memc_cmd $arg_cmd;
     set $memc_key $arg_key;
     set $memc_value $arg_val;
     set $memc_flags $arg_flags; # defaults to 0
     set $memc_exptime $arg_exptime; # defaults to 0

     memc_cmds_allowed get set add delete flush_all;

     memc_pass 127.0.0.1:11211;
 }
```

```nginx

   http {
     ...
     upstream backend {
        server 127.0.0.1:11984;
        server 127.0.0.1:11985;
     }
     server {
         location /stats {
             set $memc_cmd stats;
             memc_pass backend;
         }
         ...
     }
   }
   ...
```

```nginx

 # read the memcached flags into the Last-Modified header
 # to respond 304 to conditional GET
 location /memc {
     set $memc_key $arg_key;

     memc_pass 127.0.0.1:11984;

     memc_flags_to_last_modified on;
 }
```

```nginx

 location /memc {
     set $memc_key foo;
     set $memc_cmd get;

     # access the unix domain socket listend by memcached
     memc_pass unix:/tmp/memcached.sock;
 }
```

## Description

This module extends the standard [memcached module](http://nginx.org/en/docs/http/ngx_http_memcached_module.html) to support almost the whole [memcached ascii protocol](http://code.sixapart.com/svn/memcached/trunk/server/doc/protocol.txt).

It allows you to define a custom [REST](http://en.wikipedia.org/wiki/REST) interface to your memcached servers or access memcached in a very efficient way from within the nginx server by means of subrequests or [independent fake requests](http://github.com/srlindsay/nginx-independent-subrequest).

This module is not supposed to be merged into the Nginx core because I've used [Ragel](http://www.complang.org/ragel/) to generate the memcached response parsers (in C) for joy :)

If you are going to use this module to cache location responses out of the box, try [srcache-nginx-module](http://github.com/openresty/srcache-nginx-module) with this module to achieve that.

When used in conjunction with [lua-nginx-module](http://github.com/openresty/lua-nginx-module), it is recommended to use the [lua-resty-memcached](http://github.com/openresty/lua-resty-memcached) library instead of this module though, because the former is much more flexible and memory-efficient.


## Keep-alive connections to memcached servers

You need [HttpUpstreamKeepaliveModule](http://wiki.nginx.org/HttpUpstreamKeepaliveModule) together with this module for keep-alive TCP connections to your backend memcached servers.

Here's a sample configuration:

```nginx

   http {
     upstream backend {
       server 127.0.0.1:11211;

       # a pool with at most 1024 connections
       # and do not distinguish the servers:
       keepalive 1024;
     }

     server {
         ...
         location /memc {
             set $memc_cmd get;
             set $memc_key $arg_key;
             memc_pass backend;
         }
     }
   }
```


## How it works

It implements the memcached TCP protocol all by itself, based upon the `upstream` mechanism. Everything involving I/O is non-blocking.

The module itself does not keep TCP connections to the upstream memcached servers across requests, just like other upstream modules. For a working solution, see section [Keep-alive connections to memcached servers](#keep-alive-connections-to-memcached-servers).


## Memcached commands supported

The memcached storage commands [set](#set-memc_key-memc_flags-memc_exptime-memc_value), [add](#add-memc_key-memc_flags-memc_exptime-memc_value), [replace](#replace-memc_key-memc_flags-memc_exptime-memc_value), [prepend](#prepend-memc_key-memc_flags-memc_exptime-memc_value), and [append](#append-memc_key-memc_flags-memc_exptime-memc_value) uses the `$memc_key` as the key, `$memc_exptime` as the expiration time (or delay) (defaults to 0), `$memc_flags` as the flags (defaults to 0), to build the corresponding memcached queries.

If `$memc_value` is not defined at all, then the request body will be used as the value of the `$memc_value` except for the [incr](#incr-memc_key-memc_value) and [decr](#decr-memc_key-memc_value) commands. Note that if `$memc_value` is defined as an empty string (`""`), that empty string will still be used as the value as is.

The following memcached commands have been implemented and tested (with their parameters marked by corresponding
nginx variables defined by this module):


## get $memc_key

Retrieves the value using a key.

```nginx

   location /foo {
       set $memc_cmd 'get';
       set $memc_key 'my_key';

       memc_pass 127.0.0.1:11211;

       add_header X-Memc-Flags $memc_flags;
   }
```

Returns `200 OK` with the value put into the response body if the key is found, or `404 Not Found` otherwise. The `flags` number will be set into the `$memc_flags` variable so it's often desired to put that info into the response headers by means of the standard [add_header directive](http://nginx.org/en/docs/http/ngx_http_headers_module.html#add_header).

It returns `502` for `ERROR`, `CLIENT_ERROR`, or `SERVER_ERROR`.


## set $memc_key $memc_flags $memc_exptime $memc_value

To use the request body as the memcached value, just avoid setting the `$memc_value` variable:

```nginx

   # POST /foo
   # my value...
   location /foo {
       set $memc_cmd 'set';
       set $memc_key 'my_key';
       set $memc_flags 12345;
       set $memc_exptime 24;

       memc_pass 127.0.0.1:11211;
   }
```

Or let the `$memc_value` hold the value:

```nginx

   location /foo {
       set $memc_cmd 'set';
       set $memc_key 'my_key';
       set $memc_flags 12345;
       set $memc_exptime 24;
       set $memc_value 'my_value';

       memc_pass 127.0.0.1:11211;
   }
```

Returns `201 Created` if the upstream memcached server replies `STORED`, `200` for `NOT_STORED`, `404` for `NOT_FOUND`, `502` for `ERROR`, `CLIENT_ERROR`, or `SERVER_ERROR`.

The original memcached responses are returned as the response body except for `404 NOT FOUND`.


## add $memc_key $memc_flags $memc_exptime $memc_value

Similar to the [set command](#set-memc_key-memc_flags-memc_exptime-memc_value).


## replace $memc_key $memc_flags $memc_exptime $memc_value

Similar to the [set command](#set-memc_key-memc_flags-memc_exptime-memc_value).


## append $memc_key $memc_flags $memc_exptime $memc_value

Similar to the [set command](#set-memc_key-memc_flags-memc_exptime-memc_value).

Note that at least memcached version 1.2.2 does not support the "append" and "prepend" commands. At least 1.2.4 and later versions seem to supports these two commands.


## prepend $memc_key $memc_flags $memc_exptime $memc_value

Similar to the [append command](#append-memc_key-memc_flags-memc_exptime-memc_value).


## delete $memc_key

Deletes the memcached entry using a key.

```nginx

   location /foo
       set $memc_cmd delete;
       set $memc_key my_key;

       memc_pass 127.0.0.1:11211;
   }
```

Returns `200 OK` if deleted successfully, `404 Not Found` for `NOT_FOUND`, or `502` for `ERROR`, `CLIENT_ERROR`, or `SERVER_ERROR`.

The original memcached responses are returned as the response body except for `404 NOT FOUND`.


## delete $memc_key $memc_exptime

Similar to the [delete $memc_key](#delete-memc_key) command except it accepts an optional `expiration` time specified by the `$memc_exptime` variable.

This command is no longer available in the latest memcached version 1.4.4.


## incr $memc_key $memc_value

Increments the existing value of `$memc_key` by the amount specified by `$memc_value`:

```nginx

   location /foo {
       set $memc_cmd incr;
       set $memc_key my_key;
       set $memc_value 2;
       memc_pass 127.0.0.1:11211;
   }
```

In the preceding example, every time we access `/foo` will cause the value of `my_key` increments by `2`.

Returns `200 OK` with the new value associated with that key as the response body if successful, or `404 Not Found` if the key is not found.

It returns `502` for `ERROR`, `CLIENT_ERROR`, or `SERVER_ERROR`.


## decr $memc_key $memc_value

Similar to [incr $memc_key $memc_value](#incr-memc_key-memc_value).


## flush_all

Mark all the keys on the memcached server as expired:

```nginx

   location /foo {
       set $memc_cmd flush_all;
       memc_pass 127.0.0.1:11211;
   }
```


## flush_all $memc_exptime

Just like [flush_all](#flush_all) but also accepts an expiration time specified by the `$memc_exptime` variable.


## stats

Causes the memcached server to output general-purpose statistics and settings

```nginx

   location /foo {
       set $memc_cmd stats;
       memc_pass 127.0.0.1:11211;
   }
```

Returns `200 OK` if the request succeeds, or 502 for `ERROR`, `CLIENT_ERROR`, or `SERVER_ERROR`.

The raw `stats` command output from the upstream memcached server will be put into the response body. 


## Directives

All the standard [memcached module](http://nginx.org/en/docs/http/ngx_http_memcached_module.html) directives in nginx 0.8.28 are directly inherited, with the `memcached_` prefixes replaced by `memc_`. For example, the `memcached_pass` directive is spelled `memc_pass`.

Here we only document the most important two directives (the latter is a new directive introduced by this module).


## memc_pass

**syntax:** *memc_pass &lt;memcached server IP address&gt;:&lt;memcached server port&gt;*

**syntax:** *memc_pass &lt;memcached server hostname&gt;:&lt;memcached server port&gt;*

**syntax:** *memc_pass &lt;upstream_backend_name&gt;*

**syntax:** *memc_pass unix:&lt;path_to_unix_domain_socket&gt;*

**default:** *none*

**context:** *http, server, location, if*

**phase:** *content*

Specify the memcached server backend.


## memc_cmds_allowed
**syntax:** *memc_cmds_allowed &lt;cmd&gt;...*

**default:** *none*

**context:** *http, server, location, if*

Lists memcached commands that are allowed to access. By default, all the memcached commands supported by this module are accessible.
An example is

```nginx

    location /foo {
        set $memc_cmd $arg_cmd;
        set $memc_key $arg_key;
        set $memc_value $arg_val;

        memc_pass 127.0.0.1:11211;

        memc_cmds_allowed get;
    }
```


## memc_flags_to_last_modified
**syntax:** *memc_flags_to_last_modified on|off*

**default:** *off*

**context:** *http, server, location, if*

Read the memcached flags as epoch seconds and set it as the value of the `Last-Modified` header. For conditional GET, it will signal nginx to return `304 Not Modified` response to save bandwidth.


## memc_connect_timeout
**syntax:** *memc_connect_timeout &lt;time&gt;*

**default:** *60s*

**context:** *http, server, location*

The timeout for connecting to the memcached server, in seconds by default.

It's wise to always explicitly specify the time unit to avoid confusion. Time units supported are "s"(seconds), "ms"(milliseconds), "y"(years), "M"(months), "w"(weeks), "d"(days), "h"(hours), and "m"(minutes).

This time must be less than 597 hours.


## memc_send_timeout
**syntax:** *memc_send_timeout &lt;time&gt;*

**default:** *60s*

**context:** *http, server, location*

The timeout for sending TCP requests to the memcached server, in seconds by default.

It is wise to always explicitly specify the time unit to avoid confusion. Time units supported are "s"(seconds), "ms"(milliseconds), "y"(years), "M"(months), "w"(weeks), "d"(days), "h"(hours), and "m"(minutes).

This time must be less than 597 hours.


## memc_read_timeout
**syntax:** *memc_read_timeout &lt;time&gt;*

**default:** *60s*

**context:** *http, server, location*

The timeout for reading TCP responses from the memcached server, in seconds by default.

It's wise to always explicitly specify the time unit to avoid confusion. Time units supported are "s"(seconds), "ms"(milliseconds), "y"(years), "M"(months), "w"(weeks), "d"(days), "h"(hours), and "m"(minutes).

This time must be less than 597 hours.


## memc_buffer_size
**syntax:** *memc_buffer_size &lt;size&gt;*

**default:** *4k/8k*

**context:** *http, server, location*

This buffer size is used for the memory buffer to hold

* the complete response for memcached commands other than `get`,
* the complete response header (i.e., the first line of the response) for the `get` memcached command.

This default size is the page size, may be `4k` or `8k`.


## memc_ignore_client_abort
**syntax:** *memc_ignore_client_abort on|off*

**default:** *off*

**context:** *location*

Determines whether the connection with a memcache server should be closed when a client closes a connection without waiting for a response.

This directive was first added in the `v0.14` release.


## Changes

The changes of every release of this module can be obtained from the OpenResty bundle's change logs:

<http://openresty.org/#Changes>


## Test Suite

This module comes with a Perl-driven test suite. The [test cases](http://github.com/openresty/memc-nginx-module/tree/master/t/) are
[declarative](http://github.com/openresty/memc-nginx-module/blob/master/t/storage.t) too. Thanks to the [Test::Base](http://search.cpan.org/perldoc?Test::Base) module in the Perl world.

To run it on your side:

```bash

 $ PATH=/path/to/your/nginx-with-memc-module:$PATH prove -r t
```

You need to terminate any Nginx processes before running the test suite if you have changed the Nginx server binary.

Either [LWP::UserAgent](http://search.cpan.org/perldoc?LWP::UserAgent) or [IO::Socket](http://search.cpan.org/perldoc?IO::Socket) is used by the [test scaffold](http://github.com/openresty/memc-nginx-module/blob/master/test/lib/Test/Nginx/LWP.pm).

Because a single nginx server (by default, `localhost:1984`) is used across all the test scripts (`.t` files), it's meaningless to run the test suite in parallel by specifying `-jN` when invoking the `prove` utility.

You should also keep a memcached server listening on the `11211` port at localhost before running the test suite.

Some parts of the test suite requires modules [rewrite](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html) and [echo](http://github.com/openresty/echo-nginx-module) to be enabled as well when building Nginx.


## See Also

* The original announcement email on the nginx mailing list: [ngx_memc: "an extended version of ngx_memcached that supports set, add, delete, and many more commands"](http://forum.nginx.org/read.php?2,28359)
* My slides demonstrating various ngx_memc usage: <http://agentzh.org/misc/slides/nginx-conf-scripting/nginx-conf-scripting.html#34> (use the arrow or pageup/pagedown keys on the keyboard to swith pages)
* The latest [memcached TCP protocol](http://code.sixapart.com/svn/memcached/trunk/server/doc/protocol.txt).
* The [ngx_srcache](http://github.com/openresty/srcache-nginx-module) module
* The [lua-resty-memcached](https://github.com/openresty/lua-resty-memcached) library based on the [lua-nginx-module](http://github.com/openresty/lua-nginx-module) cosocket API.
* The standard [memcached](http://nginx.org/en/docs/http/ngx_http_memcached_module.html) module.
* The [echo module](http://github.com/openresty/echo-nginx-module) for Nginx module's automated testing.
* The standard [headers](http://nginx.org/en/docs/http/ngx_http_headers_module.html) module and the 3rd-parth [headers-more](http://github.com/openresty/headers-more-nginx-module) module.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-memc](https://github.com/openresty/memc-nginx-module){target=_blank}.

# *naxsi*: NGINX Anti XSS & SQL Injection module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-naxsi
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_naxsi_module.so;
```


This document describes nginx-module-naxsi [v1.6](https://github.com/dvershinin/naxsi/releases/tag/1.6){target=_blank} 
released on Nov 28 2023.

<hr />
![naxsi](logo.png)

## What is Naxsi?

NAXSI means [Nginx]( http://nginx.org/ ) Anti [XSS]( https://www.owasp.org/index.php/Cross-site_Scripting_%28XSS%29 ) & [SQL Injection]( https://www.owasp.org/index.php/SQL_injection ). 

Technically, it is a third party nginx module, available as a package for
many UNIX-like platforms. This module, by default, reads a small subset of
simple (and readable) rules containing 99% of known patterns involved in
website vulnerabilities. For example, `<`, `|` or `drop` are not supposed
to be part of a URI.

Being very simple, those patterns may match legitimate queries, it is
the Naxsi's administrator duty to add specific rules that will whitelist
legitimate behaviours. The administrator can either add whitelists manually
by analyzing nginx's error log, or (recommended) start the project with an
intensive auto-learning phase that will automatically generate whitelisting
rules regarding a website's behaviour.

In short, Naxsi behaves like a DROP-by-default firewall, the only task
is to add required ACCEPT rules for the target website to work properly.

## Why is it different?

Contrary to most Web Application Firewalls, Naxsi doesn't rely on a
signature base like an antivirus, and thus cannot be circumvented by an
"unknown" attack pattern.
Naxsi is [Free software]( https://www.gnu.org/licenses/gpl.html ) (as in freedom)
and free (as in free beer) to use.

## What does it run on?
Naxsi should be compatible with any nginx version.

It depends on `libpcre` for its regexp support, and is reported to work great on NetBSD, FreeBSD, OpenBSD, Debian, Ubuntu and CentOS.

## Why using this repository

*The original project is (unofficially) abandoned*, but you can fully ask for support here as i'm willing to keep the project working as last remaining developer.

## Documentation

[docs](docs/index)

## Build naxsi

**Be sure when you clone the repository to fetch all the submodules.**

```
$ git clone --recurse-submodules https://github.com/wargio/naxsi.git
$ wget --no-clobber -O nginx.tar.gz "https://nginx.org/download/nginx-1.22.0.tar.gz"
$ mkdir nginx-source
$ tar -C nginx-source -xzf nginx.tar.gz --strip-components=1
$ cd nginx-source
$ ./configure  --prefix=/etc/nginx --add-dynamic-module=../naxsi/naxsi_src
$ make
```

## Support

You can ask for support regarding NAXSI here or on the original repository https://github.com/nbs-system/naxsi

## Future plans

- Bring back nxapi via py3
- Creation of a simple tool to create rules and test them

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-naxsi](https://github.com/dvershinin/naxsi){target=_blank}.

# *nchan*: Scalable, flexible pub/sub server for the modern web


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-nchan
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_nchan_module.so;
```


This document describes nginx-module-nchan [v1.3.6](https://github.com/slact/nchan/releases/tag/v1.3.6){target=_blank} 
released on Jan 06 2023.

<hr />
<img class="logo" alt="NCHAN" src="https://nchan.io/github-logo.png" />

https://nchan.io

Nchan is a scalable, flexible pub/sub server for the modern web, built as a module for the [Nginx](http://nginx.org) web server. It can be configured as a standalone server, or as a shim between your application and hundreds, thousands, or millions of live subscribers. It can buffer messages in memory, on-disk, or via [Redis](http://redis.io). All connections are handled asynchronously and distributed among any number of worker processes. It can also scale to many Nginx servers with [Redis](http://redis.io).

Messages are [published](#publisher-endpoints) to channels with HTTP `POST` requests or Websocket, and [subscribed](#subscriber-endpoint) also through [Websocket](#websocket), [long-polling](#long-polling), [EventSource](#eventsource) (SSE), old-fashioned [interval polling](#interval-polling), [and](#http-chunked-transfer) [more](#http-multipart-mixed).

In a web browser, you can use Websocket or EventSource natively, or the [NchanSubscriber.js](https://github.com/slact/nchan.js) wrapper library. It supports Long-Polling, EventSource, and resumable Websockets, and has a few other added convenience options. It's also available on [NPM](https://www.npmjs.com/package/nchan).

## Features
 - RESTful, HTTP-native [API](#publishing-messages).
 - Supports [Websocket](#websocket), [EventSource (Server-Sent Events)](#eventsource), [Long-Polling](#long-polling) and other HTTP-based subscribers.
 - Per-channel configurable message buffers with no-repeat, no-loss message delivery guarantees.
 - Subscribe to [hundreds of channels](#channel-multiplexing) over a single subscriber connection.
 - HTTP request [callbacks and hooks](#hooks-and-callbacks) for easy integration.
 - Introspection with [channel events](#channel-events) and [url for monitoring performance statistics](#nchan_stub_status-stats).
 - Channel [group](#channel-groups) usage [accounting and limits](#limits-and-accounting).
 - Fast, nonblocking [shared-memory local message storage](#memory-storage) and optional, slower, persistent storage with [Redis](#redis).
 - Horizontally scalable (using [Redis](#redis)).
 - Auto-failover and [high availability](#high-availability) with no single point of failure using [Redis Cluster](#redis-cluster).

## Status and History

The latest Nchan release is 1.3.6 (January 6, 2023) ([changelog](https://nchan.io/changelog)).

The first iteration of Nchan was written in 2009-2010 as the [Nginx HTTP Push Module](https://pushmodule.slact.net), and was vastly refactored into its present state in 2014-2016.

#### Upgrade from Nginx HTTP Push Module

Although Nchan is backwards-compatible with all Push Module configuration directives, some of the more unusual and rarely used settings have been disabled and will be ignored (with a warning). See the [upgrade page](https://nchan.io/upgrade) for a detailed list of changes and improvements, as well as a full list of incompatibilities.


## Does it scale?

<img class="benchmark_graph" alt="benchmarking internal subscriber response times" src="https://nchan.io/img/benchmark_internal_total.png" />

Yes it does. Like Nginx, Nchan can easily handle as much traffic as you can throw at it. I've tried to benchmark it, but my benchmarking tools are much slower than Nchan. The data I've gathered is on how long Nchan itself takes to respond to every subscriber after publishing a message -- this excludes TCP handshake times and internal HTTP request parsing. Basically, it measures how Nchan scales assuming all other components are already tuned for scalability. The graphed data are averages of 5 runs with 50-byte messages.

With a well-tuned OS and network stack on commodity server hardware, expect to handle upwards of 300K concurrent subscribers per second at minimal CPU load. Nchan can also be scaled out to multiple Nginx instances using the [Redis storage engine](#nchan_use_redis), and that too can be scaled up beyond a single-point-of-failure by using [Redis Cluster](#redis-cluster).


## Getting Started

Once you've built and installed Nchan, it's very easy to start using. Add two locations to your nginx config:

```nginx
#...
http {  
  server {
    #...
    
    location = /sub {
      nchan_subscriber;
      nchan_channel_id $arg_id;
    }
    
    location = /pub {
      nchan_publisher;
      nchan_channel_id $arg_id;
    }
  }
}
```

You can now publish messages to channels by `POST`ing data to `/pub?id=channel_id` , and subscribe by pointing Websocket, EventSource, or [NchanSubscriber.js](https://github.com/slact/nchan.js) to `sub/?id=channel_id`. It's that simple.

But Nchan is very flexible and highly configurable. So, of course, it can get a lot more complicated...

### Conceptual Overview

The basic unit of most pub/sub solutions is the messaging *channel*. Nchan is no different. Publishers send messages to channels with a certain *channel id*, and subscribers subscribed to those channels receive them. Some number of messages may be buffered for a time in a channel's message buffer before they are deleted. Pretty simple, right? 

Well... the trouble is that nginx configuration does not deal with channels, publishers, and subscribers. Rather, it has several sections for incoming requests to match against *server* and *location* sections. **Nchan configuration directives map servers and locations onto channel publishing and subscribing endpoints**:

```nginx
#very basic nchan config
worker_processes 5;

http {  
  server {
    listen       80;
    
    location = /sub {
      nchan_subscriber;
      nchan_channel_id foobar;
    }
    
    location = /pub {
      nchan_publisher;
      nchan_channel_id foobar;
    }
  }
}
```

The above maps requests to the URI `/sub` onto the channel `foobar`'s *subscriber endpoint* , and similarly `/pub` onto channel `foobar`'s *publisher endpoint*.


## Publisher Endpoints

Publisher endpoints are Nginx config *locations* with the [*`nchan_publisher`*](#nchan_publisher) directive.

Messages can be published to a channel by sending HTTP **POST** requests with the message contents to the *publisher endpoint* locations. You can also publish messages through a **Websocket** connection to the same location.

```nginx
  location /pub {
    #example publisher location
    nchan_publisher;
    nchan_channel_id foo;
    nchan_channel_group test;
    nchan_message_buffer_length 50;
    nchan_message_timeout 5m;
  }
```

<!-- tag:publisher -->

### Publishing Messages

Requests and websocket messages are responded to with information about the channel at time of message publication. Here's an example from publishing with `curl`:

```console
>  curl --request POST --data "test message" http://127.0.0.1:80/pub

 queued messages: 5
 last requested: 18 sec. ago
 active subscribers: 0
 last message id: 1450755280:0
```

The response can be in plaintext (as above), JSON, or XML, based on the request's *`Accept`* header:

```console
> curl --request POST --data "test message" -H "Accept: text/json" http://127.0.0.2:80/pub

 {"messages": 5, "requested": 18, "subscribers": 0, "last_message_id": "1450755280:0" }
```

Websocket publishers also receive the same responses when publishing, with the encoding determined by the *`Accept`* header present during the handshake.

The response code for an HTTP request is *`202` Accepted* if no subscribers are present at time of publication, or *`201` Created* if at least 1 subscriber was present.

Metadata can be added to a message when using an HTTP POST request for publishing. A `Content-Type` header will be associated as the message's content type (and output to Long-Poll, Interval-Poll, and multipart/mixed subscribers). A `X-EventSource-Event` header can also be used to associate an EventSource `event:` line value with a message.

### Other Publisher Endpoint Actions

**HTTP `GET`** requests return channel information without publishing a message. The response code is `200` if the channel exists, and `404` otherwise:  
```console
> curl --request POST --data "test message" http://127.0.0.2:80/pub
  ...

> curl -v --request GET -H "Accept: text/json" http://127.0.0.2:80/pub

 {"messages": 1, "requested": 7, "subscribers": 0, "last_message_id": "1450755421:0" }
```


**HTTP `DELETE`** requests delete a channel and end all subscriber connections. Like the `GET` requests, this returns a `200` status response with channel info if the channel existed, and a `404` otherwise.

### How Channel Settings Work

*A channel's configuration is set to the that of its last-used publishing location.*
So, if you want a channel to behave consistently, and want to publish to it from multiple locations, *make sure those locations have the same configuration*.

You can also can use differently-configured publisher locations to dynamically update a channel's message buffer settings. This can be used to erase messages or to scale an existing channel's message buffer as desired.

## Subscriber Endpoints

Subscriber endpoints are Nginx config *locations* with the [*`nchan_subscriber`*](#nchan_subscriber) directive.

Nchan supports several different kinds of subscribers for receiving messages: [*Websocket*](#websocket), [*EventSource*](#eventsource) (Server Sent Events),  [*Long-Poll*](#long-polling), [*Interval-Poll*](#interval-polling). [*HTTP chunked transfer*](#http-chunked-transfer), and [*HTTP multipart/mixed*](#http-multipart-mixed).

```nginx
  location /sub {
    #example subscriber location
    nchan_subscriber;
    nchan_channel_id foo;
    nchan_channel_group test;
    nchan_subscriber_first_message oldest;
  }
```

<!-- tag:subscriber -->

- ### Long-Polling
  The tried-and-true server-push method supported by every browser out there.  
  Initiated by sending an HTTP `GET` request to a channel subscriber endpoint.  
  The long-polling subscriber walks through a channel's message queue via the built-in cache mechanism of HTTP clients, namely with the "`Last-Modified`" and "`Etag`" headers. Explicitly, to receive the next message for given a long-poll subscriber response, send a request with the "`If-Modified-Since`" header set to the previous response's "`Last-Modified`" header, and "`If-None-Match`" likewise set to the previous response's "`Etag`" header.  
  Sending a request without a "`If-Modified-Since`" or "`If-None-Match`" headers returns the oldest message in a channel's message queue, or waits until the next published message, depending on the value of the `nchan_subscriber_first_message` config directive.  
  A message's associated content type, if present, will be sent to this subscriber with the `Content-Type` header.
  <!-- tag:subscriber-longpoll -->
  
- ### Interval-Polling
  Works just like long-polling, except if the requested message is not yet available, immediately responds with a `304 Not Modified`.
  Nchan cannot automatically distinguish between long-poll and interval-poll subscriber requests, so long-polling must be disabled for a subscriber location if you wish to use interval-polling.

- ### Websocket
  Bidirectional communication for web browsers. Part of the [HTML5 spec](http://www.w3.org/TR/2014/REC-html5-20141028/single-page.html). Nchan supports the latest protocol version 13 ([RFC 6455](https://tools.ietf.org/html/rfc6455)).  
  Initiated by sending a websocket handshake to the desired subscriber endpoint location.  
  If the websocket connection is closed by the server, the `close` frame will contain the HTTP response code and status line describing the reason for closing the connection. Server-initiated keep-alive pings can be configured with the [`nchan_websocket_ping_interval`](#nchan_websocket_ping_interval) config directive.
  Messages are delivered to subscribers in `text` websocket frames, except if a message's `content-type` is "`application/octet-stream`" -- then it is delivered in a `binary` frame.
  <br />
  Websocket subscribers can use the custom `ws+meta.nchan` subprotocol to receive message metadata with messages, making websocket connections resumable. Messages received with this subprotocol are of the form
  <pre>
  id: message_id
  content-type: message_content_type
  \n
  message_data
  </pre>   
  The `content-type:` line may be omitted.
  <br />
  #### Websocket Publisher
  Messages published through a websocket connection can be forwarded to an upstream application with the [`nchan_publisher_upstream_request`](#nchan_publisher_upstream_request) config directive.   
  Messages published in a binary frame are automatically given the `content-type` "`application/octet-stream`".
  #### Permessage-deflate
  Nchan version 1.1.8 and above supports the [permessage-deflate protocol extension](https://tools.ietf.org/html/rfc7692). Messages are deflated once when they are published, and then can be broadcast to any number of compatible websocket subscribers. Message deflation is enabled by setting the [`nchan_deflate_message_for_websocket on;`](#nchan_deflate_message_for_websocket) directive in a publisher location.
  <br />
  The deflated data is stored alongside the original message in memory, or, if large enough, on disk. This means more [shared memory](#nchan_shared_memory_size) is necessary when using `nchan_deflate_message_for_websocket`.
  <br />
  Deflation parameters (speed, memory use, strategy, etc.), can be tweaked using the [`nchan_permessage_deflate_compression_window`](#nchan_permessage_deflate_compression_window), [`nchan_permessage_deflate_compression_level`](#nchan_permessage_deflate_compression_level),
  [`nchan_permessage_deflate_compression_strategy`](#nchan_permessage_deflate_compression_strategy), and 
  [`nchan_permessage_deflate_compression_window`](#nchan_permessage_deflate_compression_window) settings.
  <br />
  Nchan also supports the (deprecated) [perframe-deflate extension](https://tools.ietf.org/html/draft-tyoshino-hybi-websocket-perframe-deflate-06) still in use by Safari as `x-webkit-perframe-deflate`.
  <br />
  <!-- tag:subscriber-websocket -->
  
- ### EventSource
  Also known as [Server-Sent Events](https://en.wikipedia.org/wiki/Server-sent_events) or SSE, it predates Websockets in the [HTML5 spec](http://www.w3.org/TR/2014/REC-html5-20141028/single-page.html), and is a [very simple protocol](http://www.w3.org/TR/eventsource/#event-stream-interpretation).  
  Initiated by sending an HTTP `GET` request to a channel subscriber endpoint with the "`Accept: text/event-stream`" header.    
  Each message `data: ` segment will be prefaced by the message `id: `.  
  To resume a closed EventSource connection from the last-received message, one *should* start the connection with the "`Last-Event-ID`" header set to the last message's `id`.  
  Unfortunately, browsers [don't support setting](http://www.w3.org/TR/2011/WD-eventsource-20111020/#concept-event-stream-last-event-id) this header for an `EventSource` object, so by default the last message id is set either from the "`Last-Event-Id`" header or the `last_event_id` url query string argument.  
  This behavior can be configured via the [`nchan_subscriber_last_message_id`](#nchan_subscriber_last_message_id) config.  
  A message's `content-type` will not be received by an EventSource subscriber, as the protocol makes no provisions for this metadata.
  A message's associated `event` type, if present, will be sent to this subscriber with the `event:` line.  
  <!-- tag:subscriber-eventsource -->
  
- ### HTTP [multipart/mixed](http://www.w3.org/Protocols/rfc1341/7_2_Multipart.html#z0)
  The `multipart/mixed` MIMEtype was conceived for emails, but hey, why not use it for HTTP? It's easy to parse and includes metadata with each message.  
  Initiated by including an `Accept: multipart/mixed` header.  
  The response headers and the unused "preamble" portion of the response body are sent right away, with the boundary string generated randomly for each subscriber.  Each subsequent message will be sent as one part of the multipart message, and will include the message time and tag (`Last-Modified` and `Etag`) as well as the optional `Content-Type` headers.  
  Each message is terminated with the next multipart message's boundary **without a trailing newline**. While this conforms to the multipart spec, it is unusual as multipart messages are defined as *starting*, rather than ending with a boundary.  
  A message's associated content type, if present, will be sent to this subscriber with the `Content-Type` header.
  <!-- tag:subscriber-multipart -->
  
- ### HTTP Raw Stream
  A simple subscription method similar to the [streaming subscriber](https://github.com/wandenberg/nginx-push-stream-module/blob/master/docs/directives/subscribers.textile#push_stream_subscriber) of the [Nginx HTTP Push Stream Module](https://github.com/wandenberg/nginx-push-stream-module). Messages are appended to the response body, separated by a newline or configurable by `nchan_subscriber_http_raw_stream_separator`.
  <!-- tag:subscriber-rawstream -->

- ### HTTP [Chunked Transfer](http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.6.1)
  This subscription method uses the `chunked` `Transfer-Encoding` to receive messages.   
  Initiated by explicitly including `chunked` in the [`TE` header](http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.39):  
  `TE: chunked` (or `TE: chunked;q=??` where the qval > 0)  
  The response headers are sent right away, and each message will be sent as an individual chunk. Note that because a zero-length chunk terminates the transfer, **zero-length messages will not be sent** to the subscriber.  
  Unlike the other subscriber types, the `chunked` subscriber cannot be used with http/2 because it disallows chunked encoding.
  <!-- tag:subscriber-chunked -->

## PubSub Endpoint  

PubSub endpoints are Nginx config *locations* with the [*`nchan_pubsub`*](#nchan_pubsub) directive.

A combination of *publisher* and *subscriber* endpoints, this location treats all HTTP `GET`
requests as subscribers, and all HTTP `POST` as publishers. Channels cannot be deleted through a pubsub endpoing with an HTTP `DELETE` request.

One simple use case is an echo server:

```nginx
  location = /pubsub {
    nchan_pubsub;
    nchan_channel_id foo;
    nchan_channel_group test;
  }
```

A more interesting setup may set different publisher and subscriber channel ids:

```nginx
  location = /pubsub {
    nchan_pubsub;
    nchan_publisher_channel_id foo;
    nchan_subscriber_channel_id bar;
    nchan_channel_group test;
  }
```

Here, subscribers will listen for messages on channel `foo`, and publishers will publish messages to channel `bar`. This can be useful when setting up websocket proxying between web clients and your application.
<!-- tag:pubsub -->

## The Channel ID

So far the examples have used static channel ids, which is not very useful. In practice, the channel id can be set to any nginx *variable*, such as a querystring argument, a header value, or a part of the location url:

```nginx
  location = /sub_by_ip {
    #channel id is the subscriber's IP address
    nchan_subscriber;
    nchan_channel_id $remote_addr;
  }
  
  location /sub_by_querystring {
    #channel id is the query string parameter chanid
    # GET /sub/sub_by_querystring?foo=bar&chanid=baz will have the channel id set to 'baz'
    nchan_subscriber;
    nchan_channel_id $arg_chanid;
  }

  location ~ /sub/(\w+)$ {
    #channel id is the word after /sub/
    # GET /sub/foobar_baz will have the channel id set to 'foobar_baz'
    # I hope you know your regular expressions...
    nchan_subscriber;
    nchan_channel_id $1; #first capture of the location match
  }
```

I recommend using the last option, a channel id derived from the request URL via a regular expression. It makes things nice and RESTful.

<!-- tag:channel-id -->

### Channel Multiplexing

With channel multiplexing, subscribers can subscribe to up to 255 channels per connection. Messages published to all the specified channels will be delivered in-order to the subscriber. There are two ways to enable multiplexing:

Up to 7 channel ids can be specified for the `nchan_channel_id` or `nchan_channel_subscriber_id` config directive:

```nginx
  location ~ /multisub/(\w+)/(\w+)$ {
    nchan_subscriber;
    nchan_channel_id "$1" "$2" "common_channel";
    #GET /multisub/foo/bar will be subscribed to:
    # channels 'foo', 'bar', and 'common_channel',
    #and will receive messages from all of the above.
  }
```

For more than 7 channels, `nchan_channel_id_split_delimiter` can be used to split the `nchan_channel_id` or `nchan_channel_subscriber_id` into up to 255 individual channel ids:

```nginx
  location ~ /multisub-split/(.*)$ {
    nchan_subscriber;
    nchan_channel_id "$1";
    nchan_channel_id_split_delimiter ",";
    #GET /multisub-split/foo,bar,baz,a will be subscribed to:
    # channels 'foo', 'bar', 'baz', and 'a'
    #and will receive messages from all of the above.
  }
```

It is also possible to publish to multiple channels with a single request as well as delete multiple channels with a single request, with similar configuration:

```nginx
  location ~ /multipub/(\w+)/(\w+)$ {
    nchan_publisher;
    nchan_channel_id "$1" "$2" "another_channel";
    #POST /multipub/foo/bar will publish to:
    # channels 'foo', 'bar', 'another_channel'
    #DELETE /multipub/foo/bar will delete:
    # channels 'foo', 'bar', 'another_channel'
  }
```

When a channel is deleted, all of its messages are deleted, and all of its subscribers' connection are closed -- including ones subscribing through a multiplexed location. For example, suppose a subscriber is subscribed to channels "foo" and "bar" via a single multiplexed connection. If "foo" is deleted, the connection is closed, and the subscriber therefore loses the "bar" subscription as well.

See the [Channel Security](#securing-channels) section about using good IDs and keeping private channels secure.

<!-- tag:channel-multiplexing -->

### Channel Groups

Channels can be associated with groups to avoid channel ID conflicts:

```nginx
  location /test_pubsub {
    nchan_pubsub;
    nchan_channel_group "test";
    nchan_channel_id "foo";
  }
  
  location /pubsub {
    nchan_pubsub;
    nchan_channel_group "production";
    nchan_channel_id "foo";
    #same channel id, different channel group. Thus, different channel.
  }
  
  location /flexgroup_pubsub {
    nchan_pubsub;
    nchan_channel_group $arg_group;
    nchan_channel_id "foo";
    #group can be set with request variables too
  }
```

#### Limits and Accounting

Groups can be used to track aggregate channel usage, as well as set limits on the number of channels, subscribers, stored messages, memory use, etc:

```nginx
  #enable group accounting
  nchan_channel_group_accounting on;
  
  location ~ /pubsub/(\w+)$ {
    nchan_pubsub;
    nchan_channel_group "limited";
    nchan_channel_id $1;
  }
  
  location ~ /prelimited_pubsub/(\w+)$ {
    nchan_pubsub;
    nchan_channel_group "limited";
    nchan_channel_id $1;
    nchan_group_max_subscribers 100;
    nchan_group_max_messages_memory 50M;
  }
  
  location /group {
    nchan_channel_group limited;
    nchan_group_location;
    nchan_group_max_channels $arg_max_channels;
    nchan_group_max_messages $arg_max_messages;
    nchan_group_max_messages_memory $arg_max_messages_mem;
    nchan_group_max_messages_disk $arg_max_messages_disk;
    nchan_group_max_subscribers $arg_max_subs;
  }
```

Here, `/group` is an `nchan_group_location`, which is used for accessing and modifying group data. To get group data, send a `GET` request to a `nchan_group_location`:

```sh
>  curl http://localhost/group

channels: 10
subscribers: 0
messages: 219
shared memory used by messages: 42362 bytes
disk space used by messages: 0 bytes
limits:
  max channels: 0
  max subscribers: 0
  max messages: 0
  max messages shared memory: 0
  max messages disk space: 0  
```

By default, the data is returned in human-readable plaintext, but can also be formatted as JSON, XML, or YAML:

```sh
>  curl -H "Accept: text/json" http://localhost/group

{
  "channels": 21,
  "subscribers": 40,
  "messages": 53,
  "messages_memory": 19941,
  "messages_disk": 0,
  "limits": {
    "channels": 0,
    "subscribers": 0,
    "messages": 0,
    "messages_memory": 0,
    "messages_disk": 0
  }
}
```

The data in the response are for the single Nchan instance only, regardless of whether Redis is used. A limit of 0 means 'unlimited'.

Limits can be set per-location, as with the above `/prelimited_pubsub/...` location, or with a POST request to the `nchan_group_location`:
```sh
>  curl -X POST "http://localhost/group?max_channels=15&max_subs=1000&max_messages_disk=0.5G"

channels: 0
subscribers: 0
messages: 0
shared memory used by messages: 0 bytes
disk space used by messages: 0 bytes
limits:
  max channels: 15
  max subscribers: 1000
  max messages: 0
  max messages shared memory: 0
  max messages disk space: 536870912

```

Limits are only applied locally, regardless of whether Redis is enabled. 
If a publisher or subscriber request exceeds a group limit, Nchan will respond to it with a `403 Forbidden` response.

<!-- tag:group -->

## Hooks and Callbacks

<!-- tag:hook -->
  
### Request Authorization

This feature, configured with [`nchan_authorize_request`](#nchan_authorize_request), behaves just like the Nginx [http_auth_request module](http://nginx.org/en/docs/http/ngx_http_auth_request_module.html#auth_request_set).

Consider the configuration:
```nginx
  upstream my_app {
    server 127.0.0.1:8080;
  }
  location = /auth {
    proxy_pass http://my_app/pubsub_authorize;
    proxy_pass_request_body off;
    proxy_set_header Content-Length "";
    proxy_set_header X-Subscriber-Type $nchan_subscriber_type;
    proxy_set_header X-Publisher-Type $nchan_publisher_type;
    proxy_set_header X-Prev-Message-Id $nchan_prev_message_id;
    proxy_set_header X-Channel-Id $nchan_channel_id;
    proxy_set_header X-Original-URI $request_uri;
    proxy_set_header X-Forwarded-For $remote_addr;
  }
  
  location ~ /pubsub/auth/(\w+)$ {
    nchan_channel_id $1;
    nchan_authorize_request /auth;
    nchan_pubsub;
    nchan_channel_group test;
  }
```

Here, any request to the location `/pubsub/auth/<...>` will need to be authorized by your application (`my_app`). Nginx will generate a `GET /pubsub_authorize` request to the application, with additional headers set by the `proxy_set_header` directives. Note that Nchan-specific variables are available for this authorization request. Once your application receives this request, it should decide whether or not to authorize the subscriber. This can be done based on a forwarded session cookie, IP address, or any set of parameters of your choosing. If authorized, it should respond with an empty `200 OK` response.  
All non-`2xx` response codes (such as `403 Forbidden`) are interpreted as authorization failures. In this case, the failing response is proxied to the client. 

Note that Websocket and EventSource clients will only try to authorize during the initial handshake request, whereas Long-Poll and Interval-Poll subscribers will need to be authorized each time they request the next message, which may flood your application with too many authorization requests.

<!-- commands: nchan_authorize_request -->

### Subscriber Presence

Subscribers can notify an application when they have subscribed and unsubscribed to a channel using the [`nchan_subscribe_request`](#nchan_subscribe_request)
and [`nchan_unsubscribe_request`](#nchan_unsubscribe_request) settings. 
These should point to Nginx locations configured to forward requests to an upstream proxy (your application):

```nginx
  location ~ /sub/(\w+)$ {
    nchan_channel_id $1;
    nchan_subscribe_request /upstream/sub;
    nchan_unsubscribe_request /upstream/unsub;
    nchan_subscriber;
    nchan_channel_group test;
  }

  location = /upstream/unsub {
    proxy_pass http://127.0.0.1:9292/unsub;
    proxy_ignore_client_abort on;  #!!!important!!!!
    proxy_set_header X-Subscriber-Type $nchan_subscriber_type;
    proxy_set_header X-Channel-Id $nchan_channel_id;
    proxy_set_header X-Original-URI $request_uri;
  } 
  location = /upstream/sub {
    proxy_pass http://127.0.0.1:9292/sub;
    proxy_set_header X-Subscriber-Type $nchan_subscriber_type;
    proxy_set_header X-Message-Id $nchan_message_id;
    proxy_set_header X-Channel-Id $nchan_channel_id;
    proxy_set_header X-Original-URI $request_uri;
  } 
```

In order for `nchan_unsubscribe_request` to work correctly, the location it points to must have `proxy_ignore_client_abort on;`. Otherwise, suddenly aborted subscribers may not trigger an unsubscribe request.

Note that the subscribe/unsubscribe hooks are **disabled for long-poll and interval-poll clients**, because they would trigger these hooks each time they receive a message.

<!-- commands: nchan_subscribe_request nchan_unsubscribe_request -->

### Message Forwarding

Messages can be forwarded to an upstream application before being published using the `nchan_publisher_upstream_request` setting:

```nginx
  location ~ /pub/(\w+)$ {
    #publisher endpoint
    nchan_channel_id $1;
    nchan_pubsub;
    nchan_publisher_upstream_request /upstream_pub;
  }
  
  location = /upstream_pub {
    proxy_pass http://127.0.0.1:9292/pub;
    proxy_set_header X-Publisher-Type $nchan_publisher_type;
    proxy_set_header X-Prev-Message-Id $nchan_prev_message_id;
    proxy_set_header X-Channel-Id $nchan_channel_id;
    proxy_set_header X-Original-URI $request_uri;
  } 
```
With this configuration, incoming messages are first `POST`ed to `http://127.0.0.1:9292/pub`.
The upstream response code determines how publishing will proceed:
  - `304 Not Modified` publishes the message as received, without modifification.
  - `204 No Content` discards the message
  - `200 OK` is used for modifying the message. Instead of the original incoming message, the message contained in this HTTP response is published.

There are two main use cases for `nchan_publisher_upstream_request`: forwarding incoming data from Websocket publishers to an application, and mutating incoming messages.

<!-- commands: nchan_publisher_upstream_request -->

## Storage

Nchan can stores messages in memory, on disk, or via Redis. Memory storage is much faster, whereas Redis has additional overhead as is considerably slower for publishing messages, but offers near unlimited scalability for broadcast use cases with far more subscribers than publishers.

### Memory Storage

This default storage method uses a segment of shared memory to store messages and channel data. Large messages as determined by Nginx's caching layer are stored on-disk. The size of the memory segment is configured with `nchan_shared_memory_size`. Data stored here is not persistent, and is lost if Nginx is restarted or reloaded.

<!-- tag:memstore -->

### Redis

[Redis](http://redis.io) can be used to add **data persistence** and **horizontal scalability**, **failover** and **high availability** to your Nchan setup. 

<!-- tag:redis -->

#### Connecting to a Redis Server
To connect to a single Redis master server, use an `upstream` with `nchan_redis_server` and `nchan_redis_pass` settings:

```nginx
http {
  upstream my_redis_server {
    nchan_redis_server 127.0.0.1;
  }
  server {
    listen 80;
    
    location ~ /redis_sub/(\w+)$ {
      nchan_subscriber;
      nchan_channel_id $1;
      nchan_redis_pass my_redis_server;
    }
    location ~ /redis_pub/(\w+)$ {
      nchan_redis_pass my_redis_server;
      nchan_publisher;
      nchan_channel_id $1;
    }
  }
} 
```

All servers with the above configuration connecting to the same redis server share channel and message data.

Channels that don't use Redis can be configured side-by-side with Redis-backed channels, provided the endpoints never overlap. (This can be ensured, as above, by setting separate `nchan_channel_group`s.). Different locations can also connect to different Redis servers.

Nchan can work with a single Redis master. It can also auto-discover and use Redis slaves to balance PUBSUB traffic.

<!-- commands: nchan_redis_server nchan_redis_pass -->

#### Redis Cluster
Nchan also supports using Redis Cluster, which adds scalability via sharding channels among cluster nodes. Redis cluster also provides **automatic failover**, **high availability**, and eliminates the single point of failure of one shared Redis server. It is configured and used like so:

```nginx
http {
  upstream redis_cluster {
    nchan_redis_server redis://127.0.0.1:7000;
    nchan_redis_server redis://127.0.0.1:7001;
    nchan_redis_server redis://127.0.0.1:7002;
    # you don't need to specify all the nodes, they will be autodiscovered
    # however, it's recommended that you do specify at least a few master nodes.
  }
  server {
    listen 80;
    
    location ~ /sub/(\w+)$ {
      nchan_subscriber;
      nchan_channel_id $1;
      nchan_redis_pass redis_cluster;
    }
    location ~ /pub/(\w+)$ {
      nchan_publisher;
      nchan_channel_id $1;
      nchan_redis_pass redis_cluster;
    }
  }
} 
```

<!-- commands: nchan_redis_server nchan_redis_pass -->

##### High Availability
Redis Cluster connections are designed to be resilient and try to recover from errors. Interrupted connections will have their commands queued until reconnection, and Nchan will publish any messages it successfully received while disconnected. Nchan is also adaptive to cluster modifications. It will add new nodes and remove them as needed.

All Nchan servers sharing a Redis server or cluster should have their times synchronized (via ntpd or your favorite ntp daemon). Failure to do so may result in missed or duplicate messages.

##### Failover Recovery
Starting with version 1.3.0, Nchan will attempt to recover from cluster node failures, keyslot errors, and cluster epoch changes without disconnecting from the entire cluster. It will attempt to do this until [`nchan_redis_cluster_max_failing_time`](#nchan_redis_cluster_max_failing_time) is exceeded. Additionally, [recovery attempt delays](#nchan_redis_cluster_recovery_delay) have configurable [jitter](#nchan_redis_cluster_recovery_delay_jitter), [exponential backoff](#nchan_redis_cluster_recovery_delay_backoff), and [maximum](#nchan_redis_cluster_recovery_delay_max) values.

#### Using Redis securely

Redis servers can be connected to via TLS by using the [`nchan_redis_ssl`](#nchan_redis_ssl) config setting in an `upstream` block, or by using the `rediss://`  schema for the server URLs.

A password and optional username for the `AUTH` command can be set by the [`nchan_redis_username`](#nchan_redis_username) and [`nchan_redis_password`](#nchan_redis_password) config settings in an `upstream` block, or by using the `redis://<username>:<password>@hostname` server URL schema.

Note that autodiscovered Redis nodes inherit their parent's SSL, username, and password settings.

#### Tweaks and Optimizations

As of version 1.2.0, Nchan uses Redis slaves to load-balance PUBSUB traffic. By default, there is an equal chance that a channel's PUBSUB subscription will go to any master or slave. The [`nchan_redis_subscribe_weights`](#nchan_redis_subscribe_weights) setting is available to fine-tune this load-balancing.

Also from 1.2.0 onward, [`nchan_redis_optimize_target`](#nchan_redis_optimize_target) can be used to prefer optimizing Redis slaves for CPU or bandwidth. For heavy publishing loads, the tradeoff is very roughly 35% replication bandwidth per slave to 30% CPU load on slaves.

#### Performance Statistics

Redis command statistics were added in version 1.3.5. These provide total number of times different Redis commands were run on, and the total amount of time they took. The stats are for a given Nchan server, *not* all servers connected to a Redis upstream. They are grouped by each upstream, and totaled per node.

```nginx
http {
  upstream my_redis_cluster {
    nchan_redis_server 127.0.0.1;
  }
  
  server {
    #[...]
    
    location ~ /nchan_redis_cluster_stats$ {
      nchan_redis_upstream_stats my_redis_cluster;
    }
  }

```

To get the stats, send a GET request to the stats location.

```console
  curl http://localhost/nchan_redis_cluster_stats
```

The response is JSON of the form:

```js
{
  "upstream": "redis_cluster",
  "nodes": [
    {
      "address"        : "127.0.0.1:7000",
      "id"             : "f13d71b1d14d8bf92b72cebee61421294e95dc72",
      "command_totals" : {
        "connect"    : {
          "msec"     : 357,
          "times"    : 5
        },
        "pubsub_subscribe": {
          "msec"     : 749,
          "times"    : 37
        },
        "pubsub_unsubsribe": {
          "msec"     : 332,
          "times"    : 37
        }
        /*[...]*/
      }
    },
    {
      "address"        : "127.0.0.1:7001",
      "id"             : "b768ecb4152912bed6dc927e8f70284191a79ed7",
      "command_totals" : {
        "connect"    : {
          "msec"     : 4281,
          "times"    : 5
        },
        "pubsub_subscribe": {
          "msec"     : 309,
          "times"    : 33
        },
        "pubsub_unsubsribe": {
          "msec"     : 307,
          "times"    : 30
        },
        /*[...]*/
      },
    }
    /*[...]*/
  ]
}
```

For brevity, the entire `command_totals` hash is omitted in this documentation.

<!-- commands: nchan_redis_upstream_stats nchan_redis_upstream_stats_disconnected_timeout nchan_redis_upstream_stats_enabled -->

## Introspection

There are several ways to see what's happening inside Nchan. These are useful for debugging application integration and for measuring performance.

### Channel Events

Channel events are messages automatically published by Nchan when certain events occur in a channel. These are very useful for debugging the use of channels. However, they carry a significant performance overhead and should be used during development, and not in production.

Channel events are published to special 'meta' channels associated with normal channels. Here's how to configure them:

```nginx
location ~ /pubsub/(.+)$ {
  nchan_pubsub;
  nchan_channel_id $1;
  nchan_channel_events_channel_id $1; #enables channel events for this location
}

location ~ /channel_events/(.+) {
  #channel events subscriber location
  nchan_subscriber;
  nchan_channel_group meta; #"meta" is a SPECIAL channel group
  nchan_channel_id $1;
}
```

Note the `/channel_events/...` location has a *special* `nchan_channel_group`, `meta`. This group is reserved for accessing "channel events channels", or"metachannels".

Now, say I subscribe to `/channel_events/foo` I will refer to this as the channel events subscriber.

Let's see what this channel events subscriber receives when I publish messages to 

Subscribing to `/pubsub/foo` produces the channel event
```
subscriber_enqueue foo
```

Publishing a message to `/pubsub/foo`:
```
channel_publish foo
```

Unsubscribing from `/pubsub/foo`:
```
subscriber_dequeue foo
```

Deleting `/pubsub/foo` (with HTTP `DELETE /pubsub/foo`):
```
channel_delete foo
```

The event string itself is configirable with [nchan_channel_event_string](#nchan_channel_event_string). By default, it is set to `$nchan_channel_event $nchan_channel_id`. 
This string can use any Nginx and [Nchan variables](/#variables).


### nchan_stub_status Stats

Like Nginx's [stub_status](https://nginx.org/en/docs/http/ngx_http_stub_status_module.html),
`nchan_stub_status` is used to get performance metrics.

```nginx
  location /nchan_stub_status {
    nchan_stub_status;
  }
```

Sending a GET request to this location produces the response:

```text
total published messages: 1906
stored messages: 1249
shared memory used: 1824K
channels: 80
subscribers: 90
redis pending commands: 0
redis connected servers: 0
redis unhealthy upstreams: 0
total redis commands sent: 0
total interprocess alerts received: 1059634
interprocess alerts in transit: 0
interprocess queued alerts: 0
total interprocess send delay: 0
total interprocess receive delay: 0
nchan version: 1.1.5
```

Here's what each line means, and how to interpret it:
  - `total published messages`: Number of messages published to all channels through this Nchan server.
  - `stored messages`: Number of messages currently buffered in memory
  - `shared memory used`: Total shared memory used for buffering messages, storing channel information, and other purposes. This value should be comfortably below `nchan_shared_memory_size`.
  - `channels`: Number of channels present on this Nchan server.
  - `subscribers`: Number of subscribers to all channels on this Nchan server.
  - `redis pending commands`: Number of commands sent to Redis that are awaiting a reply. May spike during high load, especially if the Redis server is overloaded. Should tend towards 0.
  - `redis connected servers`: Number of redis servers to which Nchan is currently connected.
  - `redis unhealthy upstreams`: Number of redis upstreams (individual server or cluster mode) that are currently not usable for publishing and subscribing.
  - `total redis commands sent`: Total number of commands this Nchan instance sent to Redis.
  - `total interprocess alerts received`: Number of interprocess communication packets transmitted between Nginx workers processes for Nchan. Can grow at 100-10000 per second at high load.
  - `interprocess alerts in transit`: Number of interprocess communication packets in transit between Nginx workers. May be nonzero during high load, but should always tend toward 0 over time.
  - `interprocess queued alerts`: Number of interprocess communication packets waiting to be sent. May be nonzero during high load, but should always tend toward 0 over time.
  - `total interprocess send delay`: Total amount of time interprocess communication packets spend being queued if delayed. May increase during high load.
  - `total interprocess receive delay`: Total amount of time interprocess communication packets spend in transit if delayed. May increase during high load.
  - `nchan_version`: current version of Nchan. Available for version 1.1.5 and above.

Additionally, when there is at least one `nchan_stub_status` location, this data is also available [through variables](#variables).
  
## Securing Channels

### Securing Publisher Endpoints
Consider the use case of an application where authenticated users each use a private, dedicated channel for live updates. The configuration might look like this:

```nginx
http {
  server {
    #available only on localhost
    listen  127.0.0.1:8080;
    location ~ /pub/(\w+)$ {
      nchan_publisher;
      nchan_channel_group my_app_group;
      nchan_channel_id $1;
    }
  }
  
  server {
    #available to the world
    listen 80;
    
    location ~ /sub/(\w+)$ {
      nchan_subscriber;
      nchan_channel_group my_app_group;
      nchan_channel_id $1;
    }
  }
}

```

Here, the subscriber endpoint is available on a public-facing port 80, and the publisher endpoint is only available on localhost, so can be accessed only by applications residing on that machine. Another way to limit access to the publisher endpoint is by using the allow/deny settings:

```nginx

  server {
    #available to the world
    listen 80; 
    location ~ /pub/(\w+)$ {
      allow 127.0.0.1;
      deny all;
      nchan_publisher;
      nchan_channel_group my_app_group;
      nchan_channel_id $1;
    }
```

Here, only the local IP 127.0.0.1 is allowed to use the publisher location, even though it is defined in a non-localhost server block.

### Keeping a Channel Private

A Channel ID that is meant to be private should be treated with the same care as a session ID token. Considering the above use case of one-channel-per-user, how can we ensure that only the authenticated user, and no one else, is able to access his channel? 

First, if you intend on securing the channel contents, you must use TLS/SSL:

```nginx 
http {
  server {
    #available only on localhost
    listen  127.0.0.1:8080;
    #...publisher endpoint config
  }
  server {
    #available to the world
    listen 443 ssl;
    #SSL config goes here
    location ~ /sub/(\w+)$ {
      nchan_subscriber;
      nchan_channel_group my_app_group;
      nchan_channel_id $1;
    }
  }
}
```

Now that you have a secure connection between the subscriber client and the server, you don't need to worry about the channel ID or messages being passively intercepted. This is a minimum requirement for secure message delivery, but it is not sufficient. 

You must also take care to do at least one of the following:
  - [Generate good, high-entropy Channel IDs](#good-ids).
  - [Authorize all subscribers with the `nchan_authorize_request` config directive](#request-authorization).
  - [Authorize subscribers and hide channel IDs with the "`X-Accel-Redirect`" mechanism](#x-accel-redirect).
  
#### Good IDs

An ID that can be guessed is an ID that can be hijacked. If you are not authenticating subscribers (as described below), a channel ID should be impossible to guess. Use at least 128 bits of entropy to generate a random token, associate it with the authenticated user, and share it only with the user's client. Do not reuse tokens, just as you would not reuse session IDs.

#### X-Accel-Redirect

This feature uses the [X-Accel feature](https://www.nginx.com/resources/wiki/start/topics/examples/x-accel) of Nginx upstream proxies to perform an internal request to a subscriber endpoint.
It allows a subscriber client to be authenticated by your application, and then redirected by nginx internally to a location chosen by your application (such as a publisher or subscriber endpoint). This makes it possible to have securely authenticated clients that are unaware of the channel id they are subscribed to.

Consider the following configuration:
```nginx 
upstream upstream_app {
  server 127.0.0.1:8080;
}

server {
  listen 80; 
  
  location = /sub_upstream {
    proxy_pass http://upstream_app/subscriber_x_accel_redirect;
    proxy_set_header X-Forwarded-For $remote_addr;
  }
  
  location ~ /sub/internal/(\w+)$ {
    internal; #this location only accessible for internal nginx redirects
    nchan_subscriber;
    nchan_channel_id $1;
    nchan_channel_group test;
  }
}
```

As commented, `/sub/internal/` is inaccessible from the outside:
```console
> curl  -v  http://127.0.0.1/sub/internal/foo
  
  < HTTP/1.1 404 Not Found
  < Server: nginx/1.9.5
  <
  <html>
  <head><title>404 Not Found</title></head>
  <body bgcolor="white">
  <center><h1>404 Not Found</h1></center>
  <hr><center>nginx/1.9.5</center>
  </body>
  </html>
```

But if a request is made to `/sub_upstream`, it gets forwarded to your application (`my_app`) on port 8080 with the url `/subscriber_x_accel_redirect`.
Note that you can set any forwarded headers here like any [`proxy_pass`](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass) Nginx location, 
but unlike the case with `nchan_authorize_request`, Nchan-specific variables are not available.

Now, your application must be set up to handle the request to `/subscriber_x_accel_redirect`. You should make sure the client is properly authenticated (maybe using a session cookie), and generate an associated channel id. If authentication fails, respond with a normal `403 Forbidden` response. You can also pass extra information about the failure in the response body and headers.

If your application successfully authenticates the subscriber request, you now need to instruct Nginx to issue an internal redirect to `/sub/internal/my_channel_id`.
This is accomplished by responding with an empty `200 OK` response that includes two headers:
- `X-Accel-Redirect: /sub/internal/my_channel_id`
- `X-Accel-Buffering: no`

In the presence of these headers, Nginx will not forward your app's response to the client, and instead will *internally* redirect to `/sub/internal/my_channel_id`. 
This will behave as if the client had requested the subscriber endpoint location directly.

Thus using X-Accel-Redirect it is possible to both authenticate all subscribers *and* keep channel IDs completely hidden from subscribers.

This method is especially useful for EventSource and Websocket subscribers. Long-Polling subscribers will need to be re-authenticated for every new message, which may flood your application with too many authentication requests.

### Revoking Channel Authorization

In some cases, you may want to revoke a particular subscriber's authorization for a given channel (e.g., if the user's permissions are changed). If the channel is unique to the subscriber, this is simply accomplished by deleting the channel. The same can be achieved for shared channels by subscribing each subscriber to both the shared channel and a subscriber-specific channel via a multiplexed connection. Deleting the subscriber-specific channel will terminate the subscriber''s connection, thereby also terminating their subscription to the shared channel. Consider the following configuration:

```nginx
location ~ /sub/(\w+) {
  nchan_subscriber;
  nchan_channel_id shared_$1 user_$arg_userid;
  nchan_authorize_request /authorize;
}

location /pub/user {
  nchan_publisher;
  nchan_channel_id user_$arg_userid;
}
```

A request to `/sub/foo?userid=1234` will subscribe to channels "shared_foo" and "user_1234" via a multiplexed connection. If you later send a `DELETE` request to `/pub/user?userid=1234`, this subscriber will be disconnected and therefore unsubscribed from both "user_1234" and "shared_foo".
  
## Variables

Nchan makes several variables usabled in the config file:
 
- `$nchan_channel_id`  
  The channel id extracted from a publisher or subscriber location request. For multiplexed locations, this is the first channel id in the list.

- `$nchan_channel_id1`, `$nchan_channel_id2`, `$nchan_channel_id3`, `$nchan_channel_id4`  
  As above, but for the nth channel id in multiplexed channels.

- `$nchan_subscriber_type`  
  For subscriber locations, this variable is set to the subscriber type (websocket, longpoll, etc.).

- `$nchan_channel_subscriber_last_seen`  
  For publisher locations, this variable is set to the timestamp for the last connected subscriber.
  
- `$nchan_channel_subscriber_count`  
  For publisher locations, this variable is set to the number of subscribers in the published channel.
  
- `$nchan_channel_message_count`  
  For publisher locations, this variable is set to the number of messages buffered in the published channel.
  
- `$nchan_publisher_type`  
  For publisher locations, this variable is set to the subscriber type (http or websocket).
  
- `$nchan_prev_message_id`, `$nchan_message_id`  
  The current and previous (if applicable) message id for publisher request or subscriber response.

- `$nchan_channel_event`  
  For channel events, this is the event name. Useful when configuring `nchan_channel_event_string`.

- `$nchan_version`  
  Current Nchan version. Available since 1.1.5.
  
Additionally, `nchan_stub_status` data is also exposed as variables. These are available only when `nchan_stub_status` is enabled on at least one location:

- `$nchan_stub_status_total_published_messages`  
- `$nchan_stub_status_stored_messages`  
- `$nchan_stub_status_shared_memory_used`  
- `$nchan_stub_status_channels`  
- `$nchan_stub_status_subscribers`  
- `$nchan_stub_status_redis_pending_commands`  
- `$nchan_stub_status_redis_connected_servers`  
- `$nchan_stub_status_redis_unhealthy_upstreams`  
- `$nchan_stub_status_total_ipc_alerts_received`  
- `$nchan_stub_status_ipc_queued_alerts`  
- `$nchan_stub_status_total_ipc_send_delay`  
- `$nchan_stub_status_total_ipc_receive_delay`  


## Configuration Directives

- **nchan_channel_id**  
  arguments: 1 - 7  
  default: `(none)`  
  context: server, location, if  
  > Channel id for a publisher or subscriber location. Can have up to 4 values to subscribe to up to 4 channels.    
  [more details](#the-channel-id)  

- **nchan_channel_id_split_delimiter**  
  arguments: 1  
  default: `(none)`  
  context: server, location, if  
  > Split the channel id into several ids for multiplexing using the delimiter string provided.    
  [more details](#channel-multiplexing)  

- **nchan_deflate_message_for_websocket** `[ on | off ]`  
  arguments: 1  
  default: `off`  
  context: server, location  
  > Store a compressed (deflated) copy of the message along with the original to be sent to websocket clients supporting the permessage-deflate protocol extension    

- **nchan_eventsource_event**  
  arguments: 1  
  default: `(none)`  
  context: server, location, if  
  > Set the EventSource `event:` line to this value. When used in a publisher location, overrides the published message's `X-EventSource-Event` header and associates the message with the given value. When used in a subscriber location, overrides all messages' associated `event:` string with the given value.    

- **nchan_eventsource_ping_comment**  
  arguments: 1  
  default: `(empty)`  
  context: server, location, if  
  > Set the EventSource comment `: ...` line for periodic pings from server to client. Newlines are not allowed. If empty, no comment is sent with the ping.    

- **nchan_eventsource_ping_data**  
  arguments: 1  
  default: `(empty)`  
  context: server, location, if  
  > Set the EventSource `data:` line for periodic pings from server to client. Newlines are not allowed. If empty, no data is sent with the ping.    

- **nchan_eventsource_ping_event**  
  arguments: 1  
  default: `ping`  
  context: server, location, if  
  > Set the EventSource `event:` line for periodic pings from server to client. Newlines are not allowed. If empty, no event type is sent with the ping.    

- **nchan_eventsource_ping_interval** `<number> (seconds)`  
  arguments: 1  
  default: `0 (none)`  
  context: server, location, if  
  > Interval for sending ping messages to EventSource subscribers. Disabled by default.    

- **nchan_longpoll_multipart_response** `[ off | on | raw ]`  
  arguments: 1  
  default: `off`  
  context: server, location, if  
  > when set to 'on', enable sending multiple messages in a single longpoll response, separated using the multipart/mixed content-type scheme. If there is only one available message in response to a long-poll request, it is sent unmodified. This is useful for high-latency long-polling connections as a way to minimize round-trips to the server. When set to 'raw', sends multiple messages using the http-raw-stream message separator.    

- **nchan_permessage_deflate_compression_level** `[ 0-9 ]`  
  arguments: 1  
  default: `6`  
  context: http  
  > Compression level for the `deflate` algorithm used in websocket's permessage-deflate extension. 0: no compression, 1: fastest, worst, 9: slowest, best    

- **nchan_permessage_deflate_compression_memlevel** `[ 1-9 ]`  
  arguments: 1  
  default: `8`  
  context: http  
  > Memory level for the `deflate` algorithm used in websocket's permessage-deflate extension. How much memory should be allocated for the internal compression state. 1 - minimum memory, slow and reduces compression ratio; 9 - maximum memory for optimal speed    

- **nchan_permessage_deflate_compression_strategy** `[ default | filtered | huffman-only | rle | fixed ]`  
  arguments: 1  
  default: `default`  
  context: http  
  > Compression strategy for the `deflate` algorithm used in websocket's permessage-deflate extension. Use 'default' for normal data, For details see [zlib's section on copression strategies](http://zlib.net/manual.html#Advanced)    

- **nchan_permessage_deflate_compression_window** `[ 9-15 ]`  
  arguments: 1  
  default: `10`  
  context: http  
  > Compression window for the `deflate` algorithm used in websocket's permessage-deflate extension. The base two logarithm of the window size (the size of the history buffer). The bigger the window, the better the compression, but the more memory used by the compressor.    

- **nchan_publisher** `[ http | websocket ]`  
  arguments: 0 - 2  
  default: `http websocket`  
  context: server, location, if  
  legacy name: push_publisher  
  > Defines a server or location as a publisher endpoint. Requests to a publisher location are treated as messages to be sent to subscribers. See the protocol documentation for a detailed description.    
  [more details](#publisher-endpoints)  

- **nchan_publisher_channel_id**  
  arguments: 1 - 7  
  default: `(none)`  
  context: server, location, if  
  > Channel id for publisher location.    

- **nchan_publisher_upstream_request** `<url>`  
  arguments: 1  
  context: server, location, if  
  > Send POST request to internal location (which may proxy to an upstream server) with published message in the request body. Useful for bridging websocket publishers with HTTP applications, or for transforming message via upstream application before publishing to a channel.    
  > The upstream response code determines how publishing will proceed. A `200 OK` will publish the message from the upstream response's body. A `304 Not Modified` will publish the message as it was received from the publisher. A `204 No Content` will result in the message not being published.    
  [more details](#message-forwarding)  

- **nchan_pubsub** `[ http | websocket | eventsource | longpoll | intervalpoll | chunked | multipart-mixed | http-raw-stream ]`  
  arguments: 0 - 6  
  default: `http websocket eventsource longpoll chunked multipart-mixed`  
  context: server, location, if  
  > Defines a server or location as a pubsub endpoint. For long-polling, GETs subscribe. and POSTs publish. For Websockets, publishing data on a connection does not yield a channel metadata response. Without additional configuration, this turns a location into an echo server.    
  [more details](#pubsub-endpoint)  

- **nchan_subscribe_request** `<url>`  
  arguments: 1  
  context: server, location, if  
  > Send GET request to internal location (which may proxy to an upstream server) after subscribing. Disabled for longpoll and interval-polling subscribers.    
  [more details](#subscriber-presence)  

- **nchan_subscriber** `[ websocket | eventsource | longpoll | intervalpoll | chunked | multipart-mixed | http-raw-stream ]`  
  arguments: 0 - 5  
  default: `websocket eventsource longpoll chunked multipart-mixed`  
  context: server, location, if  
  legacy name: push_subscriber  
  > Defines a server or location as a channel subscriber endpoint. This location represents a subscriber's interface to a channel's message queue. The queue is traversed automatically, starting at the position defined by the `nchan_subscriber_first_message` setting.    
  >  The value is a list of permitted subscriber types.    
  [more details](#subscriber-endpoints)  

- **nchan_subscriber_channel_id**  
  arguments: 1 - 7  
  default: `(none)`  
  context: server, location, if  
  > Channel id for subscriber location. Can have up to 4 values to subscribe to up to 4 channels.    

- **nchan_subscriber_compound_etag_message_id**  
  arguments: 1  
  default: `off`  
  context: server, location, if  
  > Override the default behavior of using both `Last-Modified` and `Etag` headers for the message id.    
  > Enabling this option packs the entire message id into the `Etag` header, and discards  
  > `Last-Modified` and `If-Modified-Since` headers.    
  [more details]()  

- **nchan_subscriber_first_message** `[ oldest | newest | <number> ]`  
  arguments: 1  
  default: `oldest`  
  context: server, location, if  
  > Controls the first message received by a new subscriber. 'oldest' starts at the oldest available message in a channel's message queue, 'newest' waits until a message arrives. If a number `n` is specified, starts at `n`th message from the oldest. (`-n` starts at `n`th from now). 0 is equivalent to 'newest'.    

- **nchan_subscriber_http_raw_stream_separator** `<string>`  
  arguments: 1  
  default: `\n`  
  context: server, location, if  
  > Message separator string for the http-raw-stream subscriber. Automatically terminated with a newline character if not explicitly set to an empty string.    

- **nchan_subscriber_info**  
  arguments: 0  
  context: location  
  > A subscriber location for debugging the state of subscribers on a given channel. The subscribers of the channel specified by `nchan_channel_id` evaluate `nchan_subscriber_info_string` and send it back to the requested on this location. This is useful to see where subscribers are in an Nchan cluster, as well as debugging subscriber connection issues.    

- **nchan_subscriber_info_string**  
  arguments: 1  
  default: `$nchan_subscriber_type $remote_addr:$remote_port $http_user_agent $server_name $request_uri $pid`  
  context: server, location  
  > this string is evaluated by each subscriber on a given channel and sent to the requester of a `nchan_subscriber_info` location    

- **nchan_subscriber_last_message_id**  
  arguments: 1 - 5  
  default: `$http_last_event_id $arg_last_event_id`  
  context: server, location, if  
  > If `If-Modified-Since` and `If-None-Match` headers are absent, set the message id to the first non-empty of these values. Used primarily as a workaround for the inability to set the first `Last-Message-Id` of a web browser's EventSource object.     

- **nchan_subscriber_message_id_custom_etag_header**  
  arguments: 1  
  default: `(none)`  
  context: server, location, if  
  > Use a custom header instead of the Etag header for message ID in subscriber responses. This setting is a hack, useful when behind a caching proxy such as Cloudflare that under some conditions (like using gzip encoding) swallow the Etag header.    

- **nchan_subscriber_timeout** `<number> (seconds)`  
  arguments: 1  
  default: `0 (none)`  
  context: http, server, location, if  
  legacy name: push_subscriber_timeout  
  > Maximum time a subscriber may wait for a message before being disconnected. If you don't want a subscriber's connection to timeout, set this to 0. When possible, the subscriber will get a response with a `408 Request Timeout` status; otherwise the subscriber will simply be disconnected.    

- **nchan_unsubscribe_request** `<url>`  
  arguments: 1  
  context: server, location, if  
  > Send GET request to internal location (which may proxy to an upstream server) after unsubscribing. Disabled for longpoll and interval-polling subscribers.    
  [more details](#subscriber-presence)  

- **nchan_websocket_client_heartbeat** `<heartbeat_in> <heartbeat_out>`  
  arguments: 2  
  default: `none (disabled)`  
  context: server, location, if  
  > Most browser Websocket clients do not allow manually sending PINGs to the server. To overcome this limitation, this setting can be used to set up a PING/PONG message/response connection heartbeat. When the client sends the server message *heartbeat_in* (PING), the server automatically responds with *heartbeat_out* (PONG).    

- **nchan_websocket_ping_interval** `<number> (seconds)`  
  arguments: 1  
  default: `0 (none)`  
  context: server, location, if  
  > Interval for sending websocket ping frames. Disabled by default.    

- **nchan_access_control_allow_credentials**  
  arguments: 1  
  default: `on`  
  context: http, server, location, if  
  > When enabled, sets the [Cross-Origin Resource Sharing (CORS)](https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS) `Access-Control-Allow-Credentials` header to `true`.    

- **nchan_access_control_allow_origin** `<string>`  
  arguments: 1  
  default: `$http_origin`  
  context: http, server, location, if  
  > Set the [Cross-Origin Resource Sharing (CORS)](https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS) `Access-Control-Allow-Origin` header to this value. If the incoming request's `Origin` header does not match this value, respond with a `403 Forbidden`. Multiple origins can be provided in a single argument separated with a space.    

- **nchan_authorize_request** `<url>`  
  arguments: 1  
  context: server, location, if  
  > Send GET request to internal location (which may proxy to an upstream server) for authorization of a publisher or subscriber request. A 200 response authorizes the request, a 403 response forbids it.    
  [more details](#request-authorization)  

- **nchan_channel_group** `<string>`  
  arguments: 1  
  default: `(none)`  
  context: server, location, if  
  legacy name: push_channel_group  
  > The accounting and security group a channel belongs to. Works like a prefix string to the channel id. Can be set with nginx variables.    

- **nchan_channel_group_accounting**  
  arguments: 1  
  default: `off`  
  context: server, location  
  > Enable tracking channel, subscriber, and message information on a per-channel-group basis. Can be used to place upper limits on channel groups.    

- **nchan_group_location** `[ get | set | delete | off ]`  
  arguments: 0 - 3  
  default: `get set delete`  
  context: location  
  > Group information and configuration location. GET request for group info, POST to set limits, DELETE to delete all channels in group.    

- **nchan_group_max_channels** `<number>`  
  arguments: 1  
  default: `0 (unlimited)`  
  context: location  
  > Maximum number of channels allowed in the group.    

- **nchan_group_max_messages** `<number>`  
  arguments: 1  
  default: `0 (unlimited)`  
  context: location  
  > Maximum number of messages allowed for all the channels in the group.    

- **nchan_group_max_messages_disk** `<number>`  
  arguments: 1  
  default: `0 (unlimited)`  
  context: location  
  > Maximum amount of disk space allowed for the messages of all the channels in the group.    

- **nchan_group_max_messages_memory** `<number>`  
  arguments: 1  
  default: `0 (unlimited)`  
  context: location  
  > Maximum amount of shared memory allowed for the messages of all the channels in the group.    

- **nchan_group_max_subscribers** `<number>`  
  arguments: 1  
  default: `0 (unlimited)`  
  context: location  
  > Maximum number of subscribers allowed for the messages of all the channels in the group.    

- **nchan_max_channel_id_length** `<number>`  
  arguments: 1  
  default: `1024`  
  context: http, server, location  
  legacy name: push_max_channel_id_length  
  > Maximum permissible channel id length (number of characters). This settings applies to ids before they may be split by the `nchan_channel_id_split_delimiter` Requests with a channel id that is too long will receive a `403 Forbidden` response.    

- **nchan_max_channel_subscribers** `<number>`  
  arguments: 1  
  default: `0 (unlimited)`  
  context: http, server, location  
  legacy name: push_max_channel_subscribers  
  > Maximum concurrent subscribers to the channel on this Nchan server. Does not include subscribers on other Nchan instances when using a shared Redis server.    

- **nchan_subscribe_existing_channels_only** `[ on | off ]`  
  arguments: 1  
  default: `off`  
  context: http, server, location  
  legacy name: push_authorized_channels_only  
  > Whether or not a subscriber may create a channel by sending a request to a subscriber location. If set to on, a publisher must send a POST or PUT request before a subscriber can request messages on the channel. Otherwise, all subscriber requests to nonexistent channels will get a 403 Forbidden response.    

- **nchan_message_buffer_length** `[ <number> | <variable> ]`  
  arguments: 1  
  default: `10`  
  context: http, server, location  
  legacy names: push_max_message_buffer_length, push_message_buffer_length  
  > Publisher configuration setting the maximum number of messages to store per channel. A channel's message buffer will retain a maximum of this many most recent messages. An Nginx variable can also be used to set the buffer length dynamically.    

- **nchan_message_temp_path** `<path>`  
  arguments: 1  
  default: `<client_body_temp_path>`  
  context: http  
  > Large messages are stored in temporary files in the `client_body_temp_path` or the `nchan_message_temp_path` if the former is unavailable. Default is the built-in default `client_body_temp_path`    

- **nchan_message_timeout** `[ <time> | <variable> ]`  
  arguments: 1  
  default: `1h`  
  context: http, server, location  
  legacy name: push_message_timeout  
  > Publisher configuration setting the length of time a message may be queued before it is considered expired. If you do not want messages to expire, set this to 0. Note that messages always expire from oldest to newest, so an older message may prevent a newer one with a shorter timeout from expiring. An Nginx variable can also be used to set the timeout dynamically.    

- **nchan_redis_accurate_subscriber_count**  
  arguments: 1  
  default: `off`  
  context: upstream  
  > When disabled, use fast but potentially inaccurate subscriber counts. These may become inaccurate if Nginx workers exit uncleanly or are terminated. When enabled, use a slightly slower but completely accurate subscriber count. Defaults to 'off' for legacy reasons, but will be enabled by default in the future.    

- **nchan_redis_cluster_check_interval_backoff** `<floating point> >= 0, ratio of current delay`  
  arguments: 1  
  default: `2 (increase delay by 200% each try)`  
  context: upstream  
  > Add an exponentially increasing delay to the Redis cluster check interval. `Delay[n] = (Delay[n-1] + jitter) * (nchan_redis_cluster_check_interval_backoff + 1)`.    

- **nchan_redis_cluster_check_interval_jitter** `<floating point> >= 0, (0 to disable)`  
  arguments: 1  
  default: `0.2 (20% of interval value)`  
  context: upstream  
  > Introduce random jitter to Redis cluster check interval, where the range is `±(cluster_check_interval * nchan_redis_cluster_check_interval_jitter) / 2`.    

- **nchan_redis_cluster_check_interval_max** `<time> (0 to disable)`  
  arguments: 1  
  default: `30s`  
  context: upstream  
  > Maximum Redis cluster check interval after backoff and jitter.    

- **nchan_redis_cluster_check_interval_min** `<time>`  
  arguments: 1  
  default: `1s (0 to disable)`  
  context: upstream  
  > When connected to a cluster, periodically check the cluster state and layout via a random master node.    

- **nchan_redis_cluster_connect_timeout**  
  arguments: 1  
  default: `15s`  
  context: upstream  
  > Redis cluster connection timeout.    

- **nchan_redis_cluster_max_failing_time**  
  arguments: 1  
  default: `30s`  
  context: upstream  
  > Maximum time a Redis cluster can be in a failing state before Nchan disconnects from it. During this time, Nchan will try to recover from a cluster or node failure without disconnecting the entire cluster.    

- **nchan_redis_cluster_recovery_delay** `<time>`  
  arguments: 1  
  default: `100ms`  
  context: upstream  
  > After a cluster recovery failure, wait this long to try again.    

- **nchan_redis_cluster_recovery_delay_backoff** `<floating point> >= 0, ratio of current delay`  
  arguments: 1  
  default: `0.5 (increase delay by 50% each try)`  
  context: upstream  
  > Add an exponentially increasing delay to Redis cluster recovery retries. `Delay[n] = (Delay[n-1] + jitter) * (nchan_redis_cluster_recovery_delay_backoff + 1)`.    

- **nchan_redis_cluster_recovery_delay_jitter** `<floating point> >= 0, (0 to disable)`  
  arguments: 1  
  default: `0.5 (50% of delay value)`  
  context: upstream  
  > Introduce random jitter to Redis cluster recovery retry time, where the range is `±(recovery_delay * nchan_redis_cluster_recovery_delay_jitter) / 2`.    

- **nchan_redis_cluster_recovery_delay_max** `<time> (0 to disable)`  
  arguments: 1  
  default: `2s`  
  context: upstream  
  > Maximum Redis cluster recovery delay after backoff and jitter.    

- **nchan_redis_command_timeout** `<time> (0 to leave unlimited)`  
  arguments: 1  
  default: `5s`  
  context: upstream  
  > If a Redis server exceeds this time to produce a command reply, it is considered unhealthy and is disconnected.    

- **nchan_redis_connect_timeout**  
  arguments: 1  
  default: `10s`  
  context: upstream  
  > Redis server connection timeout.    

- **nchan_redis_discovered_ip_range_blacklist** `<CIDR range>`  
  arguments: 1 - 7  
  context: upstream  
  > do not attempt to connect to **autodiscovered** nodes with IPs in the specified ranges. Useful for blacklisting private network ranges for clusters and Redis slaves. NOTE that this blacklist applies only to autodiscovered nodes, and not ones specified in the upstream block    

- **nchan_redis_idle_channel_cache_timeout** `<time>`  
  arguments: 1  
  default: `30s`  
  context: http, server, location  
  > A Redis-stored channel and its messages are removed from memory (local cache) after this timeout, provided there are no local subscribers.    

- **nchan_redis_namespace** `<string>`  
  arguments: 1  
  context: http, server, upstream, location  
  > Prefix all Redis keys with this string. All Nchan-related keys in redis will be of the form "nchan_redis_namespace:*" . Default is empty.    

- **nchan_redis_nostore_fastpublish** `[ on | off ]`  
  arguments: 1  
  default: `off`  
  context: http, server, upstream  
  > Increases publishing capacity by 2-3x for Redis nostore mode at the expense of inaccurate subscriber counts in the publisher response.    

- **nchan_redis_optimize_target** `[ cpu | bandwidth ]`  
  arguments: 1  
  default: `bandwidth`  
  context: upstream  
  > This tweaks whether [effect replication](https://redis.io/commands/eval#replicating-commands-instead-of-scripts) is enabled. This setting is obsolete, as effect replication is now always enabled to support other features    

- **nchan_redis_pass** `<upstream-name>`  
  arguments: 1  
  context: http, server, location  
  > Use an upstream config block for Redis servers.    
  [more details](#connecting-to-a-redis-server)  

- **nchan_redis_password**  
  arguments: 1  
  default: `<none>`  
  context: upstream  
  > Set Redis password for AUTH command. All servers in the upstream block will use this password _unless_ a different password is specified by a server URL.    

- **nchan_redis_ping_interval**  
  arguments: 1  
  default: `4m`  
  context: http, server, upstream, location  
  > Send a keepalive command to redis to keep the Nchan redis clients from disconnecting. Set to 0 to disable.    

- **nchan_redis_reconnect_delay** `<time>`  
  arguments: 1  
  default: `500ms`  
  context: upstream  
  > After a connection failure, wait this long before trying to reconnect to Redis.    

- **nchan_redis_reconnect_delay_backoff** `<floating point> >= 0 (0 to disable)`  
  arguments: 1  
  default: `0.5 (increase delay by 50% each try)`  
  context: upstream  
  > Add an exponentially increasing delay to Redis connection retries. `Delay[n] = (Delay[n-1] + jitter) * (nchan_redis_reconnect_delay_backoff + 1)`.    

- **nchan_redis_reconnect_delay_jitter** `<floating point> >= 0 (0 to disable)`  
  arguments: 1  
  default: `0.1 (10% of delay value)`  
  context: upstream  
  > Introduce random jitter to Redis reconnection time, where the range is `±(reconnect_delay * nchan_redis_reconnect_delay_jitter) / 2`.    

- **nchan_redis_reconnect_delay_max** `<time> (0 to disable)`  
  arguments: 1  
  default: `10s`  
  context: upstream  
  > Maximum Redis reconnection delay after backoff and jitter.    

- **nchan_redis_retry_commands**  
  arguments: 1  
  default: `on`  
  context: upstream  
  > Allow Nchan to retry some Redis commands on keyslot errors and cluster unavailability. Queuing up a lot of commands while the cluster is unavailable may lead to excessive memory use, but it can also defer commands during transient failures.    

- **nchan_redis_retry_commands_max_wait** `<time> (0 to leave unlimited)`  
  arguments: 1  
  default: `500ms`  
  context: upstream  
  > When `nchan_redis_retry_commands` is on, the maximum time a command will stayed queued to be retried.    

- **nchan_redis_server** `<redis-url>`  
  arguments: 1  
  context: upstream  
  > Used in upstream { } blocks to set redis servers. Redis url is in the form 'redis://:password@hostname:6379/0'. Shorthands 'host:port' or 'host' are permitted.    
  [more details](#connecting-to-a-redis-server)  

- **nchan_redis_ssl** `[ on | off ]`  
  arguments: 1  
  default: `off`  
  context: upstream  
  > Enables SSL/TLS for all connections to Redis servers in this upstream block. When enabled, no unsecured connections are permitted    

- **nchan_redis_ssl_ciphers**  
  arguments: 1  
  default: `<system default>`  
  context: upstream  
  > Acceptable ciphers when using TLS for Redis connections    

- **nchan_redis_ssl_client_certificate**  
  arguments: 1  
  context: upstream  
  > Path to client certificate when using TLS for Redis connections    

- **nchan_redis_ssl_client_certificate_key**  
  arguments: 1  
  context: upstream  
  > Path to client certificate key when using TLS for Redis connections    

- **nchan_redis_ssl_server_name**  
  arguments: 1  
  context: upstream  
  > Server name to verify (CN) when using TLS for Redis connections    

- **nchan_redis_ssl_trusted_certificate**  
  arguments: 1  
  context: upstream  
  > Trusted certificate (CA) when using TLS for Redis connections    

- **nchan_redis_ssl_trusted_certificate_path**  
  arguments: 1  
  default: `<system default>`  
  context: upstream  
  > Trusted certificate (CA) when using TLS for Redis connections. Defaults to the system's SSL cert path unless nchan_redis_ssl_trusted_certificate is set    

- **nchan_redis_ssl_verify_certificate** `[ on | off ]`  
  arguments: 1  
  default: `on`  
  context: upstream  
  > Should the server certificate be verified when using TLS for Redis connections? Useful to disable when testing with a self-signed server certificate.    

- **nchan_redis_storage_mode** `[ distributed | backup | nostore ]`  
  arguments: 1  
  default: `distributed`  
  context: http, server, upstream, location  
  > The mode of operation of the Redis server. In `distributed` mode, messages are published directly to Redis, and retrieved in real-time. Any number of Nchan servers in distributed mode can share the Redis server (or cluster). Useful for horizontal scalability, but suffers the latency penalty of all message publishing going through Redis first.  
  >   
  > In `backup` mode, messages are published locally first, then later forwarded to Redis, and are retrieved only upon channel initialization. Only one Nchan server should use a Redis server (or cluster) in this mode. Useful for data persistence without sacrificing response times to the latency of a round-trip to Redis.  
  >   
  > In `nostore` mode, messages are published as in `distributed` mode, but are not stored. Thus Redis is used to broadcast messages to many Nchan instances with no delivery guarantees during connection failure, and only local in-memory storage. This means that there are also no message delivery guarantees for subscribers switching from one Nchan instance to another connected to the same Redis server or cluster. Nostore mode increases Redis publishing capacity by an order of magnitude.    

- **nchan_redis_subscribe_weights** `master=<integer> slave=<integer>`  
  arguments: 1 - 2  
  default: `master=1 slave=1`  
  context: upstream  
  > Determines how subscriptions to Redis PUBSUB channels are distributed between master and slave nodes. The higher the number, the more likely that each node of that type will be chosen for each new channel. The weights for slave nodes are cumulative, so an equal 1:1 master:slave weight ratio with two slaves would have a 1/3 chance of picking a master, and 2/3 chance of picking one of the slaves. The weight must be a non-negative integer.    

- **nchan_redis_upstream_stats** `<upstream_name>`  
  arguments: 1  
  default: `(none)`  
  context: server, location  
  > Defines a location as redis statistics endpoint. GET requests to this location produce a JSON response with detailed listings of total Redis command times and number of calls, broken down by node and command type. Useful for making graphs about Redis performance. Can be set with nginx variables.    

- **nchan_redis_upstream_stats_disconnected_timeout**  
  arguments: 1  
  default: `5m`  
  context: upstream  
  > Keep stats for disconnected nodes around for this long. Useful for tracking stats for nodes that have intermittent connectivity issues.    

- **nchan_redis_upstream_stats_enabled** `[ on | off ]`  
  arguments: 1  
  default: `<on> if at least 1 redis stats location is configured, otherwise <off>`  
  context: upstream  
  > Gather Redis node command timings for this upstream    

- **nchan_redis_url** `<redis-url>`  
  arguments: 1  
  default: `127.0.0.1:6379`  
  context: http, server, location  
  > Use of this command is discouraged in favor of upstreams blocks with (`nchan_redis_server`)[#nchan_redis_server]. The path to a redis server, of the form 'redis://:password@hostname:6379/0'. Shorthand of the form 'host:port' or just 'host' is also accepted.    
  [more details](#connecting-to-a-redis-server)  

- **nchan_redis_username**  
  arguments: 1  
  default: `<none>`  
  context: upstream  
  > Set Redis username for AUTH command (available when using ACLs on the Redis server). All servers in the upstream block will use this username _unless_ a different username is specified by a server URL.    

- **nchan_shared_memory_size** `<size>`  
  arguments: 1  
  default: `128M`  
  context: http  
  legacy names: push_max_reserved_memory, nchan_max_reserved_memory  
  > Shared memory slab pre-allocated for Nchan. Used for channel statistics, message storage, and interprocess communication.    
  [more details](#memory-storage)  

- **nchan_store_messages** `[ on | off ]`  
  arguments: 1  
  default: `on`  
  context: http, server, location, if  
  legacy name: push_store_messages  
  > Publisher configuration. "`off`" is equivalent to setting `nchan_message_buffer_length 0`, which disables the buffering of old messages. Using this setting is not recommended when publishing very quickly, as it may result in missed messages.    

- **nchan_use_redis** `[ on | off ]`  
  arguments: 1  
  default: `off`  
  context: http, server, location  
  > Use of this command is discouraged in favor of (`nchan_redis_pass`)[#nchan_redis_pass]. Use Redis for message storage at this location.    
  [more details](#connecting-to-a-redis-server)  

- **nchan_channel_event_string** `<string>`  
  arguments: 1  
  default: `"$nchan_channel_event $nchan_channel_id"`  
  context: server, location, if  
  > Contents of channel event message    

- **nchan_channel_events_channel_id**  
  arguments: 1  
  context: server, location, if  
  > Channel id where `nchan_channel_id`'s events should be sent. Events like subscriber enqueue/dequeue, publishing messages, etc. Useful for application debugging. The channel event message is configurable via nchan_channel_event_string. The channel group for events is hardcoded to 'meta'.    
  [more details](#channel-events)  

- **nchan_stub_status**  
  arguments: 0  
  context: location  
  > Similar to Nginx's stub_status directive, requests to an `nchan_stub_status` location get a response with some vital Nchan statistics. This data does not account for information from other Nchan instances, and monitors only local connections, published messages, etc.    
  [more details](#nchan_stub_status)  

- **nchan_channel_timeout**  
  arguments: 1  
  context: http, server, location  
  legacy name: push_channel_timeout  
  > Amount of time an empty channel hangs around. Don't mess with this setting unless you know what you are doing!    

- **nchan_storage_engine** `[ memory | redis ]`  
  arguments: 1  
  default: `memory`  
  context: http, server, location  
  > Development directive to completely replace default storage engine. Don't use unless you are an Nchan developer.    

## Contribute
Please support this project with a donation to keep me warm through the winter. I accept bitcoin at 15dLBzRS4HLRwCCVjx4emYkxXcyAPmGxM3 . Other donation methods can be found at https://nchan.io

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-nchan](https://github.com/slact/nchan){target=_blank}.

# *ndk*: Nginx Development Kit


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-ndk
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ndk_http_module.so;
```


This document describes nginx-module-ndk [v0.3.3](https://github.com/vision5/ngx_devel_kit/releases/tag/v0.3.3){target=_blank} 
released on Nov 15 2023.

<hr />

Nginx Development Kit (NDK)

## Synopsis

The NDK is an Nginx module that is designed to extend the core functionality of the
excellent Nginx webserver in a way that can be used as a basis of other Nginx modules.

It has functions and macros to deal with generic tasks that don't currently have
generic code as part of the core distribution.  The NDK itself adds few features
that are seen from a user's point of view - it's just designed to help reduce the
code that Nginx module developers need to write.

Nginx module developers wishing to use any of the features in the NDK should specify
that the NDK is a dependency of their module, and that users will need to compile
it as well when they compile their own modules.  They will also need to declare in
their own modules which features of the NDK they wish to use (explained below).

If you are not an Nginx module developer, then the only useful part of this project
will be the 'usage for users' section below.


## Status

The NDK is now considered to be stable. It is already being used in quite a few third
party modules (see list below).


## Features

* additional conf_set functions for regexes, complex/script values, paths...
* macros to simplify tasks like checking for NULL values when doing ngx_array_push
* patches to the main source code
* ngx_auto_lib_core generic external library handler is included (see separate readme)


## Design

## modular

The kit itself is designed in a modular way, so that only the required code is compiled.
It's possible to add just a single NDK module, a few or all of them.


## auto-generated & easily extensible

Many of the macros available in the NDK are auto-generated from simple configuration
files.  This makes creating similar macros for your own code very simple - it's usually
just the case of adding an extra line to a config file and re-running the build script.


## Usage for users

If another Nginx module you wish to use specifies that the NDK is a dependency, you
will need to do the following :

1. download the source (https://github.com/simpl/ngx_devel_kit)
2. unpack the source (tar -xzf $name)
3. compile Nginx with the following extra option `--add-module=/path/to/ngx_devel_kit`.

e.g.

```bash
./configure --add-module=/path/to/ngx_devel_kit \
            --add-module=/path/to/another/module
```


## Usage for developers

To use the NDK in your own module, you need to add the following:

1. add this line to your module

```C
#include    <ndk.h>
```

Note: since the NDK includes the following lines

```C
#include    <ngx_config.h>
#include    <ngx_core.h>
#include    <ngx_http.h>
```

you can replace these with the single include above.
2. add the following line in the config file for your module:

```bash
have=NDK_[module_name]  . auto/have
```

for each NDK module that you wish to use (you need to include auto/have multiple
times if you wish to use multiple NDK modules.

Note: the old method of setting

```config
CFLAGS="$CFLAGS -DNDK_[module_name]"
```

is now deprecated. It will still work, but results in unnecessary lines being
displayed when compiling Nginx.


## Warning: Using NDK_ALL

You can also set `NDK_ALL` to include all the NDK modules.  This is primarily as
a convenience in the early stages of development of another module. However,

DO NOT LEAVE `NDK_ALL` IN YOUR CONFIG FILE WHEN PUBLISHING

Although the NDK is fairly small now, it could in time become a large repository
of code that would, if using NDK_ALL, result in considerably more code being compiled
than is necessary.


## Modules using NDK

The following 3rd-party modules make use of NDK.

* [ngx_http_lua_module](https://github.com/openresty/lua-nginx-module#readme)
* [ngx_http_set_misc_module](https://github.com/openresty/set-misc-nginx-module#readme)
* [ngx_http_encrypted_session_module](https://github.com/openresty/encrypted-session-nginx-module#readme)
* [ngx_http_form_input_module](https://github.com/calio/form-input-nginx-module#readme)
* [ngx_http_iconv_module](https://github.com/calio/iconv-nginx-module#readme)
* [ngx_http_array_var_module](https://github.com/openresty/array-var-nginx-module#readme)

If you would like to add your module to this list, please let us know.


## Contributing / Feedback

If you are an Nginx module developer, and have developed some functions that are
generic in nature (or would be easily adapted to be so), then please send them to
me at the address below, and I'll addmclyne to the kit.


## Special Thanks

A special thanks goes to [Yichun Zhang](https://github.com/agentzh) for helping to maintain
this module.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-ndk](https://github.com/vision5/ngx_devel_kit){target=_blank}.

# *njs*: NGINX njs dynamic modules


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-njs
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_js_module.so;
```
```nginx
load_module modules/ngx_stream_js_module.so;
```


This document describes nginx-module-njs [v0.8.2](https://github.com/nginx/njs/releases/tag/0.8.2){target=_blank} 
released on Oct 24 2023.

<hr />

## NGINX JavaScript (njs)

njs is a subset of the JavaScript language that allows extending nginx
functionality. njs is created in compliance with ECMAScript 5.1 (strict mode)
with some ECMAScript 6 and later extensions. The compliance is still evolving.

The documentation is available online:

  https://nginx.org/en/docs/njs/

Additional examples and howtos can be found here:

  https://github.com/nginx/njs-examples

Please ask questions, report issues, and send patches to the mailing list:

    nginx-devel@nginx.org (https://mailman.nginx.org/mailman/listinfo/nginx-devel)

or via Github:

    https://github.com/nginx/njs
## 
NGINX, Inc., https://nginx.com

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-njs](https://github.com/nginx/njs){target=_blank}.

# *pagespeed*: PageSpeed dynamic module for NGINX

## Installation

CentOS/RHEL/RockyLinux/etc. and Amazon Linux are supported and require a [subscription](https://www.getpagespeed.com/repo-subscribe).

Fedora Linux is supported free of charge and doesn't require a subscription.

### OS-specific complete installation and configuration guides available:

*   [CentOS/RHEL 7](https://bit.ly/ngx-pagespeed-el)
*   [CentOS/RHEL 8](https://bit.ly/ngx-pagespeed-el)
*   [Amazon Linux 2](https://bit.ly/ngx-pagespeed-el)

### Other supported operating systems
        
```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-pagespeed
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_pagespeed.so;
```


This document describes nginx-module-pagespeed [v1.13.35.2](https://github.com/apache/incubator-pagespeed-ngx/releases/tag/v1.13.35.2-stable){target=_blank} 
released on Feb 05 2018.

<hr />
![ngx_pagespeed](https://lh6.googleusercontent.com/-qufedJIJq7Y/UXEvVYxyYvI/AAAAAAAADo8/JHDFQhs91_c/s401/04_ngx_pagespeed.png)


ngx_pagespeed speeds up your site and reduces page load time by automatically
applying web performance best practices to pages and associated assets (CSS,
JavaScript, images) without requiring you to modify your existing content or
workflow. Features include:

- Image optimization: stripping meta-data, dynamic resizing, recompression
- CSS & JavaScript minification, concatenation, inlining, and outlining
- Small resource inlining
- Deferring image and JavaScript loading
- HTML rewriting
- Cache lifetime extension
- and
  [more](https://developers.google.com/speed/docs/mod_pagespeed/config_filters)

To see ngx_pagespeed in action, with example pages for each of the
optimizations, see our <a href="http://ngxpagespeed.com">demonstration site</a>.

## How to use

Follow the steps on <a
href="https://developers.google.com/speed/pagespeed/module/configuration">PageSpeed
configuration</a>.

For feedback, questions, and to follow
the progress of the project:

- [ngx-pagespeed-discuss mailing
  list](https://groups.google.com/forum/#!forum/ngx-pagespeed-discuss)
- [ngx-pagespeed-announce mailing
  list](https://groups.google.com/forum/#!forum/ngx-pagespeed-announce)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-pagespeed](https://github.com/apache/incubator-pagespeed-ngx){target=_blank}.

# *passenger*: Passenger module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-passenger
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_passenger_module.so;
```


This document describes nginx-module-passenger [v6.0.19](https://github.com/phusion/passenger/releases/tag/release-6.0.19){target=_blank} 
released on Nov 20 2023.

<hr />
[![Gem Version](https://badge.fury.io/rb/passenger.svg)](https://badge.fury.io/rb/passenger)
## <img src="images/passenger_logo.svg" alt="passenger logo" style="margin-bottom: -.2em; width: 1.4em"> Phusion Passenger®
<h3>Supercharge your Ruby, Node.js and Python apps</h3>

[Phusion Passenger®](https://www.phusionpassenger.com/) is a web server and application server, designed to be fast, robust and lightweight. It takes a lot of complexity out of deploying web apps, adds powerful enterprise-grade features that are useful in production, and makes administration much easier and less complex. Phusion Passenger supports Ruby, Python, Node.js and Meteor, and is being used by high-profile companies such as **Apple, Pixar, New York Times, AirBnB, Juniper** etc as well as [over 650.000 websites](http://trends.builtwith.com/Web-Server/Phusion-Passenger).

<a href="https://vimeo.com/224923750"><img src="https://github.com/phusion/passenger/blob/stable-5.2/images/justin.png" height="400"></a><br><em>Phusion Passenger - the smart app server</em>

<p>What makes Passenger so fast and reliable is its <strong>C++</strong> core, its <strong>zero-copy</strong> architecture, its <strong>watchdog</strong> system and its <strong>hybrid</strong> evented, multi-threaded and multi-process design.</p>

### Learn more:
- [Website](https://www.phusionpassenger.com/)
- [Fuse Panel](https://www.phusionpassenger.com/fuse-panel)
- [Documentation &amp; Support](https://www.phusionpassenger.com/support)
- [Consultancy](https://www.phusion.nl/consultancy)
- [Twitter](https://twitter.com/phusion_nl)
- [Blog](http://blog.phusion.nl/)

<br/><br/><br/><br/><br/>

## Further reading

 * The `doc/` directory.
 * [Contributors Guide](https://github.com/phusion/passenger/blob/master/CONTRIBUTING.md)
 * [Phusion Passenger support page](https://www.phusionpassenger.com/support)
 * [Phusion Passenger release notes](https://blog.phusion.nl/tag/passenger-releases/)

## Legal

"Passenger" and "Phusion Passenger" are registered trademarks of Phusion Holding B.V.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-passenger](https://github.com/phusion/passenger){target=_blank}.

# *perl*: NGINX Perl dynamic module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-perl
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_perl_module.so;
```

<hr />


## Directives

You may find information about configuration directives for this module at the following links:        

*   https://nginx.org/en/docs/http/ngx_http_perl_module.html#directives

# *phantom-token*: Phantom Token NGINX Module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-phantom-token
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_curity_http_phantom_token_module.so;
```


This document describes nginx-module-phantom-token [v1.5.0](https://github.com/curityio/nginx_phantom_token_module/releases/tag/1.5.0){target=_blank} 
released on Sep 28 2023.

<hr />

[![Quality](https://img.shields.io/badge/quality-production-green)](https://curity.io/resources/code-examples/status/)
[![Availability](https://img.shields.io/badge/availability-binary-blue)](https://curity.io/resources/code-examples/status/)

NGINX module that introspects access tokens according to [RFC 7662](https://tools.ietf.org/html/rfc7662), producing a "phantom token" that can be forwarded to back-end APIs and Web services. Read more about the [Phantom Token approach](https://curity.io/resources/learn/phantom-token-pattern/).

This module, when enabled, filters incoming requests, denying access to those which do *not* have a valid OAuth access token presented in an `Authorization` header. From this header, the access_token is extracted and introspected using the configured endpoint. The Curity Identity Server replies to this request according to the standard. For an active access token, the body of the Curity Identity Server's response contains the JWT that replaces the access token in the header of the request that is forwarded by NGINX to the back-end. If the token is not valid or absent, no request to the back-end is made and the caller is given a 401, unauthorized, error. This flow is shown in the following diagram:

![NGINX / Curity integration](nginx_curity_integration.png?v=2 "Overview of how NGINX and Curity are integrated")

The initial calls by the app (web or native) are done using [OpenID Connect](http://openid.net/specs/openid-connect-core-1_0.html) (OIDC). The important part is that the token that is issued is an opaque access token. It is a GUID or UUID or a few handfuls of random bytes; there is no identity-related data in this token. It is a _phantom_ of the actual user data, hence the name -- _phantom token_. The app presents the token to the NGINX gateway according to the _Bearer Token Usage_ specficiation (i.e., [RFC 6750](https://tools.ietf.org/html/rfc6750)). This standard says that the app should send the phantom token in the `Authorization` request header. 

Once the NGINX server receives the access token, this module will kick in. Using configuration like that below, this module will interrogate the request, find the token, and make a sideways call to the Curity Identity Server. This web service request will be done using the _Token Introspection_ standard ([RFC 7662](https://tools.ietf.org/html/rfc7662)) with an `Accept` type of `application/jwt` (as defined in [RFC 7519](https://tools.ietf.org/html/rfc7519#section-10.3.1)). This will cause the Curity Identity Server to return not JSON but just a JWT. Then, the module will forward the JWT token to the back-end APIs and microservices. 

If the module is also configured to cache the results of the call to the Curity Identity Server (which it should be for production cases), the phantom token will be used as a cache key for the corresponding JWT token. This will eliminate the need for subsequent calls to the Curity Identity Server for as long as it tells the NGINX module it may cache the JWT for.

The tl;dr is a very simple API gateway that is blazing fast, highly scalable, and without any bells and whistles to get in the way. All the code is here, so it's easy to change and use with other OAuth servers even!

## Configuration Directives

### Required Configuration Directives

All the directives in this subsection are required; if any of these are omitted, the module will be disabled.

#### phantom_token

> **Syntax**: **`phantom_token`** `on` | `off`
>
> **Default**: *`off`*
>
> **Context**: `location`

#### phantom_token_client_credential

> **Syntax**: **`phantom_token_client_credential`** _`string`_ _`string`_ 
> 
> **Default**: *`—`*                                                                
> 
> **Context**: `location`                                                           
 
The client ID and secret of the OAuth client which will be used for introspection. The first argument to this directive is the client ID and the second is the secret. The maximum total length of the two arguments must be less than 255 characters. Both should be printable ASCII values; non-ASCII values _may_ work but are untested. If this directive is not configured, then the module will be disabled.

#### phantom_token_introspection_endpoint

> **Syntax**: **`phantom_token_introspection_endpoint`** _`string`_
>
> **Default**: *`—`*
>
> **Context**: `location`

The name of the location that proxies requests to the Curity Identity Server. Note that this location needs to be in the same server as the one referring to it using this directive.

Example configuration:

```nginx
server {
    location /api {
        ...
        phantom_token_introspection_endpoint my_good_location_name_for_curity;
    }
    
    location my_good_location_name_for_curity {
        ...
    }
}
```

### Optional Configuration Directives

The following directives are optional and do not need to be configured.

#### phantom_token_realm

> **Syntax**: **`phantom_token_realm`** _`string`_
> 
> **Default**: *`api`*
> 
> **Context**: `location`

The name of the protected realm or scope of protection that should be used when a client does not provide an access token.

Example configuration:

```nginx
location / {
   ...
   phantom_token_realm "myGoodRealm";
}   
```

#### phantom_token_scopes

> **Syntax**: **`phantom_token_scopes`** _`string`_
>
> **Default**: *`—`*
>
> **Context**: `location`

The space-separated list of scopes that the server should inform the client are required when it does not provide an access token.

Example configuration:

```nginx
location / {
   ...
   phantom_token_scopes "scope_a scope_b scope_c";
}
```

#### phantom_token_scope

> **Syntax**: **`phantom_token_scope`** _`string`_
>
> **Default**: *`—`*
>
> **Context**: `location`

An array of scopes that the server should inform the client are required when it does not provide an access token. If `phantom_token_scopes` is also configured, that value will supersede these.
 
Example configuration:
 
```nginx
location / {
   ...
   phantom_token_scope "scope_a";
   phantom_token_scope "scope_b";
   phantom_token_scope "scope_c";
}
```

## Sample Configuration

### Loading the Module
If the module is downloaded from GitHub or compiled as a shared library (the default) and not explicitly compiled into NGINX, it will need to be loaded using the [load_module](http://nginx.org/en/docs/ngx_core_module.html#load_module) directive. This needs to be done in the _main_ part of the NGINX configuration:

```nginx
load_module modules/ngx_curity_http_phantom_token_module.so;
```

The file can be an absolute or relative path. If it is not absolute, it should be relative to the NGINX root directory.

### Simple Configuration
The following is a simple configuration that might be used in demo or development environments where the NGINX reverse proxy is on the same host as the Curity Identity Server:

```nginx
server {
    location /api {
        proxy_pass         https://example.com/api;

        phantom_token on;
        phantom_token_client_credential "client_id" "client_secret";
        phantom_token_introspection_endpoint curity;
    }
    
    location curity {
        proxy_pass "https://curity.example.com/oauth/v2/introspection";
    }
}
```

### Complex Configuration
The following is a more complex configuration where the NGINX reverse proxy is on a separate host to the Curity Identity Server:

```nginx
server {
    server_name server1.example.com;n
    location /api {
        proxy_pass         https://example.com/api;

        phantom_token on;
        phantom_token_client_credential "client_id" "client_secret";
        phantom_token_introspection_endpoint curity;
        
        phantom_token_realm "myGoodAPI";
        phantom_token_scopes "scope_a scope_b scope_c";
    }
    
    location curity {
        proxy_pass "https://server2.example.com:8443/oauth/v2/introspection";
    }
}

server {
    listen 8443;
    server_name server2.example.com;
    location / {
        proxy_pass "https://curity.example.com";
    }
}
```
        
### More Advanced Configuration with Separate Servers and Caching
This module takes advantage of NGINX built-in _proxy_cache_ directive. In order to be able to cache the requests made to the introspection endpoint, except of the `proxy_cache_path` in http context and `proxy_cache` in location context, you have to add the following 3 directives in the location context of the introspection endpoint.

- `proxy_cache_methods POST;` POST requests are not cached by default.
- `proxy_cache_key $request_body;` The key of the cache is related to the _access_token_ sent in the original request. Different requests using the same _access_token_ reach the same cache.
- `proxy_ignore_headers Set-Cookie;` NGINX will not cache the response if `Set-Cookie` header is not ignored.

```nginx
http {
    proxy_cache_path /path/to/cache/cache levels=1:2 keys_zone=my_cache:10m max_size=10g
                     inactive=60m use_temp_path=off;
    server {
        server_name server1.example.com;
        location /api {
            proxy_pass         https://example.com/api;

            phantom_token on;
            phantom_token_client_credential "client_id" "client_secret";
            phantom_token_introspection_endpoint curity;
            phantom_token_scopes "scope_a scope_b scope_c";
            phantom_token_realm "myGoodAPI";
        }
        
        location curity {
            proxy_pass "https://server2.example.com:8443/oauth/v2/introspection";
            
            proxy_cache_methods POST;
            proxy_cache my_cache;
            proxy_cache_key $request_body;
            proxy_ignore_headers Set-Cookie;
        }
    }
    
    server {
        listen 8443;
        server_name server2.example.com;
        location / {
            proxy_pass "https://curity.example.com";
        }
    }
}   
```

## Status
This module is fit for production usage. 

## Development Setup
If you wish to build this module from source, in order to run against other NGINX versions, or to change the module's logic, see the [Development Wiki](https://github.com/curityio/nginx_phantom_token_module/wiki) for instructions.

## More Information
For more information about the Curity Identity Server, its capabilities, and how to use it to issue phantom tokens for microservices, visit [curity.io](https://curity.io/product/token-service/?=use-cases?tab=microservices). For background information on using the Curity Identity Server to secure API access, see our [API security resources](https://curity.io/resources/api-security).

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-phantom-token](https://github.com/curityio/nginx_phantom_token_module){target=_blank}.

# *postgres*: PostgreSQL module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-postgres
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_postgres_module.so;
```


This document describes nginx-module-postgres [v1.0](https://github.com/dvershinin/ngx_postgres/releases/tag/1.0){target=_blank} 
released on Aug 22 2020.

<hr />
`ngx_postgres` is an upstream module that allows `nginx` to communicate directly
with `PostgreSQL` database.

Response is generated in `rds` format, so it's compatible with `ngx_rds_json`
and `ngx_drizzle` modules.


## Status
This module is production-ready and it's compatible with following nginx
releases:

- 0.7.x (tested with 0.7.60 to 0.7.69),
- 0.8.x (tested with 0.8.0 to 0.8.55),
- 0.9.x (tested with 0.9.0 to 0.9.7),
- 1.0.x (tested with 1.0.0 to 1.0.11),
- 1.1.x (tested with 1.1.0 to 1.1.12).
- 1.2.x (tested with 1.2.3 to 1.2.3).
- 1.3.x (tested with 1.3.4 to 1.3.4).


## Configuration directives
## postgres_server
* **syntax**: `postgres_server ip[:port] dbname=dbname user=user password=pass`
* **default**: `none`
* **context**: `upstream`

Set details about the database server.


## postgres_keepalive
* **syntax**: `postgres_keepalive off | max=count [mode=single|multi] [overflow=ignore|reject]`
* **default**: `max=10 mode=single overflow=ignore`
* **context**: `upstream`

Configure keepalive parameters:

- `max`      - maximum number of keepalive connections (per worker process),
- `mode`     - backend matching mode,
- `overflow` - either `ignore` the fact that keepalive connection pool is full
  and allow request, but close connection afterwards or `reject` request with
  `503 Service Unavailable` response.


## postgres_pass
* **syntax**: `postgres_pass upstream`
* **default**: `none`
* **context**: `location`, `if location`

Set name of an upstream block that will be used for the database connections
(it can include variables).


## postgres_query
* **syntax**: `postgres_query [methods] query`
* **default**: `none`
* **context**: `http`, `server`, `location`, `if location`

Set query string (it can include variables). When methods are specified then
query is used only for them, otherwise it's used for all methods.

This directive can be used more than once within same context.


## postgres_rewrite
* **syntax**: `postgres_rewrite [methods] condition [=]status_code`
* **default**: `none`
* **context**: `http`, `server`, `location`, `if location`

Rewrite response `status_code` when given condition is met (first one wins!):

- `no_changes` - no rows were affected by the query,
- `changes`    - at least one row was affected by the query,
- `no_rows`    - no rows were returned in the result-set,
- `rows`       - at least one row was returned in the result-set.

When `status_code` is prefixed with `=` sign then original response body is
send to the client instead of the default error page for given `status_code`.

By design both `no_changes` and `changes` apply only to `INSERT`,
`UPDATE`, `DELETE`, `MOVE`, `FETCH` and `COPY` SQL queries.

This directive can be used more than once within same context.


## postgres_output
* **syntax**: `postgres_output rds|text|value|binary_value|none`
* **default**: `rds`
* **context**: `http`, `server`, `location`, `if location`

Set output format:

- `rds`          - return all values from the result-set in `rds` format
  (with appropriate `Content-Type`),
- `text`         - return all values from the result-set in text format
  (with default `Content-Type`), values are separated by new line,
- `value`        - return single value from the result-set in text format
  (with default `Content-Type`),
- `binary_value` - return single value from the result-set in binary format
  (with default `Content-Type`),
- `none`         - don't return anything, this should be used only when
  extracting values with `postgres_set` for use with other modules (without
  `Content-Type`).


## postgres_set
* **syntax**: `postgres_set $variable row column [optional|required]`
* **default**: `none`
* **context**: `http`, `server`, `location`

Get single value from the result-set and keep it in $variable.

When requirement level is set to `required` and value is either out-of-range,
`NULL` or zero-length, then nginx returns `500 Internal Server Error` response.
Such condition is silently ignored when requirement level is set to `optional`
(default).

Row and column numbers start at 0. Column name can be used instead of column
number.

This directive can be used more than once within same context.


## postgres_escape
* **syntax**: `postgres_escape $escaped [[=]$unescaped]`
* **default**: `none`
* **context**: `http`, `server`, `location`

Escape and quote `$unescaped` string. Result is stored in `$escaped` variable
which can be safely used in SQL queries.

Because nginx cannot tell the difference between empty and non-existing strings,
all empty strings are by default escaped to `NULL` value. This behavior can be
disabled by prefixing `$unescaped` string with `=` sign.


## postgres_connect_timeout
* **syntax**: `postgres_connect_timeout timeout`
* **default**: `10s`
* **context**: `http`, `server`, `location`

Set timeout for connecting to the database.


## postgres_result_timeout
* **syntax**: `postgres_result_timeout timeout`
* **default**: `30s`
* **context**: `http`, `server`, `location`

Set timeout for receiving result from the database.


## Configuration variables
## $postgres_columns
Number of columns in received result-set.


## $postgres_rows
Number of rows in received result-set.


## $postgres_affected
Number of rows affected by `INSERT`, `UPDATE`, `DELETE`, `MOVE`, `FETCH`
or `COPY` SQL query.


## $postgres_query
SQL query, as seen by `PostgreSQL` database.


## Sample configurations
## Sample configuration #1
Return content of table `cats` (in `rds` format).

    http {
        upstream database {
            postgres_server  127.0.0.1 dbname=test
                             user=test password=test;
        }

        server {
            location / {
                postgres_pass   database;
                postgres_query  "SELECT * FROM cats";
            }
        }
    }


## Sample configuration #2
Return only those rows from table `sites` that match `host` filter which
is evaluated for each request based on its `$http_host` variable.

    http {
        upstream database {
            postgres_server  127.0.0.1 dbname=test
                             user=test password=test;
        }

        server {
            location / {
                postgres_pass   database;
                postgres_query  SELECT * FROM sites WHERE host='$http_host'";
            }
        }
    }


## Sample configuration #3
Pass request to the backend selected from the database (traffic router).

    http {
        upstream database {
            postgres_server  127.0.0.1 dbname=test
                             user=test password=test;
        }

        server {
            location / {
                eval_subrequest_in_memory  off;

                eval $backend {
                    postgres_pass    database;
                    postgres_query   "SELECT * FROM backends LIMIT 1";
                    postgres_output  value 0 0;
                }

                proxy_pass  $backend;
            }
        }
    }

Required modules (other than `ngx_postgres`):

- [nginx-eval-module (agentzh's fork)](http://github.com/agentzh/nginx-eval-module),


## Sample configuration #4
Restrict access to local files by authenticating against `PostgreSQL` database.

    http {
        upstream database {
            postgres_server  127.0.0.1 dbname=test
                             user=test password=test;
        }

        server {
            location = /auth {
                internal;

                postgres_escape   $user $remote_user;
                postgres_escape   $pass $remote_passwd;

                postgres_pass     database;
                postgres_query    "SELECT login FROM users WHERE login=$user AND pass=$pass";
                postgres_rewrite  no_rows 403;
                postgres_output   none;
            }

            location / {
                auth_request      /auth;
                root              /files;
            }
        }
    }

Required modules (other than `ngx_postgres`):

- [ngx_http_auth_request_module](http://mdounin.ru/hg/ngx_http_auth_request_module/),
- [ngx_coolkit](http://github.com/FRiCKLE/ngx_coolkit).


## Sample configuration #5
Simple RESTful webservice returning JSON responses with appropriate HTTP status
codes.

    http {
        upstream database {
            postgres_server  127.0.0.1 dbname=test
                             user=test password=test;
        }

        server {
            set $random  123;

            location = /numbers/ {
                postgres_pass     database;
                rds_json          on;

                postgres_query    HEAD GET  "SELECT * FROM numbers";

                postgres_query    POST      "INSERT INTO numbers VALUES('$random') RETURNING *";
                postgres_rewrite  POST      changes 201;

                postgres_query    DELETE    "DELETE FROM numbers";
                postgres_rewrite  DELETE    no_changes 204;
                postgres_rewrite  DELETE    changes 204;
            }

            location ~ /numbers/(?<num>\d+) {
                postgres_pass     database;
                rds_json          on;

                postgres_query    HEAD GET  "SELECT * FROM numbers WHERE number='$num'";
                postgres_rewrite  HEAD GET  no_rows 410;

                postgres_query    PUT       "UPDATE numbers SET number='$num' WHERE number='$num' RETURNING *";
                postgres_rewrite  PUT       no_changes 410;

                postgres_query    DELETE    "DELETE FROM numbers WHERE number='$num'";
                postgres_rewrite  DELETE    no_changes 410;
                postgres_rewrite  DELETE    changes 204;
            }
        }
    }

Required modules (other than `ngx_postgres`):

- [ngx_rds_json](http://github.com/agentzh/rds-json-nginx-module).

## Sample configuration #6
Use GET parameter in SQL query.

    location /quotes {
        set_unescape_uri  $txt $arg_txt;
        postgres_escape   $txt;
        postgres_pass     database;
        postgres_query    "SELECT * FROM quotes WHERE quote=$txt";
    }

Required modules (other than `ngx_postgres`):

- [ngx_set_misc](http://github.com/agentzh/set-misc-nginx-module).

## Testing
`ngx_postgres` comes with complete test suite based on [Test::Nginx](http://github.com/agentzh/test-nginx).

You can test core functionality by running:

`$ TEST_NGINX_IGNORE_MISSING_DIRECTIVES=1 prove`

You can also test interoperability with following modules:

- [ngx_coolkit](http://github.com/FRiCKLE/ngx_coolkit),
- [ngx_echo](github.com/agentzh/echo-nginx-module),
- [ngx_form_input](http://github.com/calio/form-input-nginx-module),
- [ngx_set_misc](http://github.com/agentzh/set-misc-nginx-module),
- [ngx_http_auth_request_module](http://mdounin.ru/hg/ngx_http_auth_request_module/),
- [nginx-eval-module (agentzh's fork)](http://github.com/agentzh/nginx-eval-module),
- [ngx_rds_json](http://github.com/agentzh/rds-json-nginx-module).

by running:

`$ prove`


## See also
- [ngx_rds_json](http://github.com/agentzh/rds-json-nginx-module),
- [ngx_drizzle](http://github.com/chaoslawful/drizzle-nginx-module),
- [ngx_lua](http://github.com/chaoslawful/lua-nginx-module),
- [nginx-eval-module (agentzh's fork)](http://github.com/agentzh/nginx-eval-module).

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-postgres](https://github.com/dvershinin/ngx_postgres){target=_blank}.

# *pta*: Period of Time Authentication module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-pta
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_pta_module.so;
```


This document describes nginx-module-pta [v1.0.1](https://github.com/iij/pta/releases/tag/v1.0.1){target=_blank} 
released on Jul 04 2022.

<hr />

## Overview

PTA(Period of Time Authentication) module is a module for NGINX. Using
PTA you can control access to your contents. PTA calcurates a
encrypted query string or cookie parameter that includes an expiration
time and a path of the content.

## Usage

It's an example of nginx.conf below.

```
  worker_processes  1;
  
  events {
      worker_connections  1024;
  }
   
   
  http {
      include       mime.types;
      default_type  application/octet-stream;
   
      sendfile        on;
   
      keepalive_timeout  65;
   
      server {
          listen       80;
          server_name  localhost;
   
          pta_1st_key 0102030405060708090a0b0c0d0e0f00;
          pta_1st_iv  00000000000000000000000000000000;
          pta_2nd_key 11111111111111111111111111111111;
          pta_2nd_iv  22222222222222222222222222222222;
   
          location / {
              root   html;
              index  index.html index.htm;
          }
   
          location /foo/ {
              pta_enable on;
          }
   
          error_page   500 502 503 504  /50x.html;
          location = /50x.html {
              root   html;
          }
      }
  }
```


## pta_1st_key
- Syntax  : pta_1st_key   keystring
- Default : -
- Context : server


## pta_1st_iv
- Syntax  : pta_1st_iv   ivstring;
- Default : -
- Context : server


## pta_2nd_key
- Syntax  : pta_2nd_key   keystring;
- Default : -
- Context : server


## pta_2nd_iv
- Syntax  : pta_2nd_iv   ivstring;
- Default : -
- Context : server


## pta_enable
- Syntax  : pta_enable   on | off;
- Default : pta_enable off;
- Context : location

## pta_auth_method
- Syntax  : pta_auth_method qs | cookie | qs cookie;
- Default : pta_auth_method qs;
- Context : location


## How it works

PTA module decrypts a query string or cookie parameter starting from
`pta=...' and check CRC32, expiration time and requested URI path
embedded in it. So you need to generate PTA token and add it to a link
as query string or cookie parameter. There are some codes under the
smaples directory to generate PTA.

## format

This byte stream is encrypted with the AES AES 128 bit CBC mode.

```
  +---------------+-------------------------+----------+-----------------+
  | CRC32 (4byte) | Expiration Time (8byte) | URI Path | Padding         |
  |               | Unix Time format        |          | pkcs #7 format  |
  +---------------+-------------------------+----------+-----------------+
```

### CRC32
It's big endian. It's calculated from the Expiration Time and URI Path.
This part is used to check that AES decryption is valid.

### Expiration time
It's big endian. It's compared with the time that request is arrived
and if the time is less than or equal to the expiration time that is
contained in the PTA token the request is permitted.

### URI Path
Basically it must be identical with the path of requested content.

  e.g.
  http://example.com/index.html -> /index.html

It must be started from the slash `/'.

The asterisk character `*' means wildcard.

- The `\*' character must be only one.
  e.g. /foo/\*/bar/*.jpg isn't allowed.
  
- You can use the `*' character any part such as a part of directory
  name, file name or file name suffix.
  
- If you use the `*' character literally, you must escape it with the
  back slash.

## Query string and Cookie

pta_auth_method directive can specify the method to authenticate. 
You can choose the type of query string, cookie, or both as the method.

In case of both, query string is evaluated first, and then cookie
is done if pta parameter isn't included in query string.
When pta parameter in query string isn't valid the authentication 
fails, not fallback to ealuate cookie. Only without pta parameter
in query string cookie is evaluated.

<!--
## Local Variables:
## mode: auto-fill
## coding: utf-8-unix
## End:
-->

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-pta](https://github.com/iij/pta){target=_blank}.

# *push-stream*: NGINX push stream module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-push-stream
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_push_stream_module.so;
```


This document describes nginx-module-push-stream [v0.5.5](https://github.com/wandenberg/nginx-push-stream-module/releases/tag/0.5.5){target=_blank} 
released on Dec 11 2021.

<hr />

A pure stream http push technology for your Nginx setup.

[Comet](http://en.wikipedia.org/wiki/Comet_%28programming%29) made easy
and **really scalable**.

Supports [EventSource](http://dev.w3.org/html5/eventsource/),
[WebSocket](http://dev.w3.org/html5/websockets/), Long Polling, and
Forever Iframe. See [some examples](#examples) bellow.

\_This module is not distributed with the Nginx source. See [the
installation instructions](installation._)

Available on github at
[nginx_push_stream_module](https://github.com/wandenberg/nginx-push-stream-module)

## Changelog

Always take a look at [CHANGELOG.textile](CHANGELOG.textile) to see
what’s new.

## Contribute

After you try this module and like it, feel free to [give something
back](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=4LP6P9A7BC37S),
and help in the maintenance of the project ;)  
[![](https://www.paypalobjects.com/WEBSCR-640-20110429-1/en_US/i/btn/btn_donate_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=4LP6P9A7BC37S)

## Status

This module is considered production ready.

## Basic Configuration

        # add the push_stream_shared_memory_size to your http context
        http {
           push_stream_shared_memory_size 32M;

            # define publisher and subscriber endpoints in your server context
            server {
               location /channels-stats {
                    # activate channels statistics mode for this location
                    push_stream_channels_statistics;

                    # query string based channel id
                    push_stream_channels_path               $arg_id;
                }

                location /pub {
                   # activate publisher (admin) mode for this location
                   push_stream_publisher admin;

                    # query string based channel id
                    push_stream_channels_path               $arg_id;
                }

                location ~ /sub/(.*) {
                    # activate subscriber (streaming) mode for this location
                    push_stream_subscriber;

                    # positional channel path
                    push_stream_channels_path                   $1;
                }
            }
        }

## Basic Usage

You can feel the flavor right now at the command line. Try using more
than  
one terminal and start playing http pubsub:

        # Subs
        curl -s -v --no-buffer 'http://localhost/sub/my_channel_1'
        curl -s -v --no-buffer 'http://localhost/sub/your_channel_1'
        curl -s -v --no-buffer 'http://localhost/sub/your_channel_2'

        # Pubs
        curl -s -v -X POST 'http://localhost/pub?id=my_channel_1' -d 'Hello World!'
        curl -s -v -X POST 'http://localhost/pub?id=your_channel_1' -d 'Hi everybody!'
        curl -s -v -X POST 'http://localhost/pub?id=your_channel_2' -d 'Goodbye!'

        # Channels Stats for publisher (json format)
        curl -s -v 'http://localhost/pub?id=my_channel_1'

        # All Channels Stats summarized (json format)
        curl -s -v 'http://localhost/channels-stats'

        # All Channels Stats detailed (json format)
        curl -s -v 'http://localhost/channels-stats?id=ALL'

        # Prefixed Channels Stats detailed (json format)
        curl -s -v 'http://localhost/channels-stats?id=your_channel_*'

        # Channels Stats (json format)
        curl -s -v 'http://localhost/channels-stats?id=my_channel_1'

        # Delete Channels
        curl -s -v -X DELETE 'http://localhost/pub?id=my_channel_1'

## Some Examples <a name="examples" href="#"> </a>

-   [Curl examples](docs/examples/curl.textile#curl)
-   [Forever (hidden)
    iFrame](docs/examples/forever_iframe.textile#forever_iframe)
-   [Event Source](docs/examples/event_source.textile#event_source)
-   [WebSocket](docs/examples/websocket.textile#websocket)
-   [Long Polling](docs/examples/long_polling.textile#long_polling)
-   [JSONP](docs/examples/long_polling.textile#jsonp)
-   [M-JPEG](docs/examples/m_jpeg.textile#m_jpeg)
-   [Other
    examples](https://github.com/wandenberg/nginx-push-stream-module/wiki/_pages)

## FAQ <a names="faq" href="#"> </a>

Doubts?! Check the
[FAQ](https://github.com/wandenberg/nginx-push-stream-module/wiki/_pages).

## Bug report <a name="bug_report" href="#"> </a>

To report a bug, please provide the following information when
applicable

1.  Which push stream module version is been used (commit sha1)?
2.  Which nginx version is been used?
3.  Nginx configuration in use
4.  “nginx -V” command outuput
5.  Core dump indicating a failure on the module code. Check
    [here](http://wiki.nginx.org/Debugging) how to produce one.
6.  Step by step description to reproduce the error.

## Who is using the module? <a names="faq" href="#"> </a>

Do you use this module? Put your name on the
[list](https://github.com/wandenberg/nginx-push-stream-module/wiki/_pages).

## Javascript Client <a name="javascript_client" href="#"> </a>

There is a javascript client implementation
[here](docs/javascript_client.textile#javascript_client), which is
framework independent. Try and help improve it. ;)

## Directives

\(1\) Defining locations, (2) Main configuration, (3) Subscribers
configuration, (4) Publishers configuration, (5) Channels Statistics
configuration, (6) WebSocket configuration

|                                                                                                                                        |       |       |       |       |       |       |
|----------------------------------------------------------------------------------------------------------------------------------------|-------|-------|-------|-------|-------|-------|
| Directive                                                                                                                              | \(1\) | \(2\) | \(3\) | \(4\) | \(5\) | \(6\) |
| [push_stream_channels_statistics](docs/directives/channels_statistics.textile#push_stream_channels_statistics)                         |   x   |   -   |   -   |   -   |   -   |   -   |
| [push_stream_publisher](docs/directives/publishers.textile#push_stream_publisher)                                                      |   x   |   -   |   -   |   -   |   -   |   -   |
| [push_stream_subscriber](docs/directives/subscribers.textile#push_stream_subscriber)                                                   |   x   |   -   |   -   |   -   |   -   |   -   |
| [push_stream_shared_memory_size](docs/directives/main.textile#push_stream_shared_memory_size)                                          |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_channel_deleted_message_text](docs/directives/main.textile#push_stream_channel_deleted_message_text)                      |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_channel_inactivity_time](docs/directives/main.textile#push_stream_channel_inactivity_time)                                |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_ping_message_text](docs/directives/main.textile#push_stream_ping_message_text)                                            |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_timeout_with_body](docs/directives/subscribers.textile#push_stream_timeout_with_body)                                     |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_message_ttl](docs/directives/main.textile#push_stream_message_ttl)                                                        |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_max_subscribers_per_channel](docs/directives/main.textile#push_stream_max_subscribers_per_channel)                        |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_max_messages_stored_per_channel](docs/directives/main.textile#push_stream_max_messages_stored_per_channel)                |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_max_channel_id_length](docs/directives/main.textile#push_stream_max_channel_id_length)                                    |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_max_number_of_channels](docs/directives/main.textile#push_stream_max_number_of_channels)                                  |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_max_number_of_wildcard_channels](docs/directives/main.textile#push_stream_max_number_of_wildcard_channels)                |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_wildcard_channel_prefix](docs/directives/main.textile#push_stream_wildcard_channel_prefix)                                |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_events_channel_id](docs/directives/main.textile#push_stream_events_channel_id)                                            |   -   |   x   |   -   |   -   |   -   |   -   |
| [push_stream_channels_path](docs/directives/subscribers.textile#push_stream_channels_path)                                             |   -   |   -   |   x   |   x   |   x   |   x   |
| [push_stream_store_messages](docs/directives/publishers.textile#push_stream_store_messages)                                            |   -   |   -   |   -   |   x   |   -   |   x   |
| [push_stream_channel_info_on_publish](docs/directives/publishers.textile#push_stream_channel_info_on_publish)                          |   -   |   -   |   -   |   x   |   -   |   -   |
| [push_stream_authorized_channels_only](docs/directives/subscribers.textile#push_stream_authorized_channels_only)                       |   -   |   -   |   x   |   -   |   -   |   x   |
| [push_stream_header_template_file](docs/directives/subscribers.textile#push_stream_header_template_file)                               |   -   |   -   |   x   |   -   |   -   |   x   |
| [push_stream_header_template](docs/directives/subscribers.textile#push_stream_header_template)                                         |   -   |   -   |   x   |   -   |   -   |   x   |
| [push_stream_message_template](docs/directives/subscribers.textile#push_stream_message_template)                                       |   -   |   -   |   x   |   -   |   -   |   x   |
| [push_stream_footer_template](docs/directives/subscribers.textile#push_stream_footer_template)                                         |   -   |   -   |   x   |   -   |   -   |   x   |
| [push_stream_wildcard_channel_max_qtd](docs/directives/subscribers.textile#push_stream_wildcard_channel_max_qtd)                       |   -   |   -   |   x   |   -   |   -   |   x   |
| [push_stream_ping_message_interval](docs/directives/subscribers.textile#push_stream_ping_message_interval)                             |   -   |   -   |   x   |   -   |   -   |   x   |
| [push_stream_subscriber_connection_ttl](docs/directives/subscribers.textile#push_stream_subscriber_connection_ttl)                     |   -   |   -   |   x   |   -   |   -   |   x   |
| [push_stream_longpolling_connection_ttl](docs/directives/subscribers.textile#push_stream_longpolling_connection_ttl)                   |   -   |   -   |   x   |   -   |   -   |   -   |
| [push_stream_websocket_allow_publish](docs/directives/subscribers.textile#push_stream_websocket_allow_publish)                         |   -   |   -   |   -   |   -   |   -   |   x   |
| [push_stream_last_received_message_time](docs/directives/subscribers.textile#push_stream_last_received_message_time)                   |   -   |   -   |   x   |   -   |   -   |   -   |
| [push_stream_last_received_message_tag](docs/directives/subscribers.textile#push_stream_last_received_message_tag)                     |   -   |   -   |   x   |   -   |   -   |   -   |
| [push_stream_last_event_id](docs/directives/subscribers.textile#push_stream_last_event_id)                                             |   -   |   -   |   x   |   -   |   -   |   -   |
| [push_stream_user_agent](docs/directives/subscribers.textile#push_stream_user_agent)                                                   |   -   |   -   |   x   |   -   |   -   |   -   |
| [push_stream_padding_by_user_agent](docs/directives/subscribers.textile#push_stream_padding_by_user_agent)                             |   -   |   -   |   x   |   -   |   -   |   -   |
| [push_stream_allowed_origins](docs/directives/subscribers.textile#push_stream_allowed_origins)                                         |   -   |   -   |   x   |   -   |   -   |   -   |
| [push_stream_allow_connections_to_events_channel](docs/directives/subscribers.textile#push_stream_allow_connections_to_events_channel) |   -   |   -   |   x   |   -   |   -   |   x   |

## Installation <a name="installation" href="#"> </a>

        # clone the project
        git clone https://github.com/wandenberg/nginx-push-stream-module.git
        NGINX_PUSH_STREAM_MODULE_PATH=$PWD/nginx-push-stream-module

        # get desired nginx version (works with 1.2.0+)
        wget http://nginx.org/download/nginx-1.2.0.tar.gz

        # unpack, configure and build
        tar xzvf nginx-1.2.0.tar.gz
        cd nginx-1.2.0
        ./configure --add-module=../nginx-push-stream-module
        make

        # install and finish
        sudo make install

        # check
        sudo /usr/local/nginx/sbin/nginx -v
            nginx version: nginx/1.2.0

        # test configuration
        sudo /usr/local/nginx/sbin/nginx -c $NGINX_PUSH_STREAM_MODULE_PATH/misc/nginx.conf -t
            the configuration file $NGINX_PUSH_STREAM_MODULE_PATH/misc/nginx.conf syntax is ok
            configuration file $NGINX_PUSH_STREAM_MODULE_PATH/misc/nginx.conf test is successful

        # run
        sudo /usr/local/nginx/sbin/nginx -c $NGINX_PUSH_STREAM_MODULE_PATH/misc/nginx.conf

## Memory usage

Just as information is listed below the minimum amount of memory used
for each object:

-   message on shared = 200 bytes
-   channel on shared = 270 bytes
-   subscriber  
    on shared = 160 bytes  
    on system = 6550 bytes

## Tests

The server tests for this module are written in Ruby, and are acceptance
tests, click [here](docs/server_tests.textile) for more details.

## Discussion

Nginx Push Stream Module [Discussion
Group](https://groups.google.com/group/nginxpushstream)

## Contributors

[People](https://github.com/wandenberg/nginx-push-stream-module/contributors)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-push-stream](https://github.com/wandenberg/nginx-push-stream-module){target=_blank}.

# *[BETA!] rdns*: NGINX HTTP rDNS module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-rdns
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_rdns_module.so;
```


This document describes nginx-module-rdns [v0](https://github.com/dvershinin/nginx-http-rdns/releases/tag/v0){target=_blank} 
released on Jun 08 2020.

Production stability is *not guaranteed*.
<hr />

## Summary

This module allows to make a reverse DNS (rDNS) lookup for incoming
connection and provides simple access control of incoming hostname
by allow/deny rules (similar to HttpAccessModule allow/deny
directives; regular expressions are supported). Module works with
the DNS server defined by the standard resolver directive.
This module uses nginx core resolver cache when resolving DNS lookup,
for a maximum of 30 seconds or DNS response TTL.

## Example

    location / {
        resolver 127.0.0.1;

        rdns_deny badone\.example\.com;

        if ($http_user_agent ~* FooAgent) {
            rdns on;
        }

        if ($rdns_hostname ~* (foo\.example\.com)) {
            set $myvar foo;
        }

        #...
    }

In the example above, nginx will make a reverse DNS request (through
the 127.0.0.1 DNS server) for each request having the "FooAgent"
user agent. Requests from badone.example.com will be forbidden.
The $rdns_hostname variable will have the rDNS request result or
"not found" (in case it's not found or any error occured) for any
requests made by FooAgent. For other user agents, $rdns_hostname
will have a special value "-".


## Directives

### rdns

* Syntax: rdns on | off | double
* Default: -
* Context: http, server, location, if-in-server, if-in-location
* Phase: rewrite
* Variables: rdns_hostname

Enables/disables rDNS lookups.

* on     - enable rDNS lookup in this context.
* double - enable double DNS lookup in this context. If the reverse
           lookup (rDNS request) succeeded, module performs a forward
           lookup (DNS request) for its result. If this forward
           lookup has failed or none of the forward lookup IP
           addresses have matched the original address,
           $rdns_hostname is set to "not found".
* off    - disable rDNS lookup in this context.

The $rdns_hostname variable may have:

* result of lookup;
* special value "not found" if not found or error occurred during
  request;
* special value "-" if lookup disabled.

After performing a lookup, module restarts request handling pipeline
to make new $rdns_hostname variable value visible to other directives.

Notice on server/location "if":

Internally, in server's or location's "if", module works through
rewrite module codes. When any enabling directive (rdns on|double) is
executed for the first time, it enables DNS lookup and makes a break
(to prevent executing further directives in this "if"). After the
lookup is done, directives in "if" using rewrite module codes are
executed for the second time, without any breaks. Disabling directive
(rdns off) makes no breaks.

Core module resolver should be defined to use this directive.


### rdns_allow

* Syntax: rdns_allow regex
* Default: -
* Context: http, server, location
* Phase: access
* Variables: -

Grants access for domain matched by regular expression.


### rdns_deny

* Syntax: rdns_deny regex
* Default: -
* Context: http, server, location
* Phase: access
* Variables: -

Forbids access for domain matched by regular expression.


## Notice on access lists

The rdns_allow and rdns_deny directives define a new access list for
the context in which they are used.

Access list inheritance in contexts works only if child context
doesn't define own rules.


## Warning on named locations

Making rDNS requests in named locations isn't supported and may
invoke a loop. For example:

    server {
        rdns on;

        location / {
            echo_exec @foo;
        }

        location @foo {
            #...
        }
    }

Being in a named location and restarting request handling pipeline,
nginx continue its request handling in usual (unnamed) location.
That's why this example will make a loop if you don't disable the
module in your named location. The correct config for this example
should be as follows:

    server {
        rdns on;

        location / {
            echo_exec @foo;
        }

        location @foo {
            rdns off;
            #...
        }
    }


## Links

* The source code on GitHub:
  https://github.com/flant/nginx-http-rdns
* The module homepage (in Russian):
  http://flant.ru/projects/nginx-http-rdns

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-rdns](https://github.com/dvershinin/nginx-http-rdns){target=_blank}.

# *redis2*: NGINX upstream module for the Redis 2.0 protocol


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-redis2
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_redis2_module.so;
```


This document describes nginx-module-redis2 [v0.15](https://github.com/openresty/redis2-nginx-module/releases/tag/v0.15){target=_blank} 
released on Apr 19 2018.

<hr />

ngx_redis2 - Nginx upstream module for the Redis 2.0 protocol


## Status

This module is already production ready.

## Synopsis

```nginx

 location = /foo {
     set $value 'first';
     redis2_query set one $value;
     redis2_pass 127.0.0.1:6379;
 }

 # GET /get?key=some_key
 location = /get {
     set_unescape_uri $key $arg_key;  # this requires ngx_set_misc
     redis2_query get $key;
     redis2_pass foo.com:6379;
 }

 # GET /set?key=one&val=first%20value
 location = /set {
     set_unescape_uri $key $arg_key;  # this requires ngx_set_misc
     set_unescape_uri $val $arg_val;  # this requires ngx_set_misc
     redis2_query set $key $val;
     redis2_pass foo.com:6379;
 }

 # multiple pipelined queries
 location = /foo {
     set $value 'first';
     redis2_query set one $value;
     redis2_query get one;
     redis2_query set one two;
     redis2_query get one;
     redis2_pass 127.0.0.1:6379;
 }

 location = /bar {
     # $ is not special here...
     redis2_literal_raw_query '*1\r\n$4\r\nping\r\n';
     redis2_pass 127.0.0.1:6379;
 }

 location = /bar {
     # variables can be used below and $ is special
     redis2_raw_query 'get one\r\n';
     redis2_pass 127.0.0.1:6379;
 }

 # GET /baz?get%20foo%0d%0a
 location = /baz {
     set_unescape_uri $query $query_string; # this requires the ngx_set_misc module
     redis2_raw_query $query;
     redis2_pass 127.0.0.1:6379;
 }

 location = /init {
     redis2_query del key1;
     redis2_query lpush key1 C;
     redis2_query lpush key1 B;
     redis2_query lpush key1 A;
     redis2_pass 127.0.0.1:6379;
 }

 location = /get {
     redis2_query lrange key1 0 -1;
     redis2_pass 127.0.0.1:6379;
 }
```


## Description

This is an Nginx upstream module that makes nginx talk to a [Redis](http://redis.io/) 2.x server in a non-blocking way. The full Redis 2.0 unified protocol has been implemented including the Redis pipelining support.

This module returns the raw TCP response from the Redis server. It's recommended to use my [lua-redis-parser](http://github.com/openresty/lua-redis-parser) (written in pure C) to parse these responses into lua data structure when combined with [lua-nginx-module](http://github.com/openresty/lua-nginx-module).

When used in conjunction with [lua-nginx-module](http://github.com/openresty/lua-nginx-module), it is recommended to use the [lua-resty-redis](http://github.com/openresty/lua-resty-redis) library instead of this module though, because the former is much more flexible and memory-efficient.

If you only want to use the `get` redis command, you can try out the [HttpRedisModule](http://wiki.nginx.org/HttpRedisModule). It returns the parsed content part of the Redis response because only `get` is needed to implement.

Another option is to parse the redis responses on your client side yourself.


## Directives


## redis2_query
**syntax:** *redis2_query cmd arg1 arg2 ...*

**default:** *no*

**context:** *location, location if*

Specify a Redis command by specifying its individual arguments (including the Redis command name itself) in a similar way to the `redis-cli` utility.

Multiple instances of this directive are allowed in a single location and these queries will be pipelined. For example,

```nginx

 location = /pipelined {
     redis2_query set hello world;
     redis2_query get hello;

     redis2_pass 127.0.0.1:$TEST_NGINX_REDIS_PORT;
 }
```

then `GET /pipelined` will yield two successive raw Redis responses

```nginx

 +OK
 $5
 world
```

while newlines here are actually `CR LF` (`\r\n`).


## redis2_raw_query
**syntax:** *redis2_raw_query QUERY*

**default:** *no*

**context:** *location, location if*

Specify raw Redis queries and nginx variables are recognized in the `QUERY` argument.

Only *one* Redis command is allowed in the `QUERY` argument, or you'll receive an error. If you want to specify multiple pipelined commands in a single query, use the [redis2_raw_queries](#redis2_raw_queries) directive instead.


## redis2_raw_queries
**syntax:** *redis2_raw_queries N QUERIES*

**default:** *no*

**context:** *location, location if*

Specify `N` commands in the `QUERIES` argument. Both the `N` and `QUERIES`
arguments can take Nginx variables.

Here's some examples
```nginx

 location = /pipelined {
     redis2_raw_queries 3 "flushall\r\nget key1\r\nget key2\r\n";
     redis2_pass 127.0.0.1:6379;
 }

 # GET /pipelined2?n=2&cmds=flushall%0D%0Aget%20key%0D%0A
 location = /pipelined2 {
     set_unescape_uri $n $arg_n;
     set_unescape_uri $cmds $arg_cmds;

     redis2_raw_queries $n $cmds;

     redis2_pass 127.0.0.1:6379;
 }
```
Note that in the second sample above, the [set_unescape_uri](http://github.com/openresty/set-misc-nginx-module#set_unescape_uri) directive is provided by the [set-misc-nginx-module](http://github.com/openresty/set-misc-nginx-module).


## redis2_literal_raw_query
**syntax:** *redis2_literal_raw_query QUERY*

**default:** *no*

**context:** *location, location if*

Specify a raw Redis query but Nginx variables in it will not be *not* recognized. In other words, you're free to use the dollar sign character (`$`) in your `QUERY` argument.

Only One redis command is allowed in the `QUERY` argument.


## redis2_pass
**syntax:** *redis2_pass &lt;upstream_name&gt;*

**syntax:** *redis2_pass &lt;host&gt;:&lt;port&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *content*

Specify the Redis server backend. 


## redis2_connect_timeout
**syntax:** *redis2_connect_timeout &lt;time&gt;*

**default:** *60s*

**context:** *http, server, location*

The timeout for connecting to the Redis server, in seconds by default.

It's wise to always explicitly specify the time unit to avoid confusion. Time units supported are `s`(seconds), `ms`(milliseconds), `y`(years), `M`(months), `w`(weeks), `d`(days), `h`(hours), and `m`(minutes).

This time must be less than 597 hours.


## redis2_send_timeout
**syntax:** *redis2_send_timeout &lt;time&gt;*

**default:** *60s*

**context:** *http, server, location*

The timeout for sending TCP requests to the Redis server, in seconds by default.

It's wise to always explicitly specify the time unit to avoid confusion. Time units supported are `s`(seconds), `ms`(milliseconds), `y`(years), `M`(months), `w`(weeks), `d`(days), `h`(hours), and `m`(minutes).


## redis2_read_timeout
**syntax:** *redis2_read_timeout &lt;time&gt;*

**default:** *60s*

**context:** *http, server, location*

The timeout for reading TCP responses from the redis server, in seconds by default.

It's wise to always explicitly specify the time unit to avoid confusion. Time units supported are `s`(seconds), `ms`(milliseconds), `y`(years), `M`(months), `w`(weeks), `d`(days), `h`(hours), and `m`(minutes).


## redis2_buffer_size
**syntax:** *redis2_buffer_size &lt;size&gt;*

**default:** *4k/8k*

**context:** *http, server, location*

This buffer size is used for reading Redis replies, but it's not required to be as big as the largest possible Redis reply.

This default size is the page size, may be 4k or 8k.


## redis2_next_upstream
**syntax:** *redis2_next_upstream [ error | timeout | invalid_response | off ]*

**default:** *error timeout*

**context:** *http, server, location*

Specify which failure conditions should cause the request to be forwarded to another
upstream server. Applies only when the value in [redis2_pass](#redis2_pass) is an upstream with two or more
servers.

Here's an artificial example:
```nginx

 upstream redis_cluster {
     server 127.0.0.1:6379;
     server 127.0.0.1:6380;
 }

 server {
     location = /redis {
         redis2_next_upstream error timeout invalid_response;
         redis2_query get foo;
         redis2_pass redis_cluster;
     }
 }
```


## Connection Pool

You can use the excellent [HttpUpstreamKeepaliveModule](http://wiki.nginx.org/HttpUpstreamKeepaliveModule) with this module to provide TCP connection pool for Redis.

A sample config snippet looks like this

```nginx

 http {
     upstream backend {
       server 127.0.0.1:6379;

       # a pool with at most 1024 connections
       # and do not distinguish the servers:
       keepalive 1024;
     }

     server {
         ...
         location = /redis {
             set_unescape_uri $query $arg_query;
             redis2_query $query;
             redis2_pass backend;
         }
     }
 }
```


## Selecting Redis Databases

Redis provides the [select](http://redis.io/commands/SELECT) command to switch Redis databaess. This command is no different from other normal commands
like [get](http://redis.io/commands/GET) or [set](http://redis.io/commands/SET). So you can use them in [redis2_query](#redis2_query) directives, for
example,

```nginx
redis2_query select 8;
redis2_query get foo;
```


## Lua Interoperability

This module can be served as a non-blocking redis2 client for [lua-nginx-module](http://github.com/openresty/lua-nginx-module) (but nowadays it is recommended to use the [lua-resty-redis](http://github.com/openresty/lua-resty-redis) library instead, which is much simpler to use and more efficient most of the time).
Here's an example using a GET subrequest:

```nginx

 location = /redis {
     internal;

     # set_unescape_uri is provided by ngx_set_misc
     set_unescape_uri $query $arg_query;

     redis2_raw_query $query;
     redis2_pass 127.0.0.1:6379;
 }

 location = /main {
     content_by_lua '
         local res = ngx.location.capture("/redis",
             { args = { query = "ping\\r\\n" } }
         )
         ngx.print("[" .. res.body .. "]")
     ';
 }
```

Then accessing `/main` yields


    [+PONG\r\n]


where `\r\n` is `CRLF`. That is, this module returns the *raw* TCP responses from the remote redis server. For Lua-based application developers, they may want to utilize the [lua-redis-parser](http://github.com/openresty/lua-redis-parser) library (written in pure C) to parse such raw responses into Lua data structures.

When moving the inlined Lua code into an external `.lua` file, it's important to use the escape sequence `\r\n` directly. We used `\\r\\n` above just because the Lua code itself needs quoting when being put into an Nginx string literal.

You can also use POST/PUT subrequests to transfer the raw Redis request via request body, which does not require URI escaping and unescaping, thus saving some CPU cycles. Here's such an example:

```nginx

 location = /redis {
     internal;

     # $echo_request_body is provided by the ngx_echo module
     redis2_raw_query $echo_request_body;

     redis2_pass 127.0.0.1:6379;
 }

 location = /main {
     content_by_lua '
         local res = ngx.location.capture("/redis",
             { method = ngx.HTTP_PUT,
               body = "ping\\r\\n" }
         )
         ngx.print("[" .. res.body .. "]")
     ';
 }
```

This yeilds exactly the same output as the previous (GET) sample.

One can also use Lua to pick up a concrete Redis backend based on some complicated hashing rules. For instance,

```nginx

 upstream redis-a {
     server foo.bar.com:6379;
 }

 upstream redis-b {
     server bar.baz.com:6379;
 }

 upstream redis-c {
     server blah.blah.org:6379;
 }

 server {
     ...

     location = /redis {
         set_unescape_uri $query $arg_query;
         redis2_query $query;
         redis2_pass $arg_backend;
     }

     location = /foo {
         content_by_lua "
             -- pick up a server randomly
             local servers = {'redis-a', 'redis-b', 'redis-c'}
             local i = ngx.time() % #servers + 1;
             local srv = servers[i]

             local res = ngx.location.capture('/redis',
                 { args = {
                     query = '...',
                     backend = srv
                   }
                 }
             )
             ngx.say(res.body)
         ";
     }
 }
```


## Pipelined Redis Requests by Lua

Here's a complete example demonstrating how to use Lua to issue multiple pipelined Redis requests via this Nginx module.

First of all, we include the following in our `nginx.conf` file:

```nginx

 location = /redis2 {
     internal;

     redis2_raw_queries $args $echo_request_body;
     redis2_pass 127.0.0.1:6379;
 }

 location = /test {
     content_by_lua_file conf/test.lua;
 }
```

Basically we use URI query args to pass the number of Redis requests and request body to pass the pipelined Redis request string.

And then we create the `conf/test.lua` file (whose path is relative to the server root of Nginx) to include the following Lua code:

```lua

 -- conf/test.lua
 local parser = require "redis.parser"

 local reqs = {
     {"set", "foo", "hello world"},
     {"get", "foo"}
 }

 local raw_reqs = {}
 for i, req in ipairs(reqs) do
     table.insert(raw_reqs, parser.build_query(req))
 end

 local res = ngx.location.capture("/redis2?" .. #reqs,
     { body = table.concat(raw_reqs, "") })

 if res.status ~= 200 or not res.body then
     ngx.log(ngx.ERR, "failed to query redis")
     ngx.exit(500)
 end

 local replies = parser.parse_replies(res.body, #reqs)
 for i, reply in ipairs(replies) do
     ngx.say(reply[1])
 end
```

Here we assume that your Redis server is listening on the default port (6379) of the localhost. We also make use of the [lua-redis-parser](http://github.com/openresty/lua-redis-parser) library to construct raw Redis queries for us and also use it to parse the replies.

Accessing the `/test` location via HTTP clients like `curl` yields the following output


    OK
    hello world


A more realistic setting is to use a proper upstream definition for our Redis backend and enable TCP connection pool via the [keepalive](http://wiki.nginx.org/HttpUpstreamKeepaliveModule#keepalive) directive in it.


## Redis Publish/Subscribe Support

This module has limited support for Redis publish/subscribe feature. It cannot be fully supported due to the stateless nature of REST and HTTP model.

Consider the following example:

```nginx

 location = /redis {
     redis2_raw_queries 2 "subscribe /foo/bar\r\n";
     redis2_pass 127.0.0.1:6379;
 }
```

And then publish a message for the key `/foo/bar` in the `redis-cli` command line. And then you'll receive two multi-bulk replies from the `/redis` location.

You can surely parse the replies with the [lua-redis-parser](http://github.com/openresty/lua-redis-parser) library if you're using Lua to access this module's location.


## Limitations For Redis Publish/Subscribe

If you want to use the [Redis pub/sub](http://redis.io/topics/pubsub) feature with this module, then you must note the following limitations:

* You cannot use [HttpUpstreamKeepaliveModule](http://wiki.nginx.org/HttpUpstreamKeepaliveModule) with this Redis upstream. Only short Redis connections will work.
* There may be some race conditions that produce the harmless `Redis server returned extra bytes` warnings in your nginx's error.log. Such warnings might be rare but just be prepared for it.
* You should tune the various timeout settings provided by this module like [redis2_connect_timeout](#redis2_connect_timeout) and [redis2_read_timeout](#redis2_read_timeout).

If you cannot stand these limitations, then you are highly recommended to switch to the [lua-resty-redis](https://github.com/openresty/lua-resty-redis) library for [lua-nginx-module](http://github.com/openresty/lua-nginx-module).


## Performance Tuning

* When you're using this module, please ensure you're using a TCP connection pool (provided by [HttpUpstreamKeepaliveModule](http://wiki.nginx.org/HttpUpstreamKeepaliveModule)) and Redis pipelining wherever possible. These features will significantly improve performance.
* Using multiple instance of Redis servers on your multi-core machines also help a lot due to the sequential processing nature of a single Redis server instance.
* When you're benchmarking performance using something like `ab` or `http_load`, please ensure that your error log level is high enough (like `warn`) to prevent Nginx workers spend too much cycles on flushing the `error.log` file, which is always non-buffered and blocking and thus very expensive.


## SEE ALSO
* The [Redis](http://redis.io/) server homepage.
* The Redis wire protocol: <http://redis.io/topics/protocol>
* a redis response parser and a request constructor for Lua: [lua-redis-parser](http://github.com/openresty/lua-redis-parser).
* [lua-nginx-module](http://github.com/openresty/lua-nginx-module)
* The [ngx_openresty bundle](http://openresty.org).
* The [lua-resty-redis](https://github.com/openresty/lua-resty-redis) library based on the [lua-nginx-module](http://github.com/openresty/lua-nginx-module) cosocket API.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-redis2](https://github.com/openresty/redis2-nginx-module){target=_blank}.

# *rtmp*: NGINX RTMP module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-rtmp
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_rtmp_module.so;
```


This document describes nginx-module-rtmp [v1.2.1](https://github.com/dvershinin/nginx-rtmp-module/releases/tag/v1.2.1){target=_blank} 
released on Mar 10 2021.

<hr />
## nginx-rtmp-module


### Project blog

  http://nginx-rtmp.blogspot.com

### Wiki manual

  https://github.com/arut/nginx-rtmp-module/wiki/Directives

### Google group

  https://groups.google.com/group/nginx-rtmp

  https://groups.google.com/group/nginx-rtmp-ru (Russian)

### Donation page (Paypal etc)

  http://arut.github.com/nginx-rtmp-module/

### Features

* RTMP/HLS/MPEG-DASH live streaming

* RTMP Video on demand FLV/MP4,
  playing from local filesystem or HTTP

* Stream relay support for distributed
  streaming: push & pull models

* Recording streams in multiple FLVs

* H264/AAC support

* Online transcoding with FFmpeg

* HTTP callbacks (publish/play/record/update etc)

* Running external programs on certain events (exec)

* HTTP control module for recording audio/video and dropping clients

* Advanced buffering techniques
  to keep memory allocations at a minimum
  level for faster streaming and low
  memory footprint

* Proved to work with Wirecast, FMS, Wowza,
  JWPlayer, FlowPlayer, StrobeMediaPlayback,
  ffmpeg, avconv, rtmpdump, flvstreamer
  and many more

* Statistics in XML/XSL in machine- & human-
  readable form

* Linux/FreeBSD/MacOS/Windows

### Windows limitations

Windows support is limited. These features are not supported

* execs
* static pulls
* auto_push

### RTMP URL format

    rtmp://rtmp.example.com/app[/name]

app -  should match one of application {}
         blocks in config

name - interpreted by each application
         can be empty


### Multi-worker live streaming

Module supports multi-worker live
streaming through automatic stream pushing
to nginx workers. This option is toggled with
rtmp_auto_push directive.


### Example nginx.conf

    rtmp {

        server {

            listen 1935;

            chunk_size 4000;

            # TV mode: one publisher, many subscribers
            application mytv {

                # enable live streaming
                live on;

                # record first 1K of stream
                record all;
                record_path /tmp/av;
                record_max_size 1K;

                # append current timestamp to each flv
                record_unique on;

                # publish only from localhost
                allow publish 127.0.0.1;
                deny publish all;

                #allow play all;
            }

            # Transcoding (ffmpeg needed)
            application big {
                live on;

                # On every pusblished stream run this command (ffmpeg)
                # with substitutions: $app/${app}, $name/${name} for application & stream name.
                #
                # This ffmpeg call receives stream from this application &
                # reduces the resolution down to 32x32. The stream is the published to
                # 'small' application (see below) under the same name.
                #
                # ffmpeg can do anything with the stream like video/audio
                # transcoding, resizing, altering container/codec params etc
                #
                # Multiple exec lines can be specified.

                exec ffmpeg -re -i rtmp://localhost:1935/$app/$name -vcodec flv -acodec copy -s 32x32
                            -f flv rtmp://localhost:1935/small/${name};
            }

            application small {
                live on;
                # Video with reduced resolution comes here from ffmpeg
            }

            application webcam {
                live on;

                # Stream from local webcam
                exec_static ffmpeg -f video4linux2 -i /dev/video0 -c:v libx264 -an
                                   -f flv rtmp://localhost:1935/webcam/mystream;
            }

            application mypush {
                live on;

                # Every stream published here
                # is automatically pushed to
                # these two machines
                push rtmp1.example.com;
                push rtmp2.example.com:1934;
            }

            application mypull {
                live on;

                # Pull all streams from remote machine
                # and play locally
                pull rtmp://rtmp3.example.com pageUrl=www.example.com/index.html;
            }

            application mystaticpull {
                live on;

                # Static pull is started at nginx start
                pull rtmp://rtmp4.example.com pageUrl=www.example.com/index.html name=mystream static;
            }

            # video on demand
            application vod {
                play /var/flvs;
            }

            application vod2 {
                play /var/mp4s;
            }

            # Many publishers, many subscribers
            # no checks, no recording
            application videochat {

                live on;

                # The following notifications receive all
                # the session variables as well as
                # particular call arguments in HTTP POST
                # request

                # Make HTTP request & use HTTP retcode
                # to decide whether to allow publishing
                # from this connection or not
                on_publish http://localhost:8080/publish;

                # Same with playing
                on_play http://localhost:8080/play;

                # Publish/play end (repeats on disconnect)
                on_done http://localhost:8080/done;

                # All above mentioned notifications receive
                # standard connect() arguments as well as
                # play/publish ones. If any arguments are sent
                # with GET-style syntax to play & publish
                # these are also included.
                # Example URL:
                #   rtmp://localhost/myapp/mystream?a=b&c=d

                # record 10 video keyframes (no audio) every 2 minutes
                record keyframes;
                record_path /tmp/vc;
                record_max_frames 10;
                record_interval 2m;

                # Async notify about an flv recorded
                on_record_done http://localhost:8080/record_done;

            }


            # HLS

            # For HLS to work please create a directory in tmpfs (/tmp/hls here)
            # for the fragments. The directory contents is served via HTTP (see
            # http{} section in config)
            #
            # Incoming stream must be in H264/AAC. For iPhones use baseline H264
            # profile (see ffmpeg example).
            # This example creates RTMP stream from movie ready for HLS:
            #
            # ffmpeg -loglevel verbose -re -i movie.avi  -vcodec libx264
            #    -vprofile baseline -acodec libmp3lame -ar 44100 -ac 1
            #    -f flv rtmp://localhost:1935/hls/movie
            #
            # If you need to transcode live stream use 'exec' feature.
            #
            application hls {
                live on;
                hls on;
                hls_path /tmp/hls;
            }

            # MPEG-DASH is similar to HLS

            application dash {
                live on;
                dash on;
                dash_path /tmp/dash;
            }
        }
    }

    # HTTP can be used for accessing RTMP stats
    http {

        server {

            listen      8080;

            # This URL provides RTMP statistics in XML
            location /stat {
                rtmp_stat all;

                # Use this stylesheet to view XML as web page
                # in browser
                rtmp_stat_stylesheet stat.xsl;
            }

            location /stat.xsl {
                # XML stylesheet to view RTMP stats.
                # Copy stat.xsl wherever you want
                # and put the full directory path here
                root /path/to/stat.xsl/;
            }

            location /hls {
                # Serve HLS fragments
                types {
                    application/vnd.apple.mpegurl m3u8;
                    video/mp2t ts;
                }
                root /tmp;
                add_header Cache-Control no-cache;
            }

            location /dash {
                # Serve DASH fragments
                root /tmp;
                add_header Cache-Control no-cache;
            }
        }
    }


### Multi-worker streaming example

    rtmp_auto_push on;

    rtmp {
        server {
            listen 1935;

            application mytv {
                live on;
            }
        }
    }

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-rtmp](https://github.com/dvershinin/nginx-rtmp-module){target=_blank}.

# *secure-token*: Secure token module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-secure-token
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_secure_token_filter_module.so;
```


This document describes nginx-module-secure-token [v1.5](https://github.com/kaltura/nginx-secure-token-module/releases/tag/1.5){target=_blank} 
released on Jun 27 2022.

<hr />

Generates CDN tokens, either as a cookie or as a query string parameter (m3u8,mpd,f4m only).
Currently supports Akamai v2 tokens, and Amazon CloudFront tokens.
In addition, the module supports the encryption of URIs with a configured key.

## Configuration

### Generic token parameters

#### secure_token
* **syntax**: `secure_token value`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the token that should be embedded in the manifest/returned as a cookie.
The parameter value can contain variables, and often points to variables set by this module
(using `secure_token_akamai` / `secure_token_cloudfront` blocks)

#### secure_token_avoid_cookies
* **syntax**: `secure_token_avoid_cookies on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the module prefers to use a query string token instead of a cookie token.
A query string token is currently supported only for the following mime types (other mime types return a cookie token):
* application/vnd.apple.mpegurl
* application/dash+xml
* video/f4m

#### secure_token_types
* **syntax**: `secure_token_types mime_type ...`
* **default**: `none`
* **context**: `http`, `server`, `location`

Defines a set of mime types that should return a token

#### secure_token_uri_filename_prefix
* **syntax**: `secure_token_uri_filename_prefix prefix`
* **default**: `none`
* **context**: `http`, `server`, `location`

Defines a set of prefixes that will be matched against the URI file name, only URIs whose file name
starts with one of the defined prefixes will return a token

#### secure_token_expires_time
* **syntax**: `secure_token_expires_time time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the expiration time of responses that are not tokenized 
(determines the values of the Cache-Control and Expires HTTP headers)

#### secure_token_cookie_token_expires_time
* **syntax**: `secure_token_cookie_token_expires_time time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the expiration time of responses that are tokenized with a cookie token 
(determines the values of the Cache-Control and Expires HTTP headers)

#### secure_token_query_token_expires_time
* **syntax**: `secure_token_query_token_expires_time time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the expiration time of responses that are tokenized with a query string token 
(determines the values of the Cache-Control and Expires HTTP headers)

#### secure_token_cache_scope
* **syntax**: `secure_token_cache_scope scope`
* **default**: `public`
* **context**: `http`, `server`, `location`

Sets the cache scope (public/private) of responses that are not tokenized

#### secure_token_token_cache_scope
* **syntax**: `secure_token_token_cache_scope scope`
* **default**: `private`
* **context**: `http`, `server`, `location`

Sets the cache scope (public/private) of responses that are tokenized (query / cookie)

#### secure_token_last_modified
* **syntax**: `secure_token_last_modified time`
* **default**: `Sun, 19 Nov 2000 08:52:00 GMT`
* **context**: `http`, `server`, `location`

Sets the value of the last-modified header of responses that are not tokenized.
An empty string leaves the value of last-modified unaltered, while the string "now" sets the header to the server current time.

#### secure_token_token_last_modified
* **syntax**: `secure_token_token_last_modified time`
* **default**: `now`
* **context**: `http`, `server`, `location`

Sets the value of the last-modified header of responses that are tokenized (query / cookie)
An empty string leaves the value of last-modified unaltered, while the string "now" sets the header to the server current time.

#### secure_token_content_type_m3u8
* **syntax**: `secure_token_content_type_m3u8 type`
* **default**: `application/vnd.apple.mpegurl`
* **context**: `http`, `server`, `location`

Sets the content type that should be parsed as m3u8 for token insertion

#### secure_token_content_type_mpd
* **syntax**: `secure_token_content_type_mpd type`
* **default**: `application/dash+xml`
* **context**: `http`, `server`, `location`

Sets the content type that should be parsed as mpd for token insertion

#### secure_token_content_type_f4m
* **syntax**: `secure_token_content_type_f4m type`
* **default**: `video/f4m`
* **context**: `http`, `server`, `location`

Sets the content type that should be parsed as f4m for token insertion

### Akamai token parameters

#### secure_token_akamai
* **syntax**: `secure_token_akamai $variable { ... }`
* **context**: `http`

Creates a new variable whose value is an Akamai token, created according to the 
parameters specified within the block.

The block supports the following parameters:

#### key
* **syntax**: `key key_hex`
* **default**: `N/A (mandatory)`

Sets the secret key.

#### param_name
* **syntax**: `param_name name`
* **default**: `__hdnea__`

Sets the token parameter name (either the name of the cookie or the query string parameter)

#### acl
* **syntax**: `acl acl`
* **default**: `$secure_token_baseuri_comma`

Sets the signed part of the URL (ACL). The parameter value can contain variables.

#### start
* **syntax**: `start time`
* **default**: `0`

Sets the start time of the token (see `Time format` below)

#### end
* **syntax**: `end time`
* **default**: `86400`

Sets the end time of the token (see `Time format` below)

#### ip_address
* **syntax**: `ip_address address`
* **default**: `none`

Sets the IP address that should be embedded in the token.
The parameter value can contain variables, e.g. $remote_addr.

### CloudFront token parameters

#### secure_token_cloudfront
* **syntax**: `secure_token_cloudfront $variable { ... }`
* **context**: `http`

Creates a new variable whose value is a CloudFront token, created according to the 
parameters specified within the block.

The block supports the following parameters:

#### private_key_file
* **syntax**: `private_key_file filename`
* **default**: `N/A (mandatory)`

Sets the file name of the private key (PEM file)

#### key_pair_id
* **syntax**: `key_pair_id id`
* **default**: `N/A (mandatory)`

Sets the key pair id

#### acl
* **syntax**: `acl acl`
* **default**: `$secure_token_baseuri_comma`

Sets the signed part of the URL (ACL). The parameter value can contain variables.

#### end
* **syntax**: `end time`
* **default**: `86400`

Sets the end time of the token (see `Time format` below)

#### ip_address
* **syntax**: `ip_address address`
* **default**: `none`

Sets the IP address that should be embedded in the token.
The parameter value can contain variables, e.g. $remote_addr/32 can be used to limit the token to the specific IP of the client.

### Broadpeak token parameters

#### secure_token_broadpeak
* **syntax**: `secure_token_broadpeak $variable { ... }`
* **context**: `http`

Creates a new variable whose value is a Broadpeak token, created according to the
parameters specified within the block.

The block supports the following parameters:

#### key
* **syntax**: `key key`
* **default**: `N/A (mandatory)`

Sets the secret key. The parameter value can contain variables.

#### param_name
* **syntax**: `param_name name`
* **default**: `token`

Sets the token parameter name (either the name of the cookie or the query string parameter)

#### acl
* **syntax**: `acl acl`
* **default**: `$secure_token_baseuri_comma`

Sets the signed part of the URL (ACL). The parameter value can contain variables.

#### start
* **syntax**: `start time`
* **default**: `0`

Sets the start time of the token (see `Time format` below)

#### end
* **syntax**: `end time`
* **default**: `86400`

Sets the end time of the token (see `Time format` below)

#### session_start
* **syntax**: `session_start time`
* **default**: `N/A`

Sets the start time of the session, required for catchup. The parameter value can contain variables.

#### session_end
* **syntax**: `session_end time`
* **default**: `N/A`

Sets the end time of the session, required for catchup. The parameter value can contain variables.

#### additional_querylist
* **syntax**: `additional_querylist expr`
* **default**: `N/A`

Sets the primary token value, the value needs to be a list of name=value pairs without any separator.
For example, "ip=${arg_ip}account=${arg_account}device=${arg_device}".
The parameter value can contain variables.

### URI encryption parameters

#### secure_token_encrypt_uri
* **syntax**: `secure_token_encrypt_uri on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

Enables/disables uri encryption

#### secure_token_encrypt_uri_key
* **syntax**: `secure_token_encrypt_uri_key key_hex`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the encryption key, the key has to be 256 bits (64 hex characters)

#### secure_token_encrypt_uri_iv
* **syntax**: `secure_token_encrypt_uri_iv iv_hex`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the encryption iv, the iv has to be 128 bits (32 hex characters)

#### secure_token_encrypt_uri_part
* **syntax**: `secure_token_encrypt_uri_part expression`
* **default**: `none`
* **context**: `http`, `server`, `location`

An expression that calculates the part of the URL that should be encrypted in regular expression locations.
For non-regular expression locations, the encrypted part is everything following the path defined on the location block.

Example 1:
```nginx
  location /secret_param/([^/]+)/some_other_param/.* {
  	secure_token_encrypt_uri_part $1;
	...
  }
```
  In this configuration, only the value of secret_param will be encrypted/decrypted.

Example 2:  
```nginx
  location /base/ {
    ...
  }
```
  In this configuration, everything following /base/ will be encrypted/decrypted.
  
#### secure_token_encrypt_uri_hash_size
* **syntax**: `secure_token_encrypt_uri_hash_size size`
* **default**: `8`
* **context**: `http`, `server`, `location`

The size in bytes of hash used to validate the uri after decryption, the value has to be between 0 and 16.

### Time format

Some of the configuration parameters mentioned above, support both absolute timestamps,
and timestamps relative to `now`.
These parameters can be set in the configuration using one of the following formats:
* `epoch` - unix timestamp 0 (01/01/1970)
* `max` - unix timestamp 2147483647 (18/01/2038)
* `@1481230000` - unix timestamp 1481230000 (8/12/2016)
* `10d` / `+10d` - `now` + 10 days
* `-5m` - `now` - 5 minutes

## Sample configurations

### HLS packaging with Akamai tokens
```
	secure_token_akamai $token {
		key 1234;
		acl "$secure_token_baseuri_comma*";
	}

	server {
	
		location ~ ^/hls/p/\d+/(sp/\d+/)?serveFlavor/ {
			vod hls;

			g2o        on;

			secure_token $token;
			secure_token_types application/vnd.apple.mpegurl;
			
			secure_token_expires_time 100d;
			secure_token_query_token_expires_time 1h;

			more_set_headers 'Access-Control-Allow-Headers: *';
			more_set_headers 'Access-Control-Expose-Headers: Server,range,Content-Length,Content-Range';
			more_set_headers 'Access-Control-Allow-Methods: GET, HEAD, OPTIONS';
			more_set_headers 'Access-Control-Allow-Origin: *';
		}
		
	}
```

### HDS packaging with CloudFront tokens
```
	secure_token_cloudfront $token {
		private_key_file /path/to/pem;
		key_pair_id ABCDEF;
		acl "$scheme://$http_host$secure_token_baseuri_comma*";
	}

	server {
	
		location ~ ^/hds/p/\d+/(sp/\d+/)?serveFlavor/ {
			vod hds;
			vod_segment_duration 6000;
			vod_align_segments_to_key_frames on;
			vod_segment_count_policy last_rounded;

			secure_token $token;
			secure_token_types video/f4m;
			
			secure_token_expires_time 100d;
			secure_token_query_token_expires_time 1h;

			more_set_headers 'Access-Control-Allow-Headers: *';
			more_set_headers 'Access-Control-Expose-Headers: Server,range,Content-Length,Content-Range';
			more_set_headers 'Access-Control-Allow-Methods: GET, HEAD, OPTIONS';
			more_set_headers 'Access-Control-Allow-Origin: *';
		}
		
	}
```

### Encrypted HLS with token security on the encryption key

This configuration enables token security while having static URLs for the video segments,
this enables the caching of the segments transparently by proxies.
```
	secure_token_akamai $token {
		key 1234;
		acl "$secure_token_baseuri_comma*";
	}

	server {
	
		location ~ ^/s/hls/enc/p/\d+/(sp/\d+/)?serveFlavor/ {
			vod hls;
			vod_secret_key "password$vod_filepath";

			secure_token $token;
			secure_token_types application/vnd.apple.mpegurl;
			
			secure_token_expires_time 100d;
			secure_token_query_token_expires_time 1h;
			
			secure_token_uri_filename_prefix index;
			secure_token_tokenize_segments off;

			akamai_token_validate $arg___hdnea__;
			akamai_token_validate_key 1234;
			akamai_token_validate_uri_filename_prefix encryption;
			akamai_token_validate_uri_filename_prefix index;
		}
		
	}
```
Note: this configuration requires the module https://github.com/kaltura/nginx-akamai-token-validate-module
in addition to nginx-secure-token-module

### Adding token security on top of an existing HDS/HLS live stream
```
	secure_token_akamai $token {
		key 1234;
		acl "$secure_token_baseuri_comma*";
	}

	server {
	
		location /secure-live/ {
			proxy_pass http://original.live.domain;

			secure_token $token;
			secure_token_types text/xml application/vnd.apple.mpegurl;		
			secure_token_content_type_f4m text/xml;
			
			secure_token_expires_time 100d;
			secure_token_query_token_expires_time 1h;

			akamai_token_validate $arg___hdnea__;
			akamai_token_validate_key 1234;
			akamai_token_validate_strip_token __hdnea__;
		}
	
	}
```
Note: this configuration requires the module https://github.com/kaltura/nginx-akamai-token-validate-module
in addition to nginx-secure-token-module

### URI encryption
```nginx
	location ~ ^/hls/p/\d+/(sp/\d+/)?serveFlavor/entryId/([^/]+)/(.*) {
		vod hls;
		vod_secret_key "password$2";

		secure_token_encrypt_uri on;
		secure_token_encrypt_uri_key 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f;
		secure_token_encrypt_uri_iv 00000000000000000000000000000000;
		secure_token_encrypt_uri_part $3;
		secure_token_types application/vnd.apple.mpegurl;

		add_header Last-Modified "Sun, 19 Nov 2000 08:52:00 GMT";
		expires 100d;
	}
```

## Nginx variables

The module adds the following nginx variables:
* `$secure_token_baseuri` - contains the value of the `$uri` built in variable truncated up to the last slash (/). 
	For exmaple, if `$uri` is /a/b/c.htm then `$secure_token_baseuri` will be /a/b/.
* `$secure_token_baseuri_comma` - same as `$secure_token_baseuri`, except that if this value contains a comma (,) 
	the value is truncated up to the comma position.
	For exmaple, if `$uri` is /a/b/c.htm then `$secure_token_baseuri_comma` will be /a/b/; 
	if `$uri` is /a/b,c/d.htm then `$secure_token_baseuri_comma` will be /a/b.
* `$secure_token_original_uri` - contains the original (encrypted) uri when using uri encryption.
	Note that the built in `$uri` variable contains the modified (decrypted) uri in this case.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-secure-token](https://github.com/kaltura/nginx-secure-token-module){target=_blank}.

# *security-headers*: NGINX module for sending security headers


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-security-headers
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_security_headers_module.so;
```


This document describes nginx-module-security-headers [v0.1.0](https://github.com/GetPageSpeed/ngx_security_headers/releases/tag/0.1.0){target=_blank} 
released on Sep 05 2023.

<hr />

This NGINX module adds security headers and removes insecure headers, *the right way* (c). 

[![Test Build](https://github.com/GetPageSpeed/ngx_security_headers/actions/workflows/build.yml/badge.svg?event=push)](https://github.com/GetPageSpeed/ngx_security_headers/actions/workflows/build.yml)

## Synopsis

```nginx
http {
    security_headers on;
    ...
}
```

Running `curl -IL https://example.com/` will yield the added security headers:

<pre>
HTTP/1.1 200 OK
Server: nginx
Date: Tue, 21 May 2019 16:15:46 GMT
Content-Type: text/html; charset=UTF-8
Vary: Accept-Encoding
Accept-Ranges: bytes
Connection: keep-alive
<b>X-Frame-Options: SAMEORIGIN
X-Content-Type-Options: nosniff
X-XSS-Protection: 0
Referrer-Policy: strict-origin-when-cross-origin
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload</b>
</pre>

In general, the module features sending security HTTP headers in a way that better conforms to the standards.
For instance, `Strict-Transport-Security` header should *not* be sent for plain HTTP requests.
The module follows this recommendation.

## Important note on `Strict-Transport-Security`

The module adds several security headers, including `Strinct-Transport-Security`.
Note that `preload` is sent in the value of this header, by default.
This means Chrome may and will include your websites to its preload list of domains which are HTTPS only.

It is *usually* what you want anyway, but bear in mind that in some edge cases you want to access
a subdomain via plan unencrypted connection.

If you absolutely sure that all your domains and subdomains used with the module will ever primarily operate
on HTTPs, proceed without any extra step.

If you are *not sure* if you have or will have a need to access your websites or any of its subdomains over
plain insecure HTTP protocol, ensure `security_headers_hsts_preload off;` in your config before you ever
start NGINX with the module to avoid having your domain preloaded by Chrome.

## Key Features

*   Plug-n-Play: the default set of security headers can be enabled with `security_headers on;` in your NGINX configuration
*   Sends HTML-only security headers for relevant types only, not sending for others, e.g. `X-Frame-Options` is useless for CSS
*   Plays well with conditional `GET` requests: the security headers are not included there unnecessarily
*   Does not suffer the `add_header` directive's pitfalls
*   Hides `X-Powered-By` and other headers which often leak software version information
*   Hides `Server` header altogether, not just the version information

## Configuration directives

### `security_headers`

- **syntax**: `security_headers on | off`
- **default**: `off`
- **context**: `http`, `server`, `location`

Enables or disables applying security headers. The default set includes:

* `X-Frame-Options: SAMEORIGIN`
* `X-XSS-Protection: 0`
* `Referrer-Policy: strict-origin-when-cross-origin`
* `X-Content-Type-Options: nosniff`

The values of these headers (or their inclusion) can be controlled with other `security_headers_*` directives below.

### `hide_server_tokens`

- **syntax**: `hide_server_tokens on | off`
- **default**: `off`
- **context**: `http`, `server`, `location`

Enables hiding headers which leak software information:

*   `Server`
*   `X-Powered-By`
*   `X-Page-Speed`
*   `X-Varnish`

It's worth noting that some of those headers bear functional use, e.g. [`X-Page-Speed` docs](https://www.modpagespeed.com/doc/configuration#XHeaderValue) mention:

> ... it is used to prevent infinite loops and unnecessary rewrites when PageSpeed 
> fetches resources from an origin that also uses PageSpeed

So it's best to specify `hide_server_tokens on;` in a front-facing NGINX instances, e.g.
the one being accessed by actual browsers, and not the ones consumed by Varnish or other software.

In most cases you will be just fine with `security_headers on;` and `hide_server_tokens on;`, without any adjustments.

For fine-tuning, use the header-specific directives below. 
A special value `omit` disables sending a particular header by the module (useful if you want to let your backend app to send it). 

### `security_headers_xss`

- **syntax**: `security_headers_xss off | on | block | omit`
- **default**: `off`
- **context**: `http`, `server`, `location`

Controls `X-XSS-Protection` header. 
Special `omit` value will disable sending the header by the module. 
The `off` value is for disabling XSS protection: `X-XSS-Protection: 0`.
This is the default because 
[modern browsers do not support it](https://github.com/GetPageSpeed/ngx_security_headers/issues/19) and where it is 
supported, it introduces vulnerabilities.

### `security_headers_frame`

- **syntax**: `security_headers_frame sameorigin | deny | omit`
- **default**: `sameorigin`
- **context**: `http`, `server`, `location`

Controls inclusion and value of `X-Frame-Options` header. 
Special `omit` value will disable sending the header by the module. 


### `security_headers_referrer_policy`

- **syntax**: `security_headers_referrer_policy no-referrer | no-referrer-when-downgrade | same-origin | origin 
| strict-origin | origin-when-cross-origin | strict-origin-when-cross-origin | unsafe-url | omit`
- **default**: `strict-origin-when-cross-origin`
- **context**: `http`, `server`, `location`

Controls inclusion and value of [`Referrer-Policy`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Referrer-Policy) header. 
Special `omit` value will disable sending the header by the module. 

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-security-headers](https://github.com/GetPageSpeed/ngx_security_headers){target=_blank}.

# *security*: ModSecurity v3 Nginx Connector

## Installation

CentOS/RHEL/RockyLinux/etc. and Amazon Linux are supported and require a [subscription](https://www.getpagespeed.com/repo-subscribe).

Fedora Linux is supported free of charge and doesn't require a subscription.

### OS-specific complete installation and configuration guides available:

*   [CentOS/RHEL 7](https://bit.ly/ngx-security-el7)
*   [CentOS/RHEL 8](https://bit.ly/ngx-security-el8)

### Other supported operating systems
        
```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-security
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_modsecurity_module.so;
```


This document describes nginx-module-security [v1.0.3](https://github.com/SpiderLabs/ModSecurity-nginx/releases/tag/v1.0.3){target=_blank} 
released on May 20 2022.

<hr />

<img src="https://github.com/SpiderLabs/ModSecurity/raw/v3/master/others/modsec.png" width="50%">

[![](https://raw.githubusercontent.com/ZenHubIO/support/master/zenhub-badge.png)](https://zenhub.com)


The ModSecurity-nginx connector is the connection point between nginx and libmodsecurity (ModSecurity v3). Said another way, this project provides a communication channel between nginx and libmodsecurity. This connector is required to use LibModSecurity with nginx. 

The ModSecurity-nginx connector takes the form of an nginx module. The module simply serves as a layer of communication between nginx and ModSecurity.

Notice that this project depends on libmodsecurity rather than ModSecurity (version 2.9 or less).

### What is the difference between this project and the old ModSecurity add-on for nginx?

The old version uses ModSecurity standalone, which is a wrapper for
Apache internals to link ModSecurity to nginx. This current version is closer
to nginx, consuming the new libmodsecurity which is no longer dependent on
Apache. As a result, this current version has less dependencies, fewer bugs, and is faster. In addition, some new functionality is also provided - such as the possibility of use of global rules configuration with per directory/location customizations (e.g. SecRuleRemoveById).


## Usage

ModSecurity for nginx extends your nginx configuration directives.
It adds four new directives and they are:

## modsecurity
**syntax:** *modsecurity on | off*

**context:** *http, server, location*

**default:** *off*

Turns on or off ModSecurity functionality.
Note that this configuration directive is no longer related to the SecRule state.
Instead, it now serves solely as an nginx flag to enable or disable the module.

## modsecurity_rules_file
**syntax:** *modsecurity_rules_file &lt;path to rules file&gt;*

**context:** *http, server, location*

**default:** *no*

Specifies the location of the modsecurity configuration file, e.g.:

```nginx
server {
    modsecurity on;
    location / {
        root /var/www/html;
        modsecurity_rules_file /etc/my_modsecurity_rules.conf;
    }
}
```

## modsecurity_rules_remote
**syntax:** *modsecurity_rules_remote &lt;key&gt; &lt;URL to rules&gt;*

**context:** *http, server, location*

**default:** *no*

Specifies from where (on the internet) a modsecurity configuration file will be downloaded.
It also specifies the key that will be used to authenticate to that server:

```nginx
server {
    modsecurity on;
    location / {
        root /var/www/html;
        modsecurity_rules_remote my-server-key https://my-own-server/rules/download;
    }
}
```

## modsecurity_rules
**syntax:** *modsecurity_rules &lt;modsecurity rule&gt;*

**context:** *http, server, location*

**default:** *no*

Allows for the direct inclusion of a ModSecurity rule into the nginx configuration.
The following example is loading rules from a file and injecting specific configurations per directory/alias:

```nginx
server {
    modsecurity on;
    location / {
        root /var/www/html;
        modsecurity_rules_file /etc/my_modsecurity_rules.conf;
    }
    location /ops {
        root /var/www/html/opts;
        modsecurity_rules '
          SecRuleEngine On
          SecDebugLog /tmp/modsec_debug.log
          SecDebugLogLevel 9
          SecRuleRemoveById 10
        ';
    }
}
```

## modsecurity_transaction_id
**syntax:** *modsecurity_transaction_id string*

**context:** *http, server, location*

**default:** *no*

Allows to pass transaction ID from nginx instead of generating it in the library.
This can be useful for tracing purposes, e.g. consider this configuration:

```nginx
log_format extended '$remote_addr - $remote_user [$time_local] '
                    '"$request" $status $body_bytes_sent '
                    '"$http_referer" "$http_user_agent" $request_id';

server {
    server_name host1;
    modsecurity on;
    modsecurity_transaction_id "host1-$request_id";
    access_log logs/host1-access.log extended;
    error_log logs/host1-error.log;
    location / {
        ...
    }
}

server {
    server_name host2;
    modsecurity on;
    modsecurity_transaction_id "host2-$request_id";
    access_log logs/host2-access.log extended;
    error_log logs/host2-error.log;
    location / {
        ...
    }
}
```

Using a combination of log_format and modsecurity_transaction_id you will
be able to find correlations between access log and error log entries
using the same unique identificator.

String can contain variables.


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-security](https://github.com/SpiderLabs/ModSecurity-nginx){target=_blank}.

# *set-misc*: NGINX Set-Misc module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-set-misc
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_set_misc_module.so;
```


This document describes nginx-module-set-misc [v0.33](https://github.com/openresty/set-misc-nginx-module/releases/tag/v0.33){target=_blank} 
released on Sep 06 2021.

<hr />
<!---
Don't edit this file manually! Instead you should generate it by using:
    wiki2markdown.pl doc/HttpSetMiscModule.wiki
-->

## Name

**ngx_set_misc** - Various set_xxx directives added to nginx's rewrite module (md5/sha1, sql/json quoting, and many more)


## Synopsis

```nginx

 location /foo {
     set $a $arg_a;
     set_if_empty $a 56;

     # GET /foo?a=32 will yield $a == 32
     # while GET /foo and GET /foo?a= will
     # yeild $a == 56 here.
 }

 location /bar {
     set $foo "hello\n\n'\"\\";
     set_quote_sql_str $foo $foo; # for mysql

     # OR in-place editing:
     #   set_quote_sql_str $foo;

     # now $foo is: 'hello\n\n\'\"\\'
 }

 location /bar {
     set $foo "hello\n\n'\"\\";
     set_quote_pgsql_str $foo;  # for PostgreSQL

     # now $foo is: E'hello\n\n\'\"\\'
 }

 location /json {
     set $foo "hello\n\n'\"\\";
     set_quote_json_str $foo $foo;

     # OR in-place editing:
     #   set_quote_json_str $foo;

     # now $foo is: "hello\n\n'\"\\"
 }

 location /baz {
     set $foo "hello%20world";
     set_unescape_uri $foo $foo;

     # OR in-place editing:
     #   set_unescape_uri $foo;

     # now $foo is: hello world
 }

 upstream_list universe moon sun earth;
 upstream moon { ... }
 upstream sun { ... }
 upstream earth { ... }
 location /foo {
     set_hashed_upstream $backend universe $arg_id;
     drizzle_pass $backend; # used with ngx_drizzle
 }

 location /base32 {
     set $a 'abcde';
     set_encode_base32 $a;
     set_decode_base32 $b $a;

     # now $a == 'c5h66p35' and
     # $b == 'abcde'
 }

 location /base64 {
     set $a 'abcde';
     set_encode_base64 $a;
     set_decode_base64 $b $a;

     # now $a == 'YWJjZGU=' and
     # $b == 'abcde'
 }

 location /hex {
     set $a 'abcde';
     set_encode_hex $a;
     set_decode_hex $b $a;

     # now $a == '6162636465' and
     # $b == 'abcde'
 }

 # GET /sha1 yields the output
 #   aaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d
 location /sha1 {
     set_sha1 $a hello;
     echo $a;
 }

 # ditto
 location /sha1 {
     set $a hello;
     set_sha1 $a;
     echo $a;
 }

 # GET /today yields the date of today in local time using format 'yyyy-mm-dd'
 location /today {
     set_local_today $today;
     echo $today;
 }

 # GET /signature yields the hmac-sha-1 signature
 # given a secret and a string to sign
 # this example yields the base64 encoded singature which is
 # "HkADYytcoQQzqbjQX33k/ZBB/DQ="
 location /signature {
     set $secret_key 'secret-key';
     set $string_to_sign "some-string-to-sign";
     set_hmac_sha1 $signature $secret_key $string_to_sign;
     set_encode_base64 $signature $signature;
     echo $signature;
 }

 location = /rand {
     set $from 3;
     set $to 15;
     set_random $rand $from $to;

     # or write directly
     #   set_random $rand 3 15;

     echo $rand;  # will print a random integer in the range [3, 15]
 }
```

## Description

This module extends the standard HttpRewriteModule's directive set to provide more functionalities like URI escaping and unescaping, JSON quoting, Hexadecimal/MD5/SHA1/Base32/Base64 digest encoding and decoding, random number generator, and more!

Every directive provided by this module can be mixed freely with other [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html)'s directives, like [if](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#if) and [set](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#set). (Thanks to the [Nginx Devel Kit](https://github.com/simpl/ngx_devel_kit)!)


## Directives


## set_if_empty
**syntax:** *set_if_empty $dst &lt;src&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Assign the value of the argument `<src>` if and only if variable `$dst` is empty (i.e., not found or has an empty string value).

In the following example,

```nginx

 set $a 32;
 set_if_empty $a 56;
```

the variable `$dst` will take the value 32 at last. But in the sample

```nginx

 set $a '';
 set $value "hello, world"
 set_if_empty $a $value;
```

`$a` will take the value `"hello, world"` at last.


## set_quote_sql_str
**syntax:** *set_quote_sql_str $dst &lt;src&gt;*

**syntax:** *set_quote_sql_str $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will quote the value of the second argument `<src>` by MySQL's string value quoting rule and assign the result into the first argument, variable `$dst`. For example,

```nginx

 location /test {
     set $value "hello\n\r'\"\\";
     set_quote_sql_str $quoted $value;

     echo $quoted;
 }
```

Then request `GET /test` will yield the following output

```sql

 'hello\n\r\'\"\\'
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $value "hello\n\r'\"\\";
     set_quote_sql_str $value;

     echo $value;
 }
```

then request `GET /test` will give exactly the same output as the previous example.

This directive is usually used to prevent SQL injection.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_quote_pgsql_str
**syntax:** *set_quote_pgsql_str $dst &lt;src&gt;*

**syntax:** *set_quote_pgsql_str $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

Very much like [set_quote_sql_str](#set_quote_sql_str), but with PostgreSQL quoting rules for SQL string literals.


## set_quote_json_str
**syntax:** *set_quote_json_str $dst &lt;src&gt;*

**syntax:** *set_quote_json_str $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will quote the value of the second argument `<src>` by JSON string value quoting rule and assign the result into the first argument, variable `$dst`. For example,

```nginx

 location /test {
     set $value "hello\n\r'\"\\";
     set_quote_json_str $quoted $value;

     echo $quoted;
 }
```

Then request `GET /test` will yield the following output

```javascript

 "hello\n\r'\"\\"
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $value "hello\n\r'\"\\";
     set_quote_json_str $value;

     echo $value;
 }
```

then request `GET /test` will give exactly the same output as the previous example.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_unescape_uri
**syntax:** *set_unescape_uri $dst &lt;src&gt;*

**syntax:** *set_unescape_uri $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will unescape the value of the second argument `<src>` as a URI component and assign the result into the first argument, variable `$dst`. For example,

```nginx

 location /test {
     set_unescape_uri $key $arg_key;
     echo $key;
 }
```

Then request `GET /test?key=hello+world%21` will yield the following output

```
hello world!
```

The nginx standard [$arg_PARAMETER](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_arg_) variable holds the raw (escaped) value of the URI parameter. So we need the `set_unescape_uri` directive to unescape it first.

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $key $arg_key;
     set_unescape_uri $key;

     echo $key;
 }
```

then request `GET /test?key=hello+world%21` will give exactly the same output as the previous example.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_escape_uri
**syntax:** *set_escape_uri $dst &lt;src&gt;*

**syntax:** *set_escape_uri $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

Very much like the [set_unescape_uri](#set_unescape_uri) directive, but does the conversion the other way around, i.e., URL component escaping.


## set_hashed_upstream
**syntax:** *set_hashed_upstream $dst &lt;upstream_list_name&gt; &lt;src&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Hashes the string argument `<src>` into one of the upstream name included in the upstream list named `<upstream_list_name>`. The hash function being used is simple modulo.

Here's an example,

```nginx

 upstream moon { ... }
 upstream sun { ... }
 upstream earth { ... }

 upstream_list universe moon sun earth;

 location /test {
     set_unescape_uri $key $arg_key;
     set $list_name universe;
     set_hashed_upstream $backend $list_name $key;

     echo $backend;
 }
```

Then `GET /test?key=blah` will output either "moon", "sun", or "earth", depending on the actual value of the `key` query argument.

This directive is usually used to compute an nginx variable to be passed to [memc-nginx-module](http://github.com/openresty/memc-nginx-module)'s [memc_pass](http://github.com/openresty/memc-nginx-module#memc_pass) directive, [redis2-nginx-module](http://github.com/openresty/redis2-nginx-module)'s [[HttpRedis2Module#redis2_pass]] directive, and [ngx_http_proxy_module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html)'s [proxy_pass](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass) directive, among others.


## set_encode_base32
**syntax:** *set_encode_base32 $dst &lt;src&gt;*

**syntax:** *set_encode_base32 $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will encode the value of the second argument `<src>` to its base32(hex) digest and assign the result into the first argument, variable `$dst`. For example,

```nginx

 location /test {
     set $raw "abcde";
     set_encode_base32 $digest $raw;

     echo $digest;
 }
```

Then request `GET /test` will yield the following output

```
c5h66p35
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

RFC forces the `[A-Z2-7]` RFC-3548 compliant encoding, but we are using the "base32hex" encoding (`[0-9a-v]`) by default. The [set_base32_alphabet](#set_base32_alphabet) directive (first introduced in `v0.28`) allows you to change the alphabet used for encoding/decoding so RFC-3548 compliant encoding is still possible by custom configurations.

By default, the `=` character is used to pad the left-over bytes due to alignment. But the padding behavior can be completely disabled by setting [set_base32_padding](#set_base32_padding) `off`.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $value "abcde";
     set_encode_base32 $value;

     echo $value;
 }
```

then request `GET /test` will give exactly the same output as the previous example.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_base32_padding
**syntax:** *set_base32_padding on|off*

**default:** *on*

**context:** *http, server, server if, location, location if*

**phase:** *no*

This directive can control whether to pad left-over bytes with the "=" character when encoding a base32 digest by the
[set_encode_base32](#set_encode_base32) directive.

This directive was first introduced in `v0.28`. If you use earlier versions of this module, then you should use [set_misc_base32_padding](#set_misc_base32_padding) instead.


## set_misc_base32_padding
**syntax:** *set_misc_base32_padding on|off*

**default:** *on*

**context:** *http, server, server if, location, location if*

**phase:** *no*

This directive has been deprecated since `v0.28`. Use [set_base32_padding](#set_base32_padding) instead if you are using `v0.28+`.


## set_base32_alphabet
**syntax:** *set_base32_alphabet &lt;alphabet&gt;*

**default:** *"0123456789abcdefghijklmnopqrstuv"*

**context:** *http, server, server if, location, location if*

**phase:** *no*

This directive controls the alphabet used for encoding/decoding a base32 digest. It accepts a string containing the desired alphabet like "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567" for standard alphabet.

Extended (base32hex) alphabet is used by default.

This directive was first introduced in `v0.28`.


## set_decode_base32
**syntax:** *set_decode_base32 $dst &lt;src&gt;*

**syntax:** *set_decode_base32 $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

Similar to the [set_encode_base32](#set_encode_base32) directive, but does exactly the opposite operation, .i.e, decoding a base32(hex) digest into its original form.


## set_encode_base64
**syntax:** *set_encode_base64 $dst &lt;src&gt;*

**syntax:** *set_encode_base64 $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will encode the value of the second argument `<src>` to its base64 digest and assign the result into the first argument, variable `$dst`. For example,

```nginx

 location /test {
     set $raw "abcde";
     set_encode_base64 $digest $raw;

     echo $digest;
 }
```

Then request `GET /test` will yield the following output

```
YWJjZGU=
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $value "abcde";
     set_encode_base64 $value;

     echo $value;
 }
```

then request `GET /test` will give exactly the same output as the previous example.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_encode_base64url
**syntax:** *set_encode_base64url $dst &lt;src&gt;*

**syntax:** *set_encode_base64url $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will encode the value of the second argument `<src>` to its base64 url safe digest and assign the result into the first argument, variable `$dst`. For example,

```nginx

 location /test {
     set $raw "abcde";
     set_encode_base64url $digest $raw;

     echo $digest;
 }
```

Then request `GET /test` will yield the following output

```
YWJjZGU=
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $value "abcde";
     set_encode_base64url $value;

     echo $value;
 }
```

then request `GET /test` will give exactly the same output as the previous example.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_decode_base64
**syntax:** *set_decode_base64 $dst &lt;src&gt;*

**syntax:** *set_decode_base64 $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

Similar to the [set_encode_base64](#set_encode_base64) directive, but does exactly the opposite operation, .i.e, decoding a base64 digest into its original form.


## set_decode_base64url
**syntax:** *set_decode_base64url $dst &lt;src&gt;*

**syntax:** *set_decode_base64url $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

Similar to the [set_encode_base64url](#set_encode_base64url) directive, but does exactly the the opposite operation, .i.e, decoding a base64 url safe digest into its original form.


## set_encode_hex
**syntax:** *set_encode_hex $dst &lt;src&gt;*

**syntax:** *set_encode_hex $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will encode the value of the second argument `<src>` to its hexadecimal digest and assign the result into the first argument, variable `$dst`. For example,

```nginx

 location /test {
     set $raw "章亦春";
     set_encode_hex $digest $raw;

     echo $digest;
 }
```

Then request `GET /test` will yield the following output

```
e7aba0e4baa6e698a5
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $value "章亦春";
     set_encode_hex $value;

     echo $value;
 }
```

then request `GET /test` will give exactly the same output as the previous example.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_decode_hex
**syntax:** *set_decode_hex $dst &lt;src&gt;*

**syntax:** *set_decode_hex $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

Similar to the [set_encode_hex](#set_encode_hex) directive, but does exactly the opposite operation, .i.e, decoding a hexadecimal digest into its original form.


## set_sha1
**syntax:** *set_sha1 $dst &lt;src&gt;*

**syntax:** *set_sha1 $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will encode the value of the second argument `<src>` to its [SHA-1](http://en.wikipedia.org/wiki/SHA-1) digest and assign the result into the first argument, variable `$dst`. The hexadecimal form of the `SHA-1` digest will be generated automatically, use [set_decode_hex](#set_decode_hex) to decode the result if you want the binary form of the `SHA-1` digest.

For example,

```nginx

 location /test {
     set $raw "hello";
     set_sha1 $digest $raw;

     echo $digest;
 }
```

Then request `GET /test` will yield the following output

```
aaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $value "hello";
     set_sha1 $value;

     echo $value;
 }
```

then request `GET /test` will give exactly the same output as the previous example.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_md5
**syntax:** *set_md5 $dst &lt;src&gt;*

**syntax:** *set_md5 $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

**category:** *ndk_set_var_value*

When taking two arguments, this directive will encode the value of the second argument `<src>` to its [MD5](http://en.wikipedia.org/wiki/MD5) digest and assign the result into the first argument, variable `$dst`. The hexadecimal form of the `MD5` digest will be generated automatically, use [set_decode_hex](#set_decode_hex) to decode the result if you want the binary form of the `MD5` digest.

For example,

```nginx

 location /test {
     set $raw "hello";
     set_md5 $digest $raw;

     echo $digest;
 }
```

Then request `GET /test` will yield the following output


    5d41402abc4b2a76b9719d911017c592


Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

When taking a single argument, this directive will do in-place modification of the argument variable. For example,

```nginx

 location /test {
     set $value "hello";
     set_md5 $value;

     echo $value;
 }
```

then request `GET /test` will give exactly the same output as the previous example.

This directive can be invoked by [lua-nginx-module](http://github.com/openresty/lua-nginx-module)'s [ndk.set_var.DIRECTIVE](http://github.com/openresty/lua-nginx-module#ndkset_vardirective) interface and [array-var-nginx-module](http://github.com/openresty/array-var-nginx-module)'s [array_map_op](http://github.com/openresty/array-var-nginx-module#array_map_op) directive.


## set_hmac_sha1
**syntax:** *set_hmac_sha1 $dst &lt;secret_key&gt; &lt;src&gt;*

**syntax:** *set_hmac_sha1 $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Computes the [HMAC-SHA1](http://en.wikipedia.org/wiki/HMAC) digest of the argument `<src>` and assigns the result into the argument variable `$dst` with the secret key `<secret_key>`.

The raw binary form of the `HMAC-SHA1` digest will be generated, use [set_encode_base64](#set_encode_base64), for example, to encode the result to a textual representation if desired.

For example,

```nginx

 location /test {
     set $secret 'thisisverysecretstuff';
     set $string_to_sign 'some string we want to sign';
     set_hmac_sha1 $signature $secret $string_to_sign;
     set_encode_base64 $signature $signature;
     echo $signature;
 }
```

Then request `GET /test` will yield the following output

```
R/pvxzHC4NLtj7S+kXFg/NePTmk=
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

This directive requires the OpenSSL library enabled in your Nginx build (usually by passing the `--with-http_ssl_module` option to the `./configure` script).


## set_hmac_sha256
**syntax:** *set_hmac_sha256 $dst &lt;secret_key&gt; &lt;src&gt;*

**syntax:** *set_hmac_sha256 $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Computes the [HMAC-SHA256](http://en.wikipedia.org/wiki/HMAC) digest of the argument `<src>` and assigns the result into the argument variable `$dst` with the secret key `<secret_key>`.

The raw binary form of the `HMAC-SHA256` digest will be generated, use [set_encode_base64](#set_encode_base64), for example, to encode the result to a textual representation if desired.

For example,

```nginx

 location /test {
     set $secret 'thisisverysecretstuff';
     set $string_to_sign 'some string we want to sign';
     set_hmac_sha256 $signature $secret $string_to_sign;
     set_encode_base64 $signature $signature;
     echo $signature;
 }
```

Then request `GET /test` will yield the following output

```
4pU3GRQrKKIoeLb9CqYsavHE2l6Hx+KMmRmesU+Cfrs=
```

Please note that we're using [echo-nginx-module](http://github.com/openresty/echo-nginx-module)'s [echo directive](http://github.com/openresty/echo-nginx-module#echo) here to output values of nginx variables directly.

This directive requires the OpenSSL library enabled in your Nginx build (usually by passing the `--with-http_ssl_module` option to the `./configure` script).


## set_random
**syntax:** *set_random $res &lt;from&gt; &lt;to&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Generates a (pseudo) random number (in textual form) within the range `[<$from>, <$to>]` (inclusive).

Only non-negative numbers are allowed for the `<from>` and `<to>` arguments.

When `<from>` is greater than `<to>`, their values will be exchanged accordingly.

For instance,

```nginx

 location /test {
     set $from 5;
     set $to 7;
     set_random $res $from $to;

     echo $res;
 }
```

then request `GET /test` will output a number between 5 and 7 (i.e., among 5, 6, 7).

For now, there's no way to configure a custom random generator seed.

Behind the scene, it makes use of the standard C function `rand()`.

This directive was first introduced in the `v0.22rc1` release.

See also [set_secure_random_alphanum](#set_secure_random_alphanum) and [set_secure_random_lcalpha](#set_secure_random_lcalpha).


## set_secure_random_alphanum
**syntax:** *set_secure_random_alphanum $res &lt;length&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Generates a cryptographically-strong random string `<length>` characters long with the alphabet `[a-zA-Z0-9]`.

`<length>` may be between 1 and 64, inclusive.

For instance,

```nginx

 location /test {
     set_secure_random_alphanum $res 32;

     echo $res;
 }
```

then request `GET /test` will output a string like `ivVVRP2DGaAqDmdf3Rv4ZDJ7k0gOfASz`.

This functionality depends on the presence of the `/dev/urandom` device, available on most UNIX-like systems.

See also [set_secure_random_lcalpha](#set_secure_random_lcalpha) and [set_random](#set_random).

This directive was first introduced in the `v0.22rc8` release.


## set_secure_random_lcalpha
**syntax:** *set_secure_random_lcalpha $res &lt;length&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Generates a cryptographically-strong random string `<length>` characters long with the alphabet `[a-z]`.

`<length>` may be between 1 and 64, inclusive.

For instance,

```nginx

 location /test {
     set_secure_random_lcalpha $res 32;

     echo $res;
 }
```

then request `GET /test` will output a string like `kcuxcddktffsippuekhshdaclaquiusj`.

This functionality depends on the presence of the `/dev/urandom` device, available on most UNIX-like systems.

This directive was first introduced in the `v0.22rc8` release.

See also [set_secure_random_alphanum](#set_secure_random_alphanum) and [set_random](#set_random).


## set_rotate
**syntax:** *set_rotate $value &lt;from&gt; &lt;to&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Increments `$value` but keeps it in range from `<from>` to `<to>`. 
If `$value` is greater than `<to>` or less than `<from>` is will be 
set to `<from>` value.

The current value after running this directive will always be saved on a per-location basis. And the this saved value will be used for incrementation when the `$value` is not initialized or has a bad value.

Only non-negative numbers are allowed for the `<from>` and `<to>` arguments.

When `<from>` is greater than `<to>`, their values will be exchanged accordingly.

For instance,

```nginx

 location /rotate {
     default_type text/plain;
     set $counter $cookie_counter;
     set_rotate $counter 1 5;
     echo $counter;
     add_header Set-Cookie counter=$counter;
 }
```

then request `GET /rotate` will output next number between 1 and 5 (i.e., 1, 2, 3, 4, 5) on each
refresh of the page. This directive may be userful for banner rotation purposes.

Another example is to use server-side value persistence to do simple round-robin:

```nginx

 location /rotate {
     default_type text/plain;
     set_rotate $counter 0 3;
     echo $counter;
 }
```

And accessing `/rotate` will also output integer sequence 0, 1, 2, 3, 0, 1, 2, 3, and so on.

This directive was first introduced in the `v0.22rc7` release.


## set_local_today
**syntax:** *set_local_today $dst*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Set today's date ("yyyy-mm-dd") in localtime to the argument variable `$dst`.

Here's an example,

```nginx

 location /today {
     set_local_today $today;
     echo $today;
 }
```

then request `GET /today` will output something like

```
2011-08-16
```

and year, the actual date you get here will vary every day ;)

Behind the scene, this directive utilizes the `ngx_time` API in the Nginx core, so usually no syscall is involved due to the time caching mechanism in the Nginx core.


## set_formatted_gmt_time
**syntax:** *set_formatted_gmt_time $res &lt;time-format&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Set a formatted GMT time to variable `$res` (as the first argument) using the format string in the second argument.

All the conversion specification notations in the standard C function `strftime` are supported, like `%Y` (for 4-digit years) and `%M` (for minutes in decimal). See <http://linux.die.net/man/3/strftime> for a complete list of conversion specification symbols.

Below is an example:

```nginx

 location = /t {
     set_formatted_gmt_time $timestr "%a %b %e %H:%M:%S %Y GMT";
     echo $timestr;
 }
```

Accessing `/t` yields the output

```
Fri Dec 13 15:34:37 2013 GMT
```

This directive was first added in the `0.23` release.

See also [set_formatted_local_time](#set_formatted_local_time).


## set_formatted_local_time
**syntax:** *set_formatted_local_time $res &lt;time-format&gt;*

**default:** *no*

**context:** *location, location if*

**phase:** *rewrite*

Set a formatted local time to variable `$res` (as the first argument) using the format string in the second argument.

All the conversion specification notations in the standard C function `strftime` are supported, like `%Y` (for 4-digit years) and `%M` (for minutes in decimal). See <http://linux.die.net/man/3/strftime> for a complete list of conversion specification symbols.

Below is an example:

```nginx

 location = /t {
     set_formatted_local_time $timestr "%a %b %e %H:%M:%S %Y %Z";
     echo $timestr;
 }
```

Accessing `/t` yields the output

```
Fri Dec 13 15:42:15 2013 PST
```

This directive was first added in the `0.23` release.

See also [set_formatted_gmt_time](#set_formatted_gmt_time).


## Caveats

Do not use [$arg_PARAMETER](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_arg_), [$cookie_COOKIE](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_cookie_), [$http_HEADER](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_http_) or other special variables defined in the Nginx core module as the target variable in this module's directives. For instance,

```nginx

 set_if_empty $arg_user 'foo';  # DO NOT USE THIS!
```

may lead to segmentation faults.


## Changes

The change logs for every release of this module can be obtained from the OpenResty bundle's change logs:

<http://openresty.org/#Changes>


## Test Suite

This module comes with a Perl-driven test suite. The [test cases](http://github.com/openresty/set-misc-nginx-module/tree/master/t/) are
[declarative](http://github.com/openresty/set-misc-nginx-module/blob/master/t/escape-uri.t) too. Thanks to the [Test::Nginx](http://search.cpan.org/perldoc?Test::Nginx) module in the Perl world.

To run it on your side:

```bash

 $ PATH=/path/to/your/nginx-with-set-misc-module:$PATH prove -r t
```

You need to terminate any Nginx processes before running the test suite if you have changed the Nginx server binary.

Because a single nginx server (by default, `localhost:1984`) is used across all the test scripts (`.t` files), it's meaningless to run the test suite in parallel by specifying `-jN` when invoking the `prove` utility.


## See Also
* [Nginx Devel Kit](https://github.com/simpl/ngx_devel_kit)
* [The OpenResty bundle](http://openresty.org)


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-set-misc](https://github.com/openresty/set-misc-nginx-module){target=_blank}.

# *shibboleth*: Shibboleth Auth Request module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-shibboleth
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_shibboleth_module.so;
```


This document describes nginx-module-shibboleth [v2.0.2](https://github.com/nginx-shib/nginx-http-shibboleth/releases/tag/v2.0.2){target=_blank} 
released on May 26 2023.

<hr />

[![image](https://github.com/nginx-shib/nginx-http-shibboleth/actions/workflows/build.yml/badge.svg)](https://github.com/nginx-shib/nginx-http-shibboleth/actions/workflows/build.yml)

This module allows Nginx to work with Shibboleth, by way of Shibboleth's
FastCGI authorizer. This module requires specific configuration in order
to work correctly, as well as Shibboleth's FastCGI authorizer
application available on the system. It aims to be similar to parts of
Apache's
[mod_shib](https://wiki.shibboleth.net/confluence/display/SHIB2/NativeSPApacheConfig),
though Shibboleth authorisation and authentication settings are
configured via
[shibboleth2.xml](https://wiki.shibboleth.net/confluence/display/SHIB2/NativeSPShibbolethXML)
rather than in the web server configuration.

With this module configured against a `location` block, incoming
requests are authorized within Nginx based upon the result of a
subrequest to Shibboleth's FastCGI authorizer. In this process, this
module can be used to copy user attributes from a successful authorizer
response into Nginx's original request as headers or environment
parameters for use by any backend application. If authorization is not
successful, the authorizer response status and headers are returned to
the client, denying access or redirecting the user's browser accordingly
(such as to a WAYF page, if so configured).

This module works at access phase and therefore may be combined with
other access modules (such as `access`, `auth_basic`) via the `satisfy`
directive. This module can be also compiled alongside
`ngx_http_auth_request_module`, though use of both of these modules in
the same `location` block is untested and not advised.

Read more about the [Behaviour](#behaviour) below and consult
[Configuration](#configuration) for important notes on avoiding spoofing
if using headers for attributes.

For further information on why this is a dedicated module, see
<https://forum.nginx.org/read.php?2,238523,238523#msg-238523>

## Directives

The following directives are added into your Nginx configuration files.
The contexts mentioned below show where they may be added.

shib_request \<uri\> **Context:** `http`, `server`, `location`  
**Default:** `off`

Switches the Shibboleth auth request module on and sets URI which will
be asked for authorization. The configured URI should refer to a Nginx
location block that points to your Shibboleth FastCGI authorizer.

The HTTP status and headers of the response resulting from the
sub-request to the configured URI will be returned to the user, in
accordance with the [FastCGI Authorizer
specification](https://web.archive.org/web/20160306081510/http://fastcgi.com/drupal/node/6?q=node/22#S6.3).
The one (potentially significant) caveat is that due to the way Nginx
operates at present with regards to subrequests (what an Authorizer
effectively requires), the request body will *not* be forwarded to the
authorizer, and similarly, the response body from the authorizer will
*not* be returned to the client.

Configured URIs are not restricted to using a FastCGI backend to
generate a response, however. This may be useful during testing or
otherwise, as you can use Nginx's built in `return` and `rewrite`
directives to produce a suitable response. Additionally, this module may
be used with *any* FastCGI authorizer, although operation may be
affected by the above caveat.

Warning

The `shib_request` directive no longer requires the `shib_authorizer`
flag. This must be removed for Nginx to start. No other changes are
required.

shib_request_set \<variable\> \<value\>  
**Context:** `http`, `server`, `location`  
**Default:** `none`

Set the `variable` to the specified `value` after the auth request has
completed. The `value` may contain variables from the auth request's
response. For instance, `$upstream_http_*`, `$upstream_status`, and any
other variables mentioned in the
[nginx_http_upstream_module](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#variables)
documentation.

This directive can be used to introduce Shibboleth attributes into the
environment of the backend application, such as
<span class="title-ref">$\_SERVER</span> for a FastCGI PHP application
and is the recommended method of doing so. See the
[Configuration](#configuration) documentation for an example.

shib_request_use_headers on **Context:** `http`, `server`, `location`  
**Default:** `off`

Note

Added in v2.0.0.

Copy attributes from the Shibboleth authorizer response into the main
request as headers, making them available to upstream servers and
applications. Use this option only if your upstream/application does not
support server parameters via `shib_request_set`.

With this setting enabled, Authorizer response headers beginning with
`Variable-\*` are extracted, stripping the `Variable-` substring from
the header name, and copied into the main request before it is sent to
the backend. For example, an authorizer response header such as
`Variable-Commonname: John Smith` would result in
`Commonname: John Smith` being added to the main request, and thus sent
to the backend.

**Beware of spoofing** - you must ensure that your backend application
is protected from injection of headers. Consult the
[Configuration](#configuration) example on how to achieve this.

## Configuration

For full details about configuring the Nginx/Shibboleth environment, see
the documentation at
<https://github.com/nginx-shib/nginx-http-shibboleth/blob/master/CONFIG.rst>.

An example `server` block consists of the following:

``` nginx
#FastCGI authorizer for Auth Request module
location = /shibauthorizer {
    internal;
    include fastcgi_params;
    fastcgi_pass unix:/opt/shibboleth/shibauthorizer.sock;
}

#FastCGI responder
location /Shibboleth.sso {
    include fastcgi_params;
    fastcgi_pass unix:/opt/shibboleth/shibresponder.sock;
}

## Using the ``shib_request_set`` directive, we can introduce attributes as
## environment variables for the backend application. In this example, we
## set ``fastcgi_param`` but this could be any type of Nginx backend that
## supports parameters (by using the appropriate *_param option)
#
## The ``shib_fastcgi_params`` is an optional set of default parameters,
## available in the ``includes/`` directory in this repository.
#
## Choose this type of configuration unless your backend application
## doesn't support server parameters or specifically requires headers.
location /secure-environment-vars {
    shib_request /shibauthorizer;
    include shib_fastcgi_params;
    shib_request_set $shib_commonname $upstream_http_variable_commonname;
    shib_request_set $shib_email $upstream_http_variable_email;
    fastcgi_param COMMONNAME $shib_commonname;
    fastcgi_param EMAIL $shib_email;
    fastcgi_pass unix:/path/to/backend.socket;
}

## A secured location. All incoming requests query the Shibboleth FastCGI authorizer.
## Watch out for performance issues and spoofing!
#
## Choose this type of configuration for ``proxy_pass`` applications
## or backends that don't support server parameters.
location /secure {
    shib_request /shibauthorizer;
    shib_request_use_headers on;

    # Attributes from Shibboleth are introduced as headers by the FastCGI
    # authorizer so we must prevent spoofing. The
    # ``shib_clear_headers`` is a set of default header directives,
    # available in the `includes/` directory in this repository.
    include shib_clear_headers;

    # Add *all* attributes that your application uses, including all
    #variations.
    more_clear_input_headers 'displayName' 'mail' 'persistent-id';

    # This backend application will receive Shibboleth variables as request
    # headers (from Shibboleth's FastCGI authorizer)
    proxy_pass http://localhost:8080;
}
```

Note that we use the
[headers-more-nginx-module](https://github.com/openresty/headers-more-nginx-module)
to clear potentially dangerous input headers and avoid the potential for
spoofing. The latter example with environment variables isn't
susceptible to header spoofing, as long as the backend reads data from
the environment parameters **only**.

A [default
configuration](https://github.com/nginx-shib/nginx-http-shibboleth/blob/master/includes/shib_clear_headers)
is available to clear the basic headers from the Shibboleth authorizer,
but you must ensure you write your own clear directives for all
attributes your application uses. Bear in mind that some applications
will try to read a Shibboleth attribute from the environment and then
fall back to headers, so review your application's code even if you are
not using `shib_request_use_headers`.

With use of `shib_request_set`, a [default
params](https://github.com/nginx-shib/nginx-http-shibboleth/blob/master/includes/shib_fastcgi_params)
file is available which you can use as an nginx `include` to ensure all
core Shibboleth variables get passed from the FastCGI authorizer to the
application. Numerous default attributes are included so remove the ones
that aren't required by your application and add Federation or IDP
attributes that you need. This default params file can be re-used for
upstreams that aren't FastCGI by simply changing the `fastcgi_param`
directives to `uwsgi_param`, `scgi_param` or so forth.

### Gotchas

-   Subrequests, such as the Shibboleth auth request, aren't processed
    through header filters. This means that built-in directives like
    `add_header` will **not** work if configured as part of the a
    `/shibauthorizer` block. If you need to manipulate subrequest
    headers, use `more_set_headers` from the module `headers-more`.

    See <https://forum.nginx.org/read.php?29,257271,257272#msg-257272>.

## Behaviour

This module follows the [FastCGI Authorizer
specification](https://web.archive.org/web/20160306081510/http://fastcgi.com/drupal/node/6?q=node/22#S6.3)
where possible, but has some notable deviations - with good reason. The
behaviour is thus:

-   An authorizer subrequest is comprised of all aspects of the original
    request, excepting the request body as Nginx does not support
    buffering of request bodies. As the Shibboleth FastCGI authorizer
    does not consider the request body, this is not an issue.

-   If an authorizer subrequest returns a `200` status, access is
    allowed.

    If `shib_request_use_headers` is enabled, and response headers
    beginning with `Variable-\*` are extracted, stripping the
    `Variable-` substring from the header name, and copied into the main
    request. Other authorizer response headers not prefixed with
    `Variable-` and the response body are ignored. The FastCGI spec
    calls for `Variable-*` name-value pairs to be included in the
    FastCGI environment, but we make them headers so as they may be used
    with *any* backend (such as `proxy_pass`) and not just restrict
    ourselves to FastCGI applications. By passing the `Variable-*` data
    as headers instead, we end up following the behaviour of
    `ShibUseHeaders On` in `mod_shib` for Apache, which passes these
    user attributes as headers.

    In order to pass attributes as environment variables (the equivalent
    to `ShibUseEnvironment On` in `mod_shib`), attributes must be
    manually extracted using `shib_request_set` directives for each
    attribute. This cannot (currently) be done *en masse* for all
    attributes as each backend may accept parameters in a different way
    (`fastcgi_param`, `uwsgi_param` etc). Pull requests are welcome to
    automate this behaviour.

-   If the authorizer subrequest returns *any* other status (including
    redirects or errors), the authorizer response's status and headers
    are returned to the client.

    This means that on `401 Unauthorized` or `403 Forbidden`, access
    will be denied and headers (such as `WWW-Authenticate`) from the
    authorizer will be passed to client. All other authorizer responses
    (such as `3xx` redirects) are passed back to the client, including
    status and headers, allowing redirections such as those to WAYF
    pages and the Shibboleth responder (`Shibboleth.sso`) to work
    correctly.

    The FastCGI Authorizer spec calls for the response body to be
    returned to the client, but as Nginx does not currently support
    buffering subrequest responses (`NGX_HTTP_SUBREQUEST_IN_MEMORY`),
    the authorizer response body is effectively ignored. A workaround is
    to have Nginx serve an `error_page` of its own, like so:

    ``` nginx
    location /secure {
       shib_request /shibauthorizer;
       error_page 403 /shibboleth-forbidden.html;
       ...
    }
    ```

    This serves the given error page if the Shibboleth authorizer denies
    the user access to this location. Without `error_page` specified,
    Nginx will serve its generic error pages.

    Note that this does *not* apply to the Shibboleth responder
    (typically hosted at `Shibboleth.sso`) as it is a FastCGI responder
    and Nginx is fully compatible with this as no subrequests are used.

    For more details, see
    <https://forum.nginx.org/read.php?2,238444,238453>.

Whilst this module is geared specifically for Shibboleth's FastCGI
authorizer, it will likely work with other authorizers, bearing in mind
the deviations from the spec above.

## Tests

Tests are automatically run on GitHub Actions (using [this
configuration](https://github.com/nginx-shib/nginx-http-shibboleth/blob/master/.github/workflows/build.yml))
whenever new commits are made to the repository or when new pull
requests are opened. If something breaks, you'll be informed and the
results will be reported on GitHub.

Tests are written using a combination of a simple Bash script for
compilation of our module with different versions and configurations of
Nginx and the
[Test::Nginx](https://metacpan.org/pod/Test::Nginx::Socket) Perl test
scaffolding for integration testing. Consult the previous link for
information on how to extend the tests, and also refer to the underlying
[Test::Base](https://metacpan.org/pod/Test::Base#blocks-data-section-name)
documentation on aspects like the
<span class="title-ref">blocks()</span> function.

Integration tests are run automatically by CI but can also be run
manually (requires Perl & CPAN to be installed):

``` bash
cd nginx-http-shibboleth
cpanm --notest --local-lib=$HOME/perl5 Test::Nginx
## nginx must be present in PATH and built with debugging symbols
PERL5LIB=$HOME/perl5/lib/perl5 prove
```

## Help & Support

Support requests for Shibboleth configuration and Nginx or web server
setup should be directed to the Shibboleth community users mailing list.
See <https://www.shibboleth.net/community/lists/> for details.

## Debugging

Because of the complex nature of the nginx/FastCGI/Shibboleth stack,
debugging configuration issues can be difficult. Here's some key points:

1.  Confirm that `nginx-http-shibboleth` is successfully built and
    installed within nginx. You can check by running `nginx -V` and
    inspecting the output for
    `--add-module=[path]/nginx-http-shibboleth` or
    `--add-dynamic-module=[path]/nginx-http-shibboleth`.

2.  If using dynamic modules for nginx, confirm you have used the
    `load_module` directive to load this module. Your use of
    `shib_request` and other directives will fail if you have forgotten
    to load the module.

3.  If using a version of nginx that is different to those we [test
    with](https://github.com/nginx-shib/nginx-http-shibboleth/blob/master/.github/workflows/build.yml)
    or if you are using other third-party modules, you should run the
    test suite above to confirm compatibility. If any tests fail, then
    check your configuration or consider updating your nginx version.

4.  Shibboleth configuration: check your `shibboleth2.xml` and
    associated configuration to ensure your hosts, paths and attributes
    are being correctly released. An [example
    configuration](https://github.com/nginx-shib/nginx-http-shibboleth/blob/master/CONFIG.rst#configuring-shibboleths-shibboleth2xml-to-recognise-secured-paths)
    can help you identify key "gotchas" to configuring `shibboleth2.xml`
    to work with the FastCGI authorizer.

5.  Application-level: within your code, always start with the simplest
    possible debugging output (such as printing the request environment)
    and work up from there. If you want to create a basic, stand-alone
    app, take a look at the
    [Bottle](https://github.com/nginx-shib/nginx-http-shibboleth/wiki/bottle)
    configuration on the wiki.

6.  Debugging module internals: if you've carefully checked all of the
    above, then you can also debug the behaviour of this module itself.
    You will need to have compiled nginx with debugging support (via
    `./auto/configure --with-debug ...`) and when running nginx, it is
    easiest if you're able run in the foreground with debug logging
    enabled. Add the following to your `nginx.conf`:

    ``` nginx
    daemon off;
    error_log stderr debug;
    ```

    and run nginx. Upon starting nginx you should see lines containing
    <span class="title-ref">\[debug\]</span> and as you make requests,
    console logging will continue. If this doesn't happen, then check
    your nginx configuration and compilation process.

    When you eventually make a request that hits (or should invoke) the
    `shib_request` location block, you will see lines like so in the
    output:

    ``` nginx
    [debug] 1234#0: shib request handler
    [debug] 1234#0: shib request set variables
    [debug] 1234#0: shib request authorizer handler
    [debug] 1234#0: shib request authorizer allows access
    [debug] 1234#0: shib request authorizer copied header: "AUTH_TYPE: shibboleth"
    [debug] 1234#0: shib request authorizer copied header: "REMOTE_USER: john.smith@example.com"
    ...
    ```

    If you don't see these types of lines containing
    <span class="title-ref">shib request ...</span>, or if you see
    *some* of the lines above but not where headers/variables are being
    copied, then double-check your nginx configuration. If you're still
    not getting anywhere, then you can add your own debugging lines into
    the source (follow this module's examples) to eventually determine
    what is going wrong and when. If doing this, don't forget to
    recompile nginx and/or `nginx-http-shibboleth` whenever you make a
    change.

If you believe you've found a bug in the core module code, then please
[create an
issue](https://github.com/nginx-shib/nginx-http-shibboleth/issues).

You can also search existing issues as it is likely someone else has
encountered a similar issue before.

## Versioning

This module uses [Semantic Versioning](https://semver.org/) and all
releases are tagged on GitHub, which allows package downloads of
individual tags.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-shibboleth](https://github.com/nginx-shib/nginx-http-shibboleth){target=_blank}.

# *slowfs*: NGINX SlowFS Cache Module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-slowfs
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_slowfs_module.so;
```


This document describes nginx-module-slowfs [v1.11](https://github.com/dvershinin/ngx_slowfs_cache/releases/tag/1.11){target=_blank} 
released on Aug 23 2020.

<hr />
`ngx_slowfs_cache` is `nginx` module which allows caching of static files
(served using `root` directive). This enables one to create fast caches
for files stored on slow filesystems, for example:

- storage: network disks, cache: local disks,
- storage: 7,2K SATA drives, cache: 15K SAS drives in RAID0.


**WARNING! There is no point in using this module when cache is placed
on the same speed disk(s) as origin.**


## Sponsors
`ngx_slowfs_cache` was fully funded by [c2hosting.com](http://c2hosting.com).


## Status
This module is production-ready and it's compatible with following nginx
releases:

- 0.7.x (tested with 0.7.60 to 0.7.69),
- 0.8.x (tested with 0.8.0 to 0.8.55),
- 0.9.x (tested with 0.9.0 to 0.9.7),
- 1.0.x (tested with 1.0.0 to 1.0.15),
- 1.1.x (tested with 1.1.0 to 1.1.19),
- 1.2.x (tested with 1.2.0 to 1.2.7),
- 1.3.x (tested with 1.3.0 to 1.3.14).


## Configuration notes
`slowfs_cache_path` and `slowfs_temp_path` values should point to the same
filesystem, otherwise files will be copied twice.

`ngx_slowfs_cache` currently doesn't work when AIO is enabled.


## Configuration directives
## slowfs_cache
* **syntax**: `slowfs_cache zone_name`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets area used for caching (previously defined using `slowfs_cache_path`).
  

## slowfs_cache_key
* **syntax**: `slowfs_cache_key key`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets key for caching.


## slowfs_cache_purge
* **syntax**: `slowfs_cache_purge zone_name key`
* **default**: `none`
* **context**: `location`

Sets area and key used for purging selected pages from cache.


## slowfs_cache_path
* **syntax**: `slowfs_cache_path path [levels] keys_zone=zone_name:zone_size [inactive] [max_size]`
* **default**: `none`
* **context**: `http`

Sets cache area and its structure.


## slowfs_temp_path
* **syntax**: `slowfs_temp_path path [level1] [level2] [level3]`
* **default**: `/tmp 1 2`
* **context**: `http`
  
Sets temporary area where files are stored before they are moved to cache area.


## slowfs_cache_min_uses
* **syntax**: `slowfs_cache_min_uses number`
* **default**: `1`
* **context**: `http`, `server`, `location`

Sets number of uses after which file is copied to cache.


## slowfs_cache_valid
* **syntax**: `slowfs_cache_valid [reply_code] time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets time for which file will be served from cache.


## slowfs_big_file_size
* **syntax**: `slowfs_big_file_size size`
* **default**: `128k`
* **context**: `http`, `server`, `location`

Sets minimum file size for `big` files. Worker processes `fork()` child process
before they start copying `big` files to avoid any service disruption. 


## Configuration variables
## $slowfs_cache_status
Represents availability of cached file.

Possible values are: `MISS`, `HIT` and `EXPIRED`.


## Sample configuration
    http {
        slowfs_cache_path  /tmp/cache levels=1:2 keys_zone=fastcache:10m;
        slowfs_temp_path   /tmp/temp 1 2;

        server {
            location / {
                root                /var/www;
                slowfs_cache        fastcache;
                slowfs_cache_key    $uri;
                slowfs_cache_valid  1d;
            }

            location ~ /purge(/.*) {
                allow               127.0.0.1;
                deny                all;
                slowfs_cache_purge  fastcache $1;
            }
       }
    }

## Testing
`ngx_slowfs_cache` comes with complete test suite based on [Test::Nginx](http://github.com/agentzh/test-nginx).

You can test it by running:

`$ prove`


## See also
- [ngx_cache_purge](http://github.com/FRiCKLE/ngx_cache_purge).

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-slowfs](https://github.com/dvershinin/ngx_slowfs_cache){target=_blank}.

# *small-light*: Dynamic image transformation module For NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-small-light
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_small_light_module.so;
```


This document describes nginx-module-small-light [v0.9.4](https://github.com/dvershinin/ngx_small_light/releases/tag/v0.9.4){target=_blank} 
released on May 27 2020.

<hr />

A dynamic image transformation module for [nginx](http://nginx.org/).

## Getting started

Add the configuration below to some server context in nginx.conf and start nginx.

```nginx
small_light on;
location ~ small_light[^/]*/(.+)$ {
    set $file $1;
    rewrite ^ /$file;
}
```

If you can get the original image of image.jpg from the URL below,

```
http://$host:$port/img/image.jpg
```

You will be able to get the converted image of image.jpg from the URL below.

```
http://$host:$port/small_light(dw=300,dh=300)/img/image.jpg
```

The part of `small_light(...)` is called **small_light function**.

## Configuration example

There is some configuration example below.

```nginx
server {
    listen 8000;
    server_name localhost;

    small_light on;
    small_light_pattern_define msize dw=500,dh=500,da=l,q=95,e=imagemagick,jpeghint=y;
    small_light_pattern_define ssize dw=120,dh=120,da=l,q=95,e=imlib2,jpeghint=y;

    # http://localhost:8000/small_light(p=msize)/img/filename.jpg -> generate msize image
    # http://localhost:8000/small_light(p=ssize)/img/filename.jpg -> generate ssize image
    # http://localhost:8000/small_light(of=gif,q=100)/img/filename.jpg -> generate gif image which quality is 100
    location ~ small_light[^/]*/(.+)$ {
        set $file $1;
        rewrite ^ /$file;
    }
} 
```

## Directives

### small_light

|Syntax     |*small_light on &#124; off*|
|-----------|---------------------------|
|**Default**|off                        |
|**Context**|server, location           |

This directive sets whether image-processing with `ngx_small_light` is enabled in a server context.

### small_light_getparam_mode

|Syntax     |*small_light_getparam_mode on &#124; off*|
|-----------|-----------------------------------------|
|**Default**|off                                      |
|**Context**|server, location                         |

This directive sets whether converting-image is enabled by GET parameters 
instead of **small_light function** (e.g. `/small_light(dw=200,dh=200)`).
At the expense of it, a **small_light function** is disabled.
But you need to set both `small_light` and `small_light_getparam_mode` **on** to enable the feature of this directive.

### small_light_material_dir

|Syntax     |*small_light_material_dir path*|
|-----------|---------------------------------------------|
|**Default**|                                             |
|**Context**|server                                       |

This directive assigns the directory for embedded icon images.

### small_light_pattern_define

|Syntax     |*small_light_pattern_define pattern_name parameters*|
|-----------|----------------------------------------------------|
|**Default**|                                                    |
|**Context**|server                                              |

This directive names comma-delimited parameters.

### small_light_radius_max

|Syntax     |*small_light_radius_max number*|
|-----------|-------------------------------|
|**Default**|10                             |
|**Context**|server,location                |

This directive sets maximum radius value of geometry for `sharpen` and `unsharp` and `blur`.

### small_light_sigma_max

|Syntax     |*small_light_sigma_max number*|
|-----------|-------------------------------|
|**Default**|10                             |
|**Context**|server,location                |

This directive sets maximum sigma value of geometry for `sharpen` and `unsharp` and `blur`.

### small_light_imlib2_temp_dir

|Syntax     |*small_light_imlib2_temp_dir path* [*level1* [*level2* [*level 3* ]]]|
|-----------|---------------------------------------------------------------------|
|**Default**|small_light_imlib2_temp 1 2                                          |
|**Context**|server                                                               |

This directive assigns the directory for temporary file for Imlib2 processing.
This directive is available when Imlib2 is enabled.

### small_light_buffer

|Syntax     |*small_ligh_buffer size*|
|-----------|------------------------|
|**Default**|1m                      |
|**Context**|server                  |

This directive assigns the maximum size of the buffer used for reading images
when Content-Length is not set in response headers.

## Parameters for small_light function

|Parameter  |Type  |Default    |Description                                     |ImageMagick|Imlib2|GD |
|-----------|------|-----------|------------------------------------------------|-----------|------|---|
|p          |string|           |named pattern of comma-delimited parameters     |        :o:|   :o:|:o:|
|e          |string|imagemagick|engine name (imagemagick, imlib2, gd)           |           |      |   |
|q          |number|           |quality                                         |        :o:|   :o:|:o:|
|of         |string|           |output format (jpg, gif, png, webp)             |        :o:|   :o:|:o:|
|jpeghint   |char  |n          |enable jpeg hinting (y, n)                      |        :o:|   :o:|:x:|
|dw         |coord |sw         |destination width                               |        :o:|   :o:|:o:|
|dh         |coord |sh         |destination height                              |        :o:|   :o:|:o:|
|dx         |coord |sx         |destination x coordinate                        |        :o:|   :o:|:o:|
|dy         |coord |sy         |destination y coordinate                        |        :o:|   :o:|:o:|
|da         |char  |l          |destination aspect ratio contol (l, s, n)       |        :o:|   :o:|:o:|
|ds         |char  |n          |destination scaling control (s, n)              |        :o:|   :o:|:o:|
|cw         |number|           |canvas width                                    |        :o:|   :o:|:o:|
|ch         |number|           |canvas height                                   |        :o:|   :o:|:o:|
|cc         |color |000000     |canvas color                                    |        :o:|   :o:|:o:|
|bw         |number|           |border width                                    |        :o:|   :o:|:o:|
|bh         |number|           |border height                                   |        :o:|   :o:|:o:|
|bc         |color |000000     |border color                                    |        :o:|   :o:|:o:|
|sw         |coord |           |source witdh                                    |        :o:|   :o:|:o:|
|sh         |coord |           |source height                                   |        :o:|   :o:|:o:|
|sx         |coord |           |source x coordinate                             |        :o:|   :o:|:o:|
|sy         |coord |           |source y coordinate                             |        :o:|   :o:|:o:|
|pt         |char  |n          |pass through control (y, n)                     |        :o:|   :o:|:o:|
|sharpen    |string|           |radius,sigma (e.g. 10x5)                        |        :o:|   :o:|:o:|
|unsharp    |string|           |radius,sigma,amount,threshold (e.g 2x5+0.5+0)   |        :o:|   :x:|:x:|
|blur       |string|           |radius,sigma (e.g. 5x10)                        |        :o:|   :o:|:x:|
|embedicon  |string|           |embedded icon file in `small_light_material_dir`|        :o:|   :x:|:x:|
|ix         |number|0          |embedded icon x coordinate                      |        :o:|   :x:|:x:|
|iy         |number|0          |embedded icon y coordinate                      |        :o:|   :x:|:x:|
|angle      |number|0          |angle of rotation (90, 180, 270)                |        :o:|   :o:|:o:|
|progressive|char  |n          |make JPEG progressive (y, n)                    |        :o:|   :x:|:x:|
|cmyk2rgb   |char  |n          |convert colorspace from CMYK to sRGB (y, n)     |        :o:|   :x:|:x:|
|rmprof     |char  |n          |remove profile (y, n)                           |        :o:|   :x:|:x:|
|autoorient |char  |n          |enable adjust image orientation automatically (y, n)  |  :o:|   :x:|:x:|

The values of `da` are `l` and `s` and `n`. These present the meanings below.

 * `l`: long-edge based
 * `s`: short-edge based
 * `n`: nope

There are any limitations below.

 * `of=gif` and `of=webp` are not supported when `e=imlib2`.
 * `autoorient` is available ImageMagick-6.9.0 or later.
 * The value of `radius,sigma` for `sharpen` and `unsharp` and `blur` is limited by `small_light_radius_max` and `small_light_sigma_max`.

There are the types of each parameter below.

|Type  |Description                                      |
|------|-------------------------------------------------|
|coord |coordicante or pixel. percent when appending 'p' |
|char  |character                                        |
|number|integer number                                   |
|color |rrggbb or rrggbbaa                               |
|string|string                                           |

## Named Pattern

`ngx_small_light` supports to name comma-delimited parameters with the `small_light_define_patern`.

```nginx
small_light_pattern_define small dw=120,dh=120,q=80,e=imagemagick,jpeghint=y;
```

If the line above is added to some server context in nginx.conf, the two URLs below return same response.

 * `http://$host:$port/small_light(p=small)/img/image.jpg`
 * `http://$host:$port/small_light(dw=120,dh=120,q=80,e=imagemagick,jpeghint=y)/img/image.jpg`

## Using GET parameters

`ngx_small_light` supports to convert image not only by **small_light function** but by GET paramenters in `v0.5.0` or later.
You need to set both `small_light` and `small_light_getparam_mode` **on** to enable this feature.
At the expense of enabling this feature, **small_light function** (e.g. `/small_light(dw=300,dh=300)/img.jpg` is disabled.

```nginx
small_light on;
small_light_getparam_mode on;
```

In the configuration above, the url below does not return converted image.

```
http://localhost:8000/small_light(dw=200,dh=200)/img/image.jpg
```

Instead the url below returns converted image expected by right.

```
http://localhost:8000/img/image.jpg?dw=200&dh=200
```

## Enabling WebP Transformation

`ngx_small_light` supports WebP transformation with ImageMagick and GD.
Given `of=webp` to **small_light function**, `ngx_small_light` transforms image format into WebP.
But ImageMagick requires libwebp and GD requires libvpx.
You need to embed these libraries in building ImageMagick and GD for enabling WebP transformation.

If WebP transformation is not available, `nginx` outputs the line like below in error.log in processing image with `of=webp`.

```
WebP is not supported
```

If WebP transformation with ImageMagick is available, the output of `convert -list format` includes the line like below.

```
$ convert -list format | grep -i webp
     WEBP* WEBP      rw-   WebP Image Format (libwebp 0.5.0[0208])
```

If WebP transformation with GD is available, the output of `gdlib-config --libs` includes `-lvpx`.

In general, the packages of ImageMagick and GD provided from the linux distributions
such as Ubuntu and CentOS does not embed the library for WebP transformation by default.
In such cases, you need to build ImageMagick or GD yourself.

## Optimizing Tips

There are some optimizing tips for `ngx_small_light`.

### JPEG hinting

When the output format is JPEG and image-converting engine is ImageMagick or Imlib2,
you may give `jpeghint=y`. The speed of processing images is improved dramatically.

### Limit thread-number with OpenMP

When image-converting engine is ImageMagick and the version of `ngx_small_light` is lower than `v0.6.14`, 
giving 1 to `OMP_NUM_THREADS` or `MAGICK_THREAD_LIMIT` in `nginx.conf` is recommended strongly.
Because OpenMP is enabled in ImageMagick by default and ImageMagick enabled OpenMP is very slow on multi-process environment.

```nginx
env OMP_NUM_THREADS=1; # or env MAGICK_THREAD_LIMIT=1;
```

Or you can avoid this problem by building ImageMagick with `--disable-openmp`.

In `v0.6.14` or later, they are no longer required. Because `ngx_small_light` always sets the thread-number with OpenMP 1.

## Limitations

`ngx_small_light` has the limitations below.

## Not supported features with Imlib2

The transformation with Imlib2 does not support to write GIF-image.
Because Imlib2 has the function for loading GIF-image but does not have the function for saving.
Additionally, the transformation by Imlib2 does not support to write and read WebP-image.
So `of=gif` and `e=imlib2` are not enabled to specify at once.
If these are specified, `ngx_small_light` returns 415(Unsupported Media Type).

## Not supported features with GD

The transformation with GD supports to write WebP-image. But it is the experimental feature.

## Not supported animated GIF

`ngx_small_light` does not support the transformation kept animation for animated GIF.
Because it takes long time to transform(e.g. resize, crop) animated GIF kept animation.
So it is not realistic for `ngx_small_light` to support an animated GIF.

If the animated GIF is given, `ngx_small_light` transforms only the first frame.

## Running Tests

```sh
perl Build.PL
cpanm --installdeps .
NGINX_BIN=${nginx_prefix_dir}/sbin/nginx ./Build test
## or
NGINX_BIN=${nginx_prefix_dir}/sbin/nginx prove t/**/*.t
```

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-small-light](https://github.com/dvershinin/ngx_small_light){target=_blank}.

# *spnego-http-auth*: Nginx module for HTTP SPNEGO auth


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-spnego-http-auth
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_auth_spnego_module.so;
```


This document describes nginx-module-spnego-http-auth [v1.1.1](https://github.com/stnoonan/spnego-http-auth-nginx-module/releases/tag/v1.1.1){target=_blank} 
released on Feb 26 2021.

<hr />

This module implements adds [SPNEGO](http://tools.ietf.org/html/rfc4178)
support to nginx(http://nginx.org).  It currently supports only Kerberos
authentication via [GSSAPI](http://en.wikipedia.org/wiki/GSSAPI)


## Prerequisites

Authentication has been tested with (at least) the following:

* Nginx 1.2 through 1.7
* Internet Explorer 8 and above
* Firefox 10 and above
* Chrome 20 and above
* Curl 7.x (GSS-Negotiate), 7.x (SPNEGO/fbopenssl)

The underlying kerberos library used for these tests was MIT KRB5 v1.8.


## Configuration reference

You can configure GSS authentication on a per-location and/or a global basis:

These options are required.
* `auth_gss`: on/off, for ease of unsecuring while leaving other options in
  the config file
* `auth_gss_keytab`: absolute path-name to keytab file containing service
  credentials

These options should ONLY be specified if you have a keytab containing
privileged principals.  In nearly all cases, you should not put these
in the configuration file, as `gss_accept_sec_context` will do the right
thing.
* `auth_gss_realm`: Kerberos realm name.  If this is specified, the realm is only passed to the nginx variable $remote_user if it differs from this default.  To override this behavior, set *auth_gss_format_full* to 1 in your configuration.
* `auth_gss_service_name`: service principal name to use when acquiring
  credentials.

If you would like to authorize only a specific set of users, you can use the
`auth_gss_authorized_principal` directive.  The configuration syntax supports
multiple entries, one per line.

    auth_gss_authorized_principal <username>@<realm>
    auth_gss_authorized_principal <username2>@<realm>

The remote user header in nginx can only be set by doing basic authentication.
Thus, this module sets a bogus basic auth header that will reach your backend
application in order to set this header/nginx variable.  The easiest way to disable
this behavior is to add the following configuration to your location config.

    proxy_set_header Authorization "";
    
A future version of the module may make this behavior an option, but this should
be a sufficient workaround for now.

If you would like to enable GSS local name rules to rewrite usernames, you can
specify the `auth_gss_map_to_local` option.

## Basic authentication fallback

The module falls back to basic authentication by default if no negotiation is
attempted by the client.  If you are using SPNEGO without SSL, it is recommended
you disable basic authentication fallback, as the password would be sent in
plaintext.  This is done by setting `auth_gss_allow_basic_fallback` in the
config file.

    auth_gss_allow_basic_fallback off

These options affect the operation of basic authentication:
* `auth_gss_realm`: Kerberos realm name.  If this is specified, the realm is
  only passed to the nginx variable $remote_user if it differs from this
  default.  To override this behavior, set *auth_gss_format_full* to 1 in your
  configuration.
* `auth_gss_force_realm`: Forcibly authenticate using the realm configured in
  `auth_gss_realm` or the system default realm if `auth_gss_realm` is not set.
  This will rewrite $remote_user if the client provided a different realm.  If
  *auth_gss_format_full* is not set, $remote_user will not include a realm even
  if one was specified by the client.


## Troubleshooting

###
Check the logs.  If you see a mention of NTLM, your client is attempting to
connect using [NTLMSSP](http://en.wikipedia.org/wiki/NTLMSSP), which is
unsupported and insecure.

### Verify that you have an HTTP principal in your keytab ###

#### MIT Kerberos utilities ####

    $ KRB5_KTNAME=FILE:<path to your keytab> klist -k

or

    $ ktutil
    ktutil: read_kt <path to your keytab>
    ktutil: list

#### Heimdal Kerberos utilities ####

    $ ktutil -k <path to your keytab> list

### Obtain an HTTP principal

If you find that you do not have the HTTP service principal,
are running in an Active Directory environment,
and are bound to the domain such that Samba tools work properly

    $ env KRB5_KTNAME=FILE:<path to your keytab> net ads -P keytab add HTTP

If you are running in a different kerberos environment, you can likely run

    $ env KRB5_KTNAME=FILE:<path to your keytab> krb5_keytab HTTP

### Increase maximum allowed header size

In Active Directory environment, SPNEGO token in the Authorization header includes
PAC (Privilege Access Certificate) information, which includes all security groups
the user belongs to. This may cause the header to grow beyond default 8kB limit and
causes following error message:

    400 Bad Request
    Request Header Or Cookie Too Large

For performance reasons, best solution is to reduce the number of groups the user
belongs to. When this is impractical, you may also choose to increase the allowed
header size by explicitly setting the number and size of Nginx header buffers:

    large_client_header_buffers 8 32k;

## Debugging

The module prints all sort of debugging information if nginx is compiled with
the `--with-debug` option, and the `error_log` directive has a `debug` level.


## NTLM

Note that the module does not support [NTLMSSP](http://en.wikipedia.org/wiki/NTLMSSP)
in Negotiate. NTLM, both v1 and v2, is an exploitable protocol and should be avoided
where possible.

## Help

If you're unable to figure things out, please feel free to open an 
issue on Github and I'll do my best to help you.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-spnego-http-auth](https://github.com/stnoonan/spnego-http-auth-nginx-module){target=_blank}.

# *srcache*: Transparent subrequest-based caching layout for arbitrary NGINX locations


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-srcache
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_srcache_filter_module.so;
```


This document describes nginx-module-srcache [v0.32](https://github.com/dvershinin/srcache-nginx-module/releases/tag/v0.32){target=_blank} 
released on Jun 28 2022.

<hr />

**ngx_srcache** - Transparent subrequest-based caching layout for arbitrary nginx locations


## Status

This module is production ready.

## Synopsis

```nginx

 upstream my_memcached {
     server 10.62.136.7:11211;
     keepalive 10;
 }

 location = /memc {
     internal;

     memc_connect_timeout 100ms;
     memc_send_timeout 100ms;
     memc_read_timeout 100ms;
     memc_ignore_client_abort on;

     set $memc_key $query_string;
     set $memc_exptime 300;

     memc_pass my_memcached;
 }

 location /foo {
     set $key $uri$args;
     srcache_fetch GET /memc $key;
     srcache_store PUT /memc $key;
     srcache_store_statuses 200 301 302 307 308;

     # proxy_pass/fastcgi_pass/drizzle_pass/echo/etc...
     # or even static files on the disk
 }
```

```nginx

 location = /memc2 {
     internal;

     memc_connect_timeout 100ms;
     memc_send_timeout 100ms;
     memc_read_timeout 100ms;
     memc_ignore_client_abort on;

     set_unescape_uri $memc_key $arg_key;
     set $memc_exptime $arg_exptime;

     memc_pass unix:/tmp/memcached.sock;
 }

 location /bar {
     set_escape_uri $key $uri$args;
     srcache_fetch GET /memc2 key=$key;
     srcache_store PUT /memc2 key=$key&exptime=$srcache_expire;

     # proxy_pass/fastcgi_pass/drizzle_pass/echo/etc...
     # or even static files on the disk
 }
```

```nginx

 map $request_method $skip_fetch {
     default     0;
     POST        1;
     PUT         1;
 }

 server {
     listen 8080;

     location /api/ {
         set $key "$uri?$args";

         srcache_fetch GET /memc $key;
         srcache_store PUT /memc $key;

         srcache_methods GET PUT POST;
         srcache_fetch_skip $skip_fetch;

         # proxy_pass/drizzle_pass/content_by_lua/echo/...
     }
 }
```


## Description

This module provides a transparent caching layer for arbitrary nginx locations (like those use an upstream or even serve static disk files). The caching behavior is mostly compatible with [RFC 2616](http://www.ietf.org/rfc/rfc2616.txt).

Usually, [memc-nginx-module](https://github.com/openresty/memc-nginx-module) is used together with this module to provide a concrete caching storage backend. But technically, any modules that provide a REST interface can be used as the fetching and storage subrequests used by this module.

For main requests, the [srcache_fetch](#srcache_fetch) directive works at the end of the access phase, so the [standard access module](http://nginx.org/en/docs/http/ngx_http_access_module.html)'s [allow](http://nginx.org/en/docs/http/ngx_http_access_module.html#allow) and [deny](http://nginx.org/en/docs/http/ngx_http_access_module.html#deny) direcives run *before* ours, which is usually the desired behavior for security reasons.

The workflow of this module looks like below:

![srcache flowchart](http://agentzh.org/misc/image/srcache-flowchart.png "srcache flowchart")


## Subrequest caching

For *subrequests*, we explicitly **disallow** the use of this module because it's too difficult to get right. There used to be an implementation but it was buggy and I finally gave up fixing it and abandoned it.

However, if you're using [lua-nginx-module](https://github.com/openresty/lua-nginx-module), it's easy to do subrequest caching in Lua all by yourself. That is, first issue a subrequest to an [memc-nginx-module](https://github.com/openresty/memc-nginx-module) location to do an explicit cache lookup, if cache hit, just use the cached data returned; otherwise, fall back to the true backend, and finally do a cache insertion to feed the data into the cache.

Using this module for main request caching and Lua for subrequest caching is the approach that we're taking in our business. This hybrid solution works great in production.


## Distributed Memcached Caching

Here is a simple example demonstrating a distributed memcached caching mechanism built atop this module. Suppose we do have three different memcached nodes and we use simple modulo to hash our keys.

```nginx

 http {
     upstream moon {
         server 10.62.136.54:11211;
         server unix:/tmp/memcached.sock backup;
     }

     upstream earth {
         server 10.62.136.55:11211;
     }

     upstream sun {
         server 10.62.136.56:11211;
     }

     upstream_list universe moon earth sun;

     server {
         memc_connect_timeout 100ms;
         memc_send_timeout 100ms;
         memc_read_timeout 100ms;

         location = /memc {
             internal;

             set $memc_key $query_string;
             set_hashed_upstream $backend universe $memc_key;
             set $memc_exptime 3600; # in seconds
             memc_pass $backend;
         }

         location / {
             set $key $uri;
             srcache_fetch GET /memc $key;
             srcache_store PUT /memc $key;

             # proxy_pass/fastcgi_pass/content_by_lua/drizzle_pass/...
         }
     }
 }
```
Here's what is going on in the sample above:
1. We first define three upstreams, `moon`, `earth`, and `sun`. These are our three memcached servers.
1. And then we group them together as an upstream list entity named `universe` with the `upstream_list` directive provided by [set-misc-nginx-module](https://github.com/openresty/set-misc-nginx-module).
1. After that, we define an internal location named `/memc` for talking to the memcached cluster.
1. In this `/memc` location, we first set the `$memc_key` variable with the query string (`$args`), and then use the [set_hashed_upstream](https://github.com/openresty/set-misc-nginx-module#set_hashed_upstream) directive to hash our [$memc_key](https://github.com/openresty/memc-nginx-module#memc_key) over the upsteam list `universe`, so as to obtain a concrete upstream name to be assigned to the variable `$backend`.
1. We pass this `$backend` variable into the [memc_pass](https://github.com/openresty/memc-nginx-module#memc_pass) directive. The `$backend` variable can hold a value among `moon`, `earth`, and `sun`.
1. Also, we define the memcached caching expiration time to be 3600 seconds (i.e., an hour) by overriding the [$memc_exptime](https://github.com/openresty/memc-nginx-module#memc_exptime) variable.
1. In our main public location `/`, we configure the `$uri` variable as our cache key, and then configure [srcache_fetch](#srcache_fetch) for cache lookups and [srcache_store](#srcache_store) for cache updates. We're using two subrequests to our `/memc` location defined earlier in these two directives.

One can use [lua-nginx-module](https://github.com/openresty/lua-nginx-module)'s [set_by_lua](https://github.com/openresty/lua-nginx-module#set_by_lua) or [rewrite_by_lua](https://github.com/openresty/lua-nginx-module#rewrite_by_lua) directives to inject custom Lua code to compute the `$backend` and/or `$key` variables in the sample above.

One thing that should be taken care of is that memcached does have restriction on key lengths, i.e., 250 bytes, so for keys that may be very long, one could use the [set_md5](https://github.com/openresty/set-misc-nginx-module#set_md5) directive or its friends to pre-hash the key to a fixed-length digest before assigning it to `$memc_key` in the `/memc` location or the like.

Further, one can utilize the [srcache_fetch_skip](#srcache_fetch_skip) and [srcache_store_skip](#srcache_store_skip) directives to control what to cache and what not on a per-request basis, and Lua can also be used here in a similar way. So the possibility is really unlimited.

To maximize speed, we often enable TCP (or Unix Domain Socket) connection pool for our memcached upstreams provided by [HttpUpstreamKeepaliveModule](http://wiki.nginx.org/HttpUpstreamKeepaliveModule), for example,

```nginx

 upstream moon {
     server 10.62.136.54:11211;
     server unix:/tmp/memcached.sock backup;
     keepalive 10;
 }
```

where we define a connection pool which holds up to 10 keep-alive connections (per nginx worker process) for our `moon` upstream (cluster).


## Caching with Redis

Redis is an alternative key-value store with many additional features.

Here is a working example using the lua-resty-redis module:

```nginx
  location ~ '\.php$|^/update.php' {
    # cache setup
    set $key $request_uri;
    try_files $uri =404;

    srcache_fetch_skip $skip_cache;
    srcache_store_skip $skip_cache;

    srcache_response_cache_control off;
    srcache_store_statuses 200 201 301 302 307 308 404 503;

    set_escape_uri $escaped_key $key;

    srcache_fetch GET /redis-fetch $key;
    srcache_store PUT /redis-store key=$escaped_key;

    more_set_headers 'X-Cache-Fetch-Status $srcache_fetch_status';
    more_set_headers 'X-Cache-Store-Status $srcache_store_status';

    fastcgi_split_path_info ^(.+?\.php)(|/.*)$;
    # Security note: If you're running a version of PHP older than the
    # latest 5.3, you should have "cgi.fix_pathinfo = 0;" in php.ini.
    # See http://serverfault.com/q/627903/94922 for details.
    include fastcgi_params;
    # Block httproxy attacks. See https://httpoxy.org/.
    fastcgi_param HTTP_PROXY "";
    fastcgi_param SCRIPT_FILENAME /var/www/html/$fastcgi_script_name;
    fastcgi_param PATH_INFO $fastcgi_path_info;
    fastcgi_param QUERY_STRING $query_string;
    fastcgi_intercept_errors on;

    fastcgi_pass upstream-name;
  }

  location /redis-fetch {
    internal;

    resolver 8.8.8.8 valid=300s;
    resolver_timeout 10s;

    content_by_lua_block {
      local key = assert(ngx.var.request_uri, "no key found")
      local redis = require "resty.redis"
      local red, err = redis:new()
      if not red then
        ngx.log(ngx.ERR, "Failed to create redis variable, error -> ", err)
        ngx.exit(500)
      end
      assert(red:connect("redis-master.default.svc.cluster.local", 6379))
      if not red then
        ngx.log(ngx.ERR, "Failed to connect to redis, error -> ", err)
        ngx.exit(500)
      end
      local res, err = red:auth("redispassword")
      if not res then
        ngx.say("failed to authenticate, ", err)
        ngx.exit(500)
      end
      local data = assert(red:get(key))
      assert(red:set_keepalive(10000, 100))
      if res == ngx.null then
        return ngx.exit(404)
      end
      ngx.print(data)
    }
  }

  location /redis-store {
    internal;

    resolver 8.8.8.8 valid=300s;
    resolver_timeout 10s;

    content_by_lua_block {
      local value = assert(ngx.req.get_body_data(), "no value found")
      local key = assert(ngx.var.request_uri, "no key found")
      local redis = require "resty.redis"
      local red, err = redis:new()
      if not red then
        ngx.log(ngx.ERR, "Failed to create redis variable, error -> ", err)
        ngx.exit(500)
      end
      assert(red:connect("redis-master.default.svc.cluster.local", 6379))
      if not red then
        ngx.log(ngx.ERR, "Failed to connect to redis, error -> ", err)
        ngx.exit(500)
      end
      local res, err = red:auth("redispassword")
      if not res then
        ngx.say("failed to authenticate, ", err)
        ngx.exit(500)
      end
      local data = assert(red:set(key, value))
      assert(red:set_keepalive(10000, 100))
      if res == ngx.null then
        return ngx.exit(404)
      end
    }
  }
```


Here is a working example by using the HTTPRedis (fetch) and Redis2 (store) modules:

```nginx

 location /api {
     default_type text/css;

     set $key $uri;
     set_escape_uri $escaped_key $key;

     srcache_fetch GET /redis $key;
     srcache_store PUT /redis2 key=$escaped_key&exptime=120;

     # fastcgi_pass/proxy_pass/drizzle_pass/postgres_pass/echo/etc
 }

 location = /redis {
     internal;

     set_md5 $redis_key $args;
     redis_pass 127.0.0.1:6379;
 }

 location = /redis2 {
     internal;

     set_unescape_uri $exptime $arg_exptime;
     set_unescape_uri $key $arg_key;
     set_md5 $key;

     redis2_query set $key $echo_request_body;
     redis2_query expire $key $exptime;
     redis2_pass 127.0.0.1:6379;
 }
```

This example makes use of the [$echo_request_body](https://github.com/openresty/echo-nginx-module#echo_request_body) variable provided by [echo-nginx-module](https://github.com/openresty/echo-nginx-module). Note that you need the latest version of [echo-nginx-module](https://github.com/openresty/echo-nginx-module), `v0.38rc2` because earlier versions may not work reliably.

Also, you need both [HttpRedisModule](http://wiki.nginx.org/HttpRedisModule) and [redis2-nginx-module](https://github.com/openresty/redis2-nginx-module). The former is used in the [srcache_fetch](#srcache_fetch) subrequest and the latter is used in the [srcache_store](#srcache_store) subrequest.

The Nginx core also has a bug that could prevent [redis2-nginx-module](https://github.com/openresty/redis2-nginx-module)'s pipelining support from working properly in certain extreme conditions. And the following patch fixes this:

   http://mailman.nginx.org/pipermail/nginx-devel/2012-March/002040.html

Note that, however, if you are using the [OpenResty](http://openresty.org/) 1.0.15.3 bundle or later, then you already have everything that you need here in the bundle.


## Cache Key Preprocessing

It is often desired to preprocess the cache key to exclude random noises that may hurt the cache hit rate. For example, random session IDs in the URI arguments are usually desired to get removed.

Consider the following URI querystring

    SID=BC3781C3-2E02-4A11-89CF-34E5CFE8B0EF&UID=44332&L=EN&M=1&H=1&UNC=0&SRC=LK&RT=62

we want to remove the `SID` and `UID` arguments from it. It is easy to achieve if you use [lua-nginx-module](https://github.com/openresty/lua-nginx-module) at the same time:

```nginx

 location = /t {
     rewrite_by_lua '
         local args = ngx.req.get_uri_args()
         args.SID = nil
         args.UID = nil
         ngx.req.set_uri_args(args)
     ';

     echo $args;
 }
```

Here we use the [echo](https://github.com/openresty/echo-nginx-module#echo) directive from [echo-nginx-module](https://github.com/openresty/echo-nginx-module) to dump out
the final value of [$args](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_args) in the end. You can replace it with your
[srcache-nginx-module](https://github.com/openresty/srcache-nginx-module) configurations and upstream configurations instead for
your case. Let's test this /t interface with curl:

    $ curl 'localhost:8081/t?RT=62&SID=BC3781C3-2E02-4A11-89CF-34E5CFE8B0EF&UID=44332&L=EN&M=1&H=1&UNC=0&SRC=LK'
    M=1&UNC=0&RT=62&H=1&L=EN&SRC=LK

It is worth mentioning that, if you want to retain the order of the URI
arguments, then you can do string substitutions on the value of [$args](http://nginx.org/en/docs/http/ngx_http_core_module.html#var_args)
directly, for example,

    location = /t {
        rewrite_by_lua '
            local args = ngx.var.args
            newargs, n, err = ngx.re.gsub(args, [[\b[SU]ID=[^&]*&?]], "", "jo")
            if n and n > 0 then
                ngx.var.args = newargs
            end
        ';

        echo $args;
    }

Now test it with the original curl command again, we get exactly what
we would expect:

    RT=62&L=EN&M=1&H=1&UNC=0&SRC=LK

But for caching purposes, it's good to normalize the URI argument
order so that you can increase the cache hit rate. And the hash table
entry order used by LuaJIT or Lua can be used to normalize the order
as a nice side effect.


## Directives


## srcache_fetch
**syntax:** *srcache_fetch &lt;method&gt; &lt;uri&gt; &lt;args&gt;?*

**default:** *no*

**context:** *http, server, location, location if*

**phase:** *post-access*

This directive registers an access phase handler that will issue an Nginx subrequest to lookup the cache.

When the subrequest returns status code other than `200`, than a cache miss is signaled and the control flow will continue to the later phases including the content phase configured by [ngx_http_proxy_module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html), [ngx_http_fastcgi_module](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html), and others. If the subrequest returns `200 OK`, then a cache hit is signaled and this module will send the subrequest's response as the current main request's response to the client directly.

This directive will always run at the end of the access phase, such that [ngx_http_access_module](http://nginx.org/en/docs/http/ngx_http_access_module.html)'s [allow](http://nginx.org/en/docs/http/ngx_http_access_module.html#allow) and [deny](http://nginx.org/en/docs/http/ngx_http_access_module.html#deny) will always run *before* this.

You can use the [srcache_fetch_skip](#srcache_fetch_skip) directive to disable cache look-up selectively.


## srcache_fetch_skip
**syntax:** *srcache_fetch_skip &lt;flag&gt;*

**default:** *srcache_fetch_skip 0*

**context:** *http, server, location, location if*

**phase:** *post-access*

The `<flag>` argument supports nginx variables. When this argument's value is not empty *and* not equal to `0`, then the fetching process will be unconditionally skipped.

For example, to skip caching requests which have a cookie named `foo` with the value `bar`, we can write

```nginx

 location / {
     set $key ...;
     set_by_lua $skip '
         if ngx.var.cookie_foo == "bar" then
             return 1
         end
         return 0
     ';

     srcache_fetch_skip $skip;
     srcache_store_skip $skip;

     srcache_fetch GET /memc $key;
     srcache_store GET /memc $key;

     # proxy_pass/fastcgi_pass/content_by_lua/...
 }
```
where [lua-nginx-module](https://github.com/openresty/lua-nginx-module) is used to calculate the value of the `$skip` variable at the (earlier) rewrite phase. Similarly, the `$key` variable can be computed by Lua using the [set_by_lua](https://github.com/openresty/lua-nginx-module#set_by_lua) or [rewrite_by_lua](https://github.com/openresty/lua-nginx-module#rewrite_by_lua) directive too.

The standard [map](http://nginx.org/en/docs/http/ngx_http_map_module.html#map) directive can also be used to compute the value of the `$skip` variable used in the sample above:

```nginx

 map $cookie_foo $skip {
     default     0;
     bar         1;
 }
```

but your [map](http://nginx.org/en/docs/http/ngx_http_map_module.html#map) statement should be put into the `http` config block in your `nginx.conf` file though.


## srcache_store
**syntax:** *srcache_store &lt;method&gt; &lt;uri&gt; &lt;args&gt;?*

**default:** *no*

**context:** *http, server, location, location if*

**phase:** *output-filter*

This directive registers an output filter handler that will issue an Nginx subrequest to save the response of the current main request into a cache backend. The status code of the subrequest will be ignored.

You can use the [srcache_store_skip](#srcache_store_skip) and [srcache_store_max_size](#srcache_store_max_size) directives to disable caching for certain requests in case of a cache miss.

Since the `v0.12rc7` release, both the response status line, response headers, and response bodies will be put into the cache. By default, the following special response headers will not be cached:

* Connection
* Keep-Alive
* Proxy-Authenticate
* Proxy-Authorization
* TE
* Trailers
* Transfer-Encoding
* Upgrade
* Set-Cookie

You can use the [srcache_store_pass_header](#srcache_store_pass_header) and/or [srcache_store_hide_header](#srcache_store_hide_header) directives to control what headers to cache and what not.

The original response's data chunks get emitted as soon as
they arrive. `srcache_store` just copies and collects the data in an output filter without postponing them from being sent downstream.

But please note that even though all the response data will be sent immediately, the current Nginx request lifetime will not finish until the srcache_store subrequest completes. That means a delay in closing the TCP connection on the server side (when HTTP keepalive is disabled, but proper HTTP clients should close the connection actively on the client side, which adds no extra delay or other issues at all) or serving the next request sent on the same TCP connection (when HTTP keepalive is in action).


## srcache_store_max_size
**syntax:** *srcache_store_max_size &lt;size&gt;*

**default:** *srcache_store_max_size 0*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

When the response body length is exceeding this size, this module will not try to store the response body into the cache using the subrequest template that is specified in [srcache_store](#srcache_store).

This is particular useful when using a cache storage backend that does have a hard upper limit on the input data. For example, the Memcached server has a default limit of `1 MB` by item.

When `0` is specified (the default value), there's no limit check at all.


## srcache_store_skip
**syntax:** *srcache_store_skip &lt;flag&gt;*

**default:** *srcache_store_skip 0*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

The `<flag>` argument supports Nginx variables. When this argument's value is not empty *and* not equal to `0`, then the storing process will be unconditionally skipped.

Starting from the `v0.25` release, the `<flag>` expression (possibly containing Nginx variables) can be evaluated up to twice: the first time is right after the response header is being sent and when the `<flag>` expression is not evaluated to true values it will be evaluated again right after the end of the response body data stream is seen. Before `v0.25`, only the first time evaluation is performed.

Here's an example using Lua to set $nocache to avoid storing URIs that contain the string "/tmp":

```nginx

 set_by_lua $nocache '
     if string.match(ngx.var.uri, "/tmp") then
         return 1
     end
     return 0';

 srcache_store_skip $nocache;
```


## srcache_store_statuses
**syntax:** *srcache_store_statuses &lt;status1&gt; &lt;status2&gt; ..*

**default:** *srcache_store_statuses 200 301 302 307 308*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

This directive controls what responses to store to the cache according to their status code.

By default, only `200`, `301`, `302`, `307` and `308` responses will be stored to cache and any other responses will skip [srcache_store](#srcache_store).

You can specify arbitrary positive numbers for the response status code that you'd like to cache, even including error code like `404` and `503`. For example:

```nginx

 srcache_store_statuses 200 201 301 302 307 308 404 503;
```

At least one argument should be given to this directive.

This directive was first introduced in the `v0.13rc2` release.


## srcache_store_ranges
**syntax:** *srcache_store_ranges on|off*

**default:** *srcache_store_ranges off*

**context:** *http, server, location, location if*

**phase:** *output-body-filter*

When this directive is turned on (default to `off`), [srcache_store](#srcache_store) will also store 206 Partial Content responses generated by the standard `ngx_http_range_filter_module`. If you turn this directive on, you MUST add `$http_range` to your cache keys. For example,

```nginx

 location / {
     set $key "$uri$args$http_range";
     srcache_fetch GET /memc $key;
     srcache_store PUT /memc $key;
 }
```

This directive was first introduced in the `v0.27` release.


## srcache_header_buffer_size
**syntax:** *srcache_header_buffer_size &lt;size&gt;*

**default:** *srcache_header_buffer_size 4k/8k*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

This directive controles the header buffer when serializing response headers for [srcache_store](#srcache_store). The default size is the page size, usually `4k` or `8k` depending on specific platforms.

Note that the buffer is not used to hold all the response headers, but just each individual header. So the buffer is merely needed to be big enough to hold the longest response header.

This directive was first introduced in the `v0.12rc7` release.


## srcache_store_hide_header
**syntax:** *srcache_store_hide_header &lt;header&gt;*

**default:** *no*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

By default, this module caches all the response headers except the following ones:

* Connection
* Keep-Alive
* Proxy-Authenticate
* Proxy-Authorization
* TE
* Trailers
* Transfer-Encoding
* Upgrade
* Set-Cookie

You can hide even more response headers from [srcache_store](#srcache_store) by listing their names (case-insensitive) by means of this directive. For examples,

```nginx

 srcache_store_hide_header X-Foo;
 srcache_store_hide_header Last-Modified;
```

Multiple occurrences of this directive are allowed in a single location.

This directive was first introduced in the `v0.12rc7` release.

See also [srcache_store_pass_header](#srcache_store_pass_header).


## srcache_store_pass_header
**syntax:** *srcache_store_pass_header &lt;header&gt;*

**default:** *no*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

By default, this module caches all the response headers except the following ones:

* Connection
* Keep-Alive
* Proxy-Authenticate
* Proxy-Authorization
* TE
* Trailers
* Transfer-Encoding
* Upgrade
* Set-Cookie

You can force [srcache_store](#srcache_store) to store one or more of these response headers from [srcache_store](#srcache_store) by listing their names (case-insensitive) by means of this directive. For examples,

```nginx

 srcache_store_pass_header Set-Cookie;
 srcache_store_pass_header Proxy-Autenticate;
```

Multiple occurrences of this directive are allowed in a single location.

This directive was first introduced in the `v0.12rc7` release.

See also [srcache_store_hide_header](#srcache_store_hide_header).


## srcache_methods
**syntax:** *srcache_methods &lt;method&gt;...*

**default:** *srcache_methods GET HEAD*

**context:** *http, server, location*

**phase:** *post-access, output-header-filter*

This directive specifies HTTP request methods that are considered by either [srcache_fetch](#srcache_fetch) or [srcache_store](#srcache_store). HTTP request methods not listed will be skipped completely from the cache.

The following HTTP methods are allowed: `GET`, `HEAD`, `POST`, `PUT`, and `DELETE`. The `GET` and `HEAD` methods are always implicitly included in the list regardless of their presence in this directive.

Note that since the `v0.17` release `HEAD` requests are always skipped by [srcache_store](#srcache_store) because their responses never carry a response body.

This directive was first introduced in the `v0.12rc7` release.


## srcache_ignore_content_encoding
**syntax:** *srcache_ignore_content_encoding on|off*

**default:** *srcache_ignore_content_encoding off*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

When this directive is turned `off` (which is the default), non-empty `Content-Encoding` response header will cause [srcache_store](#srcache_store) skip storing the whole response into the cache and issue a warning into nginx's `error.log` file like this:


    [warn] 12500#0: *1 srcache_store skipped due to response header "Content-Encoding: gzip"
                (maybe you forgot to disable compression on the backend?)


Turning on this directive will ignore the `Content-Encoding` response header and store the response as usual (and also without warning).

It's recommended to always disable gzip/deflate compression on your backend server by specifying the following line in your `nginx.conf` file:

```nginx

 proxy_set_header  Accept-Encoding  "";
```

This directive was first introduced in the `v0.12rc7` release.


## srcache_request_cache_control
**syntax:** *srcache_request_cache_control on|off*

**default:** *srcache_request_cache_control off*

**context:** *http, server, location*

**phase:** *post-access, output-header-filter*

When this directive is turned `on`, the request headers `Cache-Control` and `Pragma` will be honored by this module in the following ways:

1. [srcache_fetch](#srcache_fetch), i.e., the cache lookup operation, will be skipped when request headers `Cache-Control: no-cache` and/or `Pragma: no-cache` are present.
1. [srcache_store](#srcache_store), i.e., the cache store operation, will be skipped when the request header `Cache-Control: no-store` is specified.

Turning off this directive will disable this functionality and is considered safer for busy sites mainly relying on cache for speed.

This directive was first introduced in the `v0.12rc7` release.

See also [srcache_response_cache_control](#srcache_response_cache_control).


## srcache_response_cache_control
**syntax:** *srcache_response_cache_control on|off*

**default:** *srcache_response_cache_control on*

**context:** *http, server, location*

**phase:** *output-header-filter*

When this directive is turned `on`, the response headers `Cache-Control` and `Expires` will be honored by this module in the following ways:

* `Cache-Control: private` skips [srcache_store](#srcache_store),
* `Cache-Control: no-store` skips [srcache_store](#srcache_store),
* `Cache-Control: no-cache` skips [srcache_store](#srcache_store),
* `Cache-Control: max-age=0` skips [srcache_store](#srcache_store),
* and `Expires: <date-no-more-recently-than-now>` skips [srcache_store](#srcache_store).

This directive takes priority over the [srcache_store_no_store](#srcache_store_no_store), [srcache_store_no_cache](#srcache_store_no_cache), and [srcache_store_private](#srcache_store_private) directives.

This directive was first introduced in the `v0.12rc7` release.

See also [srcache_request_cache_control](#srcache_request_cache_control).


## srcache_store_no_store
**syntax:** *srcache_store_no_store on|off*

**default:** *srcache_store_no_store off*

**context:** *http, server, location*

**phase:** *output-header-filter*

Turning this directive on will force responses with the header `Cache-Control: no-store` to be stored into the cache when [srcache_response_cache_control](#srcache_response_cache_control) is turned `on` *and* other conditions are met. Default to `off`.

This directive was first introduced in the `v0.12rc7` release.


## srcache_store_no_cache
**syntax:** *srcache_store_no_cache on|off*

**default:** *srcache_store_no_cache off*

**context:** *http, server, location*

**phase:** *output-header-filter*

Turning this directive on will force responses with the header `Cache-Control: no-cache` to be stored into the cache when [srcache_response_cache_control](#srcache_response_cache_control) is turned `on` *and* other conditions are met. Default to `off`.

This directive was first introduced in the `v0.12rc7` release.


## srcache_store_private
**syntax:** *srcache_store_private on|off*

**default:** *srcache_store_private off*

**context:** *http, server, location*

**phase:** *output-header-filter*

Turning this directive on will force responses with the header `Cache-Control: private` to be stored into the cache when [srcache_response_cache_control](#srcache_response_cache_control) is turned `on` *and* other conditions are met. Default to `off`.

This directive was first introduced in the `v0.12rc7` release.


## srcache_default_expire
**syntax:** *srcache_default_expire &lt;time&gt;*

**default:** *srcache_default_expire 60s*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

This directive controls the default expiration time period that is allowed for the [$srcache_expire](#srcache_expire) variable value when neither `Cache-Control: max-age=N` nor `Expires` are specified in the response headers.

The `<time>` argument values are in seconds by default. But it's wise to always explicitly specify the time unit to avoid confusion. Time units supported are "s"(seconds), "ms"(milliseconds), "y"(years), "M"(months), "w"(weeks), "d"(days), "h"(hours), and "m"(minutes). For example,

```nginx

 srcache_default_expire 30m; # 30 minutes
```

This time must be less than 597 hours.

The semantics of a zero expiration time depends on the actual cache backend storage you are currently using, which is agnostic to this
module. In the case of memcached, for example, zero expiration times mean that the item will never expire.

This directive was first introduced in the `v0.12rc7` release.


## srcache_max_expire
**syntax:** *srcache_max_expire &lt;time&gt;*

**default:** *srcache_max_expire 0*

**context:** *http, server, location, location if*

**phase:** *output-header-filter*

This directive controls the maximal expiration time period that is allowed for the [$srcache_expire](#srcache_expire) variable value. This setting takes priority over other calculating methods.

The `<time>` argument values are in seconds by default. But it's wise to always explicitly specify the time unit to avoid confusion. Time units supported are "s"(seconds), "ms"(milliseconds), "y"(years), "M"(months), "w"(weeks), "d"(days), "h"(hours), and "m"(minutes). For example,

```nginx

 srcache_max_expire 2h;  # 2 hours
```

This time must be less than 597 hours.

When `0` is specified, which is the default setting, then there will be *no* limit at all.

This directive was first introduced in the `v0.12rc7` release.


## Variables

## $srcache_expire
**type:** *integer*

**cacheable:** *no*

**writable:** *no*

This Nginx variable gives the recommended expiration time period (in seconds) for the current response being stored into the cache. The algorithm of computing the value is as follows:

1. When the response header `Cache-Control: max-age=N` is specified, then `N` will be used as the expiration time,
1. otherwise if the response header `Expires` is specified, then the expiration time will be obtained by subtracting the current time stamp from the time specified in the `Expires` header,
1. when neither `Cache-Control: max-age=N` nor `Expires` headers are specified, use the value specified in the [srcache_default_expire](#srcache_default_expire) directive.

The final value of this variable will be the value specified by the [srcache_max_expire](#srcache_max_expire) directive if the value obtained in the algorithm above exceeds the maximal value (if any).

You don't have to use this variable for the expiration time.

This variable was first introduced in the `v0.12rc7` release.


## $srcache_fetch_status
**type:** *string*

**cacheable:** *no*

**writable:** *no*

This Nginx variable is evaluated to the status of the "fetch" phase for the caching system. Three values are possible, `HIT`, `MISS`, and `BYPASS`.

When the "fetch" subrequest returns status code other than `200` or its response data is not well-formed, then this variable is evaluated to the value `MISS`.

The value of this variable is only meaningful after the `access` request processing phase, or `BYPASS` is always given.

This variable was first introduced in the `v0.14` release.


## $srcache_store_status
**type:** *string*

**cacheable:** *no*

**writable:** *no*

This Nginx variable gives the current caching status for the "store" phase. Two possible values, `STORE` and `BYPASS` can be obtained.

Because the responses for the "store" subrequest are always discarded, so the value of this variable will always be `STORE` as long as the "store" subrequest is actually issued.

The value of this variable is only meaningful at least when the request headers of the current (main) request are being sent. The final result can only be obtained after all the response body has been sent if the `Content-Length` response header is not specified for the main request.

This variable was first introduced in the `v0.14` release.


## Known Issues
* On certain systems, enabling aio and/or sendfile may stop [srcache_store](#srcache_store) from working. You can disable them in the locations configured by [srcache_store](#srcache_store).
* The [srcache_store](#srcache_store) directive can not be used to capture the responses generated by [echo-nginx-module](https://github.com/openresty/echo-nginx-module)'s subrequest directivees like [echo_subrequest_async](https://github.com/openresty/echo-nginx-module#echo_subrequest_async) and [echo_location](https://github.com/openresty/echo-nginx-module#echo_location). You are recommended to use HttpLuaModule to initiate and capture subrequests, which should work with [srcache_store](#srcache_store).


## Caveats
* It is recommended to disable your backend server's gzip compression and use nginx's [ngx_http_gzip_module](http://nginx.org/en/docs/http/ngx_http_gzip_module.html) to do the job. In case of [ngx_http_proxy_module](http://nginx.org/en/docs/http/ngx_http_proxy_module.html), you can use the following configure setting to disable backend gzip compression:
```nginx

 proxy_set_header  Accept-Encoding  "";
```
* Do *not* use [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html)'s [if](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#if) directive in the same location as this module's, because "[if](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#if) is evil". Instead, use [ngx_http_map_module](http://nginx.org/en/docs/http/ngx_http_map_module.html) or [lua-nginx-module](https://github.com/openresty/lua-nginx-module) combined with this module's [srcache_store_skip](#srcache_store_skip) and/or [srcache_fetch_skip](#srcache_fetch_skip) directives. For example:
```nginx

 map $request_method $skip_fetch {
     default     0;
     POST        1;
     PUT         1;
 }

 server {
     listen 8080;

     location /api/ {
         set $key "$uri?$args";

         srcache_fetch GET /memc $key;
         srcache_store PUT /memc $key;

         srcache_methods GET PUT POST;
         srcache_fetch_skip $skip_fetch;

         # proxy_pass/drizzle_pass/content_by_lua/echo/...
     }
 }
```


## Trouble Shooting

To debug issues, you should always check your Nginx `error.log` file first. If no error messages are printed, you need to enable the Nginx debugging logs to get more details, as explained in [debugging log](http://nginx.org/en/docs/debugging_log.html).

Several common pitfalls for beginners:

* The original response carries a `Cache-Control` header that explicitly disables caching and you do not configure directives like [srcache_response_cache_control](#srcache_response_cache_control).
* The original response is already gzip compressed, which is not cached by default (see [srcache_ignore_content_encoding](#srcache_ignore_content_encoding)).
* Memcached might return `CLIENT_ERROR bad command line format` when using a too long key (250 chars as of version 1.4.25). It is thus safer to use `set_md5 $key $uri$args;` instead of `set $key $uri$args;`. The `set_md5` directive (and more) is available from [OpenResty's set-misc module](https://github.com/openresty/set-misc-nginx-module).
* Nginx might return `client intended to send too large body` when trying to store objects larger than 1m to the storage backend, in which case nginx `client_max_body_size` must be set to a higher value.
* Memcached might fail to store objects larger than 1m, causing errors like `srcache_store subrequest failed status=502`. Since version 1.4.2, memcached supports a command-line `-I` option to override the default size of each slab page. Please read its manpage for more information.


## Test Suite
This module comes with a Perl-driven test suite. The [test cases](https://github.com/openresty/srcache-nginx-module/tree/master/test/t) are [declarative](https://github.com/openresty/srcache-nginx-module/blob/master/test/t/main-req.t) too. Thanks to the [Test::Nginx](http://search.cpan.org/perldoc?Test::Base) module in the Perl world.

To run it on your side:
```bash

 $ PATH=/path/to/your/nginx-with-srcache-module:$PATH prove -r t
```
You need to terminate any Nginx processes before running the test suite if you have changed the Nginx server binary.

Because a single nginx server (by default, `localhost:1984`) is used across all the test scripts (`.t` files), it's meaningless to run the test suite in parallel by specifying `-jN` when invoking the `prove` utility.

Some parts of the test suite requires modules [ngx_http_rewrite_module](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html), [echo-nginx-module](https://github.com/openresty/echo-nginx-module), [rds-json-nginx-module](https://github.com/openresty/rds-json-nginx-module), and [drizzle-nginx-module](https://github.com/openresty/drizzle-nginx-module) to be enabled as well when building Nginx.


## See Also
* [memc-nginx-module](https://github.com/openresty/memc-nginx-module)
* [lua-nginx-module](https://github.com/openresty/lua-nginx-module)
* [set-misc-nginx-module](https://github.com/openresty/set-misc-nginx-module)
* The [openresty bundle](http://openresty.org)


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-srcache](https://github.com/dvershinin/srcache-nginx-module){target=_blank}.

# *statsd*: NGINX module for sending stats to statsd


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-statsd
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_statsd_module.so;
```


This document describes nginx-module-statsd [v0.0.1](https://github.com/dvershinin/nginx-statsd/releases/tag/v0.0.1){target=_blank} 
released on Feb 24 2020.

<hr />

An nginx module for sending statistics to statsd.

This is how to use the nginx-statsd module:

	http {
		
		# Set the server that you want to send stats to.
		statsd_server your.statsd.server.com;

		# Randomly sample 10% of requests so that you do not overwhelm your statsd server.
		# Defaults to sending all statsd (100%). 
		statsd_sample_rate 10; # 10% of requests


		server {
			listen 80;
			server_name www.your.domain.com;
				
			# Increment "your_product.requests" by 1 whenever any request hits this server. 
			statsd_count "your_product.requests" 1;

			location / {
				
				# Increment the key by 1 when this location is hit.
				statsd_count "your_product.pages.index_requests" 1;

				# Increment the key by 1, but only if $request_completion is set to something.
				statsd_count "your_product.pages.index_responses" 1 "$request_completion";

				# Send a timing to "your_product.pages.index_response_time" equal to the value
				# returned from the upstream server. If this value evaluates to 0 or empty-string,
				# it will not be sent. Thus, there is no need to add a test.
				statsd_timing "your_product.pages.index_response_time" "$upstream_response_time";

				# Increment a key based on the value of a custom header. Only sends the value if
				# the custom header exists in the upstream response.
				statsd_count "your_product.custom_$upstream_http_x_some_custom_header" 1 
					"$upstream_http_x_some_custom_header";

				proxy_pass http://some.other.domain.com;
			}
		}
	}

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-statsd](https://github.com/dvershinin/nginx-statsd){target=_blank}.

# *sticky*: NGINX sticky cookie module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-sticky
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_sticky_module.so;
```


This document describes nginx-module-sticky [v1.3.0](https://github.com/dvershinin/nginx-sticky-module-ng/releases/tag/v1.3.0){target=_blank} 
released on Jun 27 2022.

<hr />


modified and extended version; see Changelog.txt 

## Description

A nginx module to add a sticky cookie to be always forwarded to the same
upstream server.

When dealing with several backend servers, it's sometimes useful that one
client (browser) is always served by the same backend server
(for session persistance for example).

Using a persistance by IP (with the ip_hash upstream module) is maybe not
a good idea because there could be situations where a lot of different
browsers are coming with the same IP address (behind proxies)and the load
balancing system won't be fair.

Using a cookie to track the upstream server makes each browser unique.

When the sticky module can't apply, it switchs back to the classic Round Robin
Upstream or returns a "Bad Gateway" (depending on the no_fallback flag).

Sticky module can't apply when cookies are not supported by the browser

> Sticky module is based on a "best effort" algorithm. Its aim is not to handle
> security somehow. It's been made to ensure that normal users are always
> redirected to the same  backend server: that's all!

## Usage

    upstream {
      sticky;
      server 127.0.0.1:9000;
      server 127.0.0.1:9001;
      server 127.0.0.1:9002;
    }

	  sticky [hash=index|md5|sha1] [no_fallback]
           [name=route] [domain=.foo.bar] [path=/] [expires=1h] [secure] [httponly];
       or
	  sticky [hmac=md5|sha1 hmac_key=<foobar_key>] [no_fallback]
           [name=route] [domain=.foo.bar] [path=/] [expires=1h] [secure] [httponly];
       or
	  sticky [text=raw] [no_fallback]
           [name=route] [domain=.foo.bar] [path=/] [expires=1h] [secure] [httponly];

Server selection algorithm:
- hash:    the hash mechanism to encode upstream server. It can't be used with hmac or text.
  default: md5

    - md5|sha1: well known hash
    - index:    it's not hashed, an in-memory index is used instead, it's quicker and the overhead is shorter
    Warning: the matching against upstream servers list
    is inconsistent. So, at reload, if upstreams servers
    has changed, index values are not guaranted to
    correspond to the same server as before!
    USE IT WITH CAUTION and only if you need to!

- hmac:    the HMAC hash mechanism to encode upstream server
    It's like the hash mechanism but it uses hmac_key
    to secure the hashing. It can't be used with hash or text.
    md5|sha1: well known hash

- hmac_key: the key to use with hmac. It's mandatory when hmac is set

- no_fallback: when this flag is set, nginx will return a 502 (Bad Gateway or
              Proxy Error) if a request comes with a cookie and the
              corresponding backend is unavailable. You can set it to the
              upstream block, or set "sticky_no_fallback" in a server or
              location block.

Cookie settings:
- name:    the name of the cookie used to track the persistant upstream srv;
  default: route

- domain:  the domain in which the cookie will be valid
  default: none. Let the browser handle this.

- path:    the path in which the cookie will be valid
  default: /

- expires: the validity duration of the cookie
  default: nothing. It's a session cookie.
  restriction: must be a duration greater than one second

- secure    enable secure cookies; transferred only via https
- httponly  enable cookies not to be leaked via js


## Detail Mechanism

- see docs/sticky.{vsd,pdf}	

## Issues and Warnings:

- when using different upstream-configs with stickyness that use the same domain but 
  refer to different location - configs it might be wise to set a different path / route -  
  option on each of this upstream-configs like described here:
  https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng/issue/7/leaving-cookie-path-empty-in-module

- sticky module does not work with the "backup" option of the "server" configuration item.
- sticky module might work with the nginx_http_upstream_check_module (up from version 1.2.3)
  


## Downloads

- tarballs are available via tags from the repo: https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng/downloads


## Authors & Credits

- Jerome Loyet, initial module
- Markus Linnala, httponly/secure-cookies-patch
- Peter Bowey, Nginx 1.5.8 API-Change 
- Michael Chernyak for Max-Age-Patch 
- anybody who suggested a patch, created an issue on bitbucket or helped improving this module 



## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-sticky](https://github.com/dvershinin/nginx-sticky-module-ng){target=_blank}.

# *stream-lua*: Lua scripting support for NGINX streams


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-stream-lua
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_stream_lua_module.so;
```


This document describes nginx-module-stream-lua [v0.0.13](https://github.com/openresty/stream-lua-nginx-module/releases/tag/v0.0.13){target=_blank} 
released on Mar 21 2023.

<hr />

## Name

ngx_stream_lua_module - Embed the power of Lua into Nginx stream/TCP Servers.

This module is a core component of OpenResty. If you are using this module,
then you are essentially using OpenResty.

instructions](#installation).

## Status

Production ready.

## Synopsis

```nginx
events {
    worker_connections 1024;
}

stream {
    # define a TCP server listening on the port 1234:
    server {
        listen 1234;

        content_by_lua_block {
            ngx.say("Hello, Lua!")
        }
    }
}
```

Set up as an SSL TCP server:

```nginx
stream {
    server {
        listen 4343 ssl;

        ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;
        ssl_ciphers         AES128-SHA:AES256-SHA:RC4-SHA:DES-CBC3-SHA:RC4-MD5;
        ssl_certificate     /path/to/cert.pem;
        ssl_certificate_key /path/to/cert.key;
        ssl_session_cache   shared:SSL:10m;
        ssl_session_timeout 10m;

        content_by_lua_block {
            local sock = assert(ngx.req.socket(true))
            local data = sock:receive()  -- read a line from downstream
            if data == "thunder!" then
                ngx.say("flash!")  -- output data
            else
                ngx.say("boom!")
            end
            ngx.say("the end...")
        }
    }
}
```

Listening on a UNIX domain socket is also supported:

```nginx
stream {
    server {
        listen unix:/tmp/nginx.sock;

        content_by_lua_block {
            ngx.say("What's up?")
            ngx.flush(true)  -- flush any pending output and wait
            ngx.sleep(3)  -- sleeping for 3 sec
            ngx.say("Bye bye...")
        }
    }
}
```


## Description

This is a port of the
[ngx_http_lua_module](https://github.com/openresty/lua-nginx-module#readme) to
the Nginx "stream" subsystem so as to support generic stream/TCP clients.

The available Lua APIs and Nginx directives remain the same as those of the
ngx_http_lua module.


## Directives

The following directives are ported directly from ngx_http_lua. Please check
the documentation of ngx_http_lua for more details about their usage and
behavior.

* [lua_load_resty_core](https://github.com/openresty/lua-nginx-module#lua_load_resty_core)
* [lua_code_cache](https://github.com/openresty/lua-nginx-module#lua_code_cache)
* [lua_regex_cache_max_entries](https://github.com/openresty/lua-nginx-module#lua_regex_cache_max_entries)
* [lua_package_path](https://github.com/openresty/lua-nginx-module#lua_package_path)
* [lua_package_cpath](https://github.com/openresty/lua-nginx-module#lua_package_cpath)
* [init_by_lua_block](https://github.com/openresty/lua-nginx-module#init_by_lua_block)
* [init_by_lua_file](https://github.com/openresty/lua-nginx-module#init_by_lua_file)
* [init_worker_by_lua_block](https://github.com/openresty/lua-nginx-module#init_worker_by_lua_block)
* [init_worker_by_lua_file](https://github.com/openresty/lua-nginx-module#init_worker_by_lua_file)
* [preread_by_lua_block](#preread_by_lua_block)
* [preread_by_lua_file](#preread_by_lua_file)
* [content_by_lua_block](https://github.com/openresty/lua-nginx-module#content_by_lua_block)
* [content_by_lua_file](https://github.com/openresty/lua-nginx-module#content_by_lua_file)
* [balancer_by_lua_block](https://github.com/openresty/lua-nginx-module#balancer_by_lua_block)
* [balancer_by_lua_file](https://github.com/openresty/lua-nginx-module#balancer_by_lua_file)
* [log_by_lua_block](#log_by_lua_block)
* [log_by_lua_file](#log_by_lua_file)
* [ssl_client_hello_by_lua_block](https://github.com/openresty/lua-nginx-module#ssl_client_hello_by_lua_block)
* [ssl_client_hello_by_lua_file](https://github.com/openresty/lua-nginx-module#ssl_client_hello_by_lua_file)
* [ssl_certificate_by_lua_block](https://github.com/openresty/lua-nginx-module#ssl_certificate_by_lua_block)
* [ssl_certificate_by_lua_file](https://github.com/openresty/lua-nginx-module#ssl_certificate_by_lua_file)
* [lua_shared_dict](https://github.com/openresty/lua-nginx-module#lua_shared_dict)
* [lua_socket_connect_timeout](https://github.com/openresty/lua-nginx-module#lua_socket_connect_timeout)
* [lua_socket_buffer_size](https://github.com/openresty/lua-nginx-module#lua_socket_buffer_size)
* [lua_socket_pool_size](https://github.com/openresty/lua-nginx-module#lua_socket_pool_size)
* [lua_socket_keepalive_timeout](https://github.com/openresty/lua-nginx-module#lua_socket_keepalive_timeout)
* [lua_socket_log_errors](https://github.com/openresty/lua-nginx-module#lua_socket_log_errors)
* [lua_ssl_ciphers](https://github.com/openresty/lua-nginx-module#lua_ssl_ciphers)
* [lua_ssl_crl](https://github.com/openresty/lua-nginx-module#lua_ssl_crl)
* [lua_ssl_protocols](https://github.com/openresty/lua-nginx-module#lua_ssl_protocols)
* [lua_ssl_trusted_certificate](https://github.com/openresty/lua-nginx-module#lua_ssl_trusted_certificate)
* [lua_ssl_verify_depth](https://github.com/openresty/lua-nginx-module#lua_ssl_verify_depth)
* [lua_ssl_conf_command](https://github.com/openresty/lua-nginx-module#lua_ssl_conf_command)
* [lua_check_client_abort](https://github.com/openresty/lua-nginx-module#lua_check_client_abort)
* [lua_max_pending_timers](https://github.com/openresty/lua-nginx-module#lua_max_pending_timers)
* [lua_max_running_timers](https://github.com/openresty/lua-nginx-module#lua_max_running_timers)
* [lua_sa_restart](https://github.com/openresty/lua-nginx-module#lua_sa_restart)
* [lua_add_variable](#lua_add_variable)
* [lua_capture_error_log](https://github.com/openresty/lua-nginx-module#lua_capture_error_log)
* [preread_by_lua_no_postpone](#preread_by_lua_no_postpone)

The [send_timeout](https://nginx.org/r/send_timeout) directive in the Nginx
"http" subsystem is missing in the "stream" subsystem. As such,
ngx_stream_lua_module uses the `lua_socket_send_timeout` directive for this
purpose instead.

**Note:** the lingering close directive that used to exist in older version of
`stream_lua_nginx_module` has been removed and can now be simulated with the
newly added [tcpsock:shutdown](#tcpsockshutdown) API if necessary.


## preread_by_lua_block

**syntax:** *preread_by_lua_block { lua-script }*

**context:** *stream, server*

**phase:** *preread*

Acts as a `preread` phase handler and executes Lua code string specified in `lua-script` for every connection
(or packet in datagram mode).
The Lua code may make [API calls](#nginx-api-for-lua) and is executed as a new spawned coroutine in an independent global environment (i.e. a sandbox).

It is possible to acquire the raw request socket using [ngx.req.socket](https://github.com/openresty/lua-nginx-module#ngxreqsocket)
and receive data from or send data to the client. However, keep in mind that calling the `receive()` method
of the request socket will consume the data from the buffer and such consumed data will not be seen by handlers
further down the chain.

The `preread_by_lua_block` code will always run at the end of the `preread` processing phase unless
[preread\_by\_lua\_no\_postpone](#preread_by_lua_no_postpone) is turned on.

This directive was first introduced in the `v0.0.3` release.

[Back to TOC](#directives)

## preread_by_lua_file

**syntax:** *preread_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *stream, server*

**phase:** *preread*

Equivalent to [preread_by_lua_block](#preread_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code
or LuaJIT bytecode to be executed.

Nginx variables can be used in the `<path-to-lua-script-file>` string to provide flexibility. This however carries some risks and is not ordinarily recommended.

When a relative path like `foo/bar.lua` is given, it will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option given when starting the Nginx server.

When the Lua code cache is turned on (by default), the user code is loaded once at the first connection and cached. The Nginx config must be reloaded each time the Lua source file is modified. The Lua code cache can be temporarily disabled during development by switching [lua_code_cache](#lua_code_cache) `off` in `nginx.conf` to avoid having to reload Nginx.

This directive was first introduced in the `v0.0.3` release.

[Back to TOC](#directives)

## log_by_lua_block

**syntax:** *log_by_lua_block { lua-script }*

**context:** *stream, server*

**phase:** *log*

Runs the Lua source code specified as `<lua-script>` during the `log` request processing phase. This does not replace the current access logs, but runs before.

Yielding APIs such as `ngx.req.socket`, `ngx.socket.*`, `ngx.sleep`, or `ngx.say` are **not** available in this phase.

This directive was first introduced in the `v0.0.3` release.

[Back to TOC](#directives)

## log_by_lua_file

**syntax:** *log_by_lua_file &lt;path-to-lua-script-file&gt;*

**context:** *stream, server*

**phase:** *log*

Equivalent to [log_by_lua_block](#log_by_lua_block), except that the file specified by `<path-to-lua-script-file>` contains the Lua code
or LuaJIT bytecode to be executed.

Nginx variables can be used in the `<path-to-lua-script-file>` string to provide flexibility. This however carries some risks and is not ordinarily recommended.

When a relative path like `foo/bar.lua` is given, it will be turned into the absolute path relative to the `server prefix` path determined by the `-p PATH` command-line option given when starting the Nginx server.

When the Lua code cache is turned on (by default), the user code is loaded once at the first connection and cached. The Nginx config must be reloaded each time the Lua source file is modified. The Lua code cache can be temporarily disabled during development by switching [lua_code_cache](#lua_code_cache) `off` in `nginx.conf` to avoid having to reload Nginx.

This directive was first introduced in the `v0.0.3` release.

[Back to TOC](#directives)

## lua_add_variable

**syntax:** *lua_add_variable $var*

**context:** *stream*

Add the variable `$var` to the "stream" subsystem and makes it changeable. If `$var` already exists,
this directive will do nothing.

By default, variables added using this directive are considered "not found" and reading them
using `ngx.var` will return `nil`. However, they could be re-assigned via the `ngx.var.VARIABLE` API at any time.

This directive was first introduced in the `v0.0.4` release.

[Back to TOC](#directives)

## preread_by_lua_no_postpone

**syntax:** *preread_by_lua_no_postpone on|off*

**context:** *stream*

Controls whether or not to disable postponing [preread\_by\_lua*](#preread_by_lua_block) directives
to run at the end of the `preread` processing phase. By default, this directive is turned off
and the Lua code is postponed to run at the end of the `preread` phase.

This directive was first introduced in the `v0.0.4` release.

[Back to TOC](#directives)

## Nginx API for Lua

Many Lua API functions are ported from ngx_http_lua. Check out the official
manual of ngx_http_lua for more details on these Lua API functions.

* [ngx.var.VARIABLE](https://github.com/openresty/lua-nginx-module#ngxvarvariable)

This module fully supports the new variable subsystem inside the Nginx stream core. You may access any
[built-in variables](https://nginx.org/en/docs/stream/ngx_stream_core_module.html#variables) provided by the stream core or
other stream modules.
* [Core constants](https://github.com/openresty/lua-nginx-module#core-constants)

    `ngx.OK`, `ngx.ERROR`, and etc.
* [Nginx log level constants](https://github.com/openresty/lua-nginx-module#nginx-log-level-constants)

    `ngx.ERR`, `ngx.WARN`, and etc.
* [print](https://github.com/openresty/lua-nginx-module#print)
* [ngx.ctx](https://github.com/openresty/lua-nginx-module#ngxctx)
* [ngx.balancer](https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/balancer.md)

* [ngx.req.socket](https://github.com/openresty/lua-nginx-module#ngxreqsocket)

Only raw request sockets are supported, for obvious reasons. The `raw` argument value
is ignored and the raw request socket is always returned. Unlike ngx_http_lua,
you can still call output API functions like `ngx.say`, `ngx.print`, and `ngx.flush`
after acquiring the raw request socket via this function.

When the stream server is in UDP mode, reading from the downstream socket returned by the
`ngx.req.socket` call will only return the content of a single packet. Therefore
the reading call will never block and will return `nil, "no more data"` when all the
data from the datagram has been consumed. However, you may choose to send multiple UDP
packets back to the client using the downstream socket.

The raw TCP sockets returned by this module will contain the following extra method:

[Back to TOC](#directives)

## reqsock:receiveany

**syntax:** *data, err = reqsock:receiveany(max)*

**context:** *content_by_lua&#42;, ngx.timer.&#42;, ssl_certificate_by_lua&#42;*

This method is similar to [tcpsock:receiveany](https://github.com/openresty/lua-nginx-module#tcpsockreceiveany) method

This method was introduced into `stream-lua-nginx-module` since `v0.0.8`.

[Back to TOC](#directives)

## tcpsock:shutdown

**syntax:** *ok, err = tcpsock:shutdown("send")*

**context:** *content_by_lua&#42;*

Shuts down the write part of the request socket, prevents all further writing to the client
and sends TCP FIN, while keeping the reading half open.

Currently only the `"send"` direction is supported. Using any parameters other than "send" will return
an error.

If you called any output functions (like [ngx.say](https://github.com/openresty/lua-nginx-module#ngxsay))
before calling this method, consider use `ngx.flush(true)` to make sure all busy buffers are complely
flushed before shutting down the socket. If any busy buffers were detected, this method will return `nil`
will error message `"socket busy writing"`.

This feature is particularly useful for protocols that generate a response before actually
finishing consuming all incoming data. Normally, the kernel will send RST to the client when
[tcpsock:close](https://github.com/openresty/lua-nginx-module#tcpsockclose) is called without
emptying the receiving buffer first. Calling this method will allow you to keep reading from
the receiving buffer and prevents RST from being sent.

You can also use this method to simulate lingering close similar to that
[provided by the ngx_http_core_module](https://nginx.org/en/docs/http/ngx_http_core_module.html#lingering_close)
for protocols in need of such behavior. Here is an example:

```lua
local LINGERING_TIME = 30 -- 30 seconds
local LINGERING_TIMEOUT = 5000 -- 5 seconds

local ok, err = sock:shutdown("send")
if not ok then
    ngx.log(ngx.ERR, "failed to shutdown: ", err)
    return
end

local deadline = ngx.time() + LINGERING_TIME

sock:settimeouts(nil, nil, LINGERING_TIMEOUT)

repeat
    local data, _, partial = sock:receive(1024)
until (not data and not partial) or ngx.time() >= deadline
```

[Back to TOC](#directives)

## reqsock:peek

**syntax:** *ok, err = reqsock:peek(size)*

**context:** *preread_by_lua&#42;*

Peeks into the [preread](https://nginx.org/en/docs/stream/stream_processing.html#preread_phase)
buffer that contains downstream data sent by the client without consuming them.
That is, data returned by this API will still be forwarded upstream in later phases.

This function takes a single required argument, `size`, which is the number of bytes to be peeked.
Repeated calls to this function always returns data from the beginning of the preread buffer.

Note that preread phase happens after the TLS handshake. If the stream server was configured with
TLS enabled, the returned data will be in clear text.

If preread buffer does not have the requested amount of data, then the current Lua thread will
be yielded until more data is available, [`preread_buffer_size`](https://nginx.org/en/docs/stream/ngx_stream_core_module.html#preread_buffer_size)
has been exceeded, or [`preread_timeout`](https://nginx.org/en/docs/stream/ngx_stream_core_module.html#preread_timeout)
has elapsed. Successful calls always return the requested amounts of data, that is, no partial
data will be returned.

When [`preread_buffer_size`](https://nginx.org/en/docs/stream/ngx_stream_core_module.html#preread_buffer_size)
has been exceeded, the current stream session will be terminated with the
[session status code](https://nginx.org/en/docs/stream/ngx_stream_core_module.html#var_status) `400`
immediately by the stream core module, with error message `"preread buffer full"` that will be printed to the error log.

When [`preread_timeout`](https://nginx.org/en/docs/stream/ngx_stream_core_module.html#preread_timeout) has been exceeded,
the current stream session will be terminated with the
[session status code](https://nginx.org/en/docs/stream/ngx_stream_core_module.html#var_status) `200` immediately by the stream core module.

In both cases, no further processing on the session is possible (except `log_by_lua*`). The connection will be closed by the
stream core module automatically.

Note that this API cannot be used if consumption of client data has occurred. For example, after calling
`reqsock:receive`. If such an attempt was made, the Lua error `"attempt to peek on a consumed socket"` will
be thrown. Consuming client data after calling this API is allowed and safe.

Here is an example of using this API:

```lua
local sock = assert(ngx.req.socket())

local data = assert(sock:peek(1)) -- peek the first 1 byte that contains the length
data = string.byte(data)

data = assert(sock:peek(data + 1)) -- peek the length + the size byte

local payload = data:sub(2) -- trim the length byte to get actual payload

ngx.log(ngx.INFO, "payload is: ", payload)
```

This API was first introduced in the `v0.0.6` release.

[Back to TOC](#directives)

* [ngx.print](https://github.com/openresty/lua-nginx-module#ngxprint)
* [ngx.say](https://github.com/openresty/lua-nginx-module#ngxsay)
* [ngx.log](https://github.com/openresty/lua-nginx-module#ngxlog)
* [ngx.flush](https://github.com/openresty/lua-nginx-module#ngxflush)

    This call currently ignores the `wait` argument and always wait for all the pending
output to be completely flushed out (to the system socket send buffers).
* [ngx.exit](https://github.com/openresty/lua-nginx-module#ngxexit)
* [ngx.eof](https://github.com/openresty/lua-nginx-module#ngxeof)
* [ngx.sleep](https://github.com/openresty/lua-nginx-module#ngxsleep)
* [ngx.escape_uri](https://github.com/openresty/lua-nginx-module#ngxescape_uri)
* [ngx.unescape_uri](https://github.com/openresty/lua-nginx-module#ngxunescape_uri)
* [ngx.encode_args](https://github.com/openresty/lua-nginx-module#ngxencode_args)
* [ngx.decode_args](https://github.com/openresty/lua-nginx-module#ngxdecode_args)
* [ngx.encode_base64](https://github.com/openresty/lua-nginx-module#ngxencode_base64)
* [ngx.decode_base64](https://github.com/openresty/lua-nginx-module#ngxdecode_base64)
* [ngx.crc32_short](https://github.com/openresty/lua-nginx-module#ngxcrc32_short)
* [ngx.crc32_long](https://github.com/openresty/lua-nginx-module#ngxcrc32_long)
* [ngx.hmac_sha1](https://github.com/openresty/lua-nginx-module#ngxhmac_sha1)
* [ngx.md5](https://github.com/openresty/lua-nginx-module#ngxmd5)
* [ngx.md5_bin](https://github.com/openresty/lua-nginx-module#ngxmd5_bin)
* [ngx.sha1_bin](https://github.com/openresty/lua-nginx-module#ngxsha1_bin)
* [ngx.quote_sql_str](https://github.com/openresty/lua-nginx-module#ngxquote_sql_str)
* [ngx.today](https://github.com/openresty/lua-nginx-module#ngxtoday)
* [ngx.time](https://github.com/openresty/lua-nginx-module#ngxtime)
* [ngx.now](https://github.com/openresty/lua-nginx-module#ngxnow)
* [ngx.update_time](https://github.com/openresty/lua-nginx-module#ngxupdate_time)
* [ngx.localtime](https://github.com/openresty/lua-nginx-module#ngxlocaltime)
* [ngx.utctime](https://github.com/openresty/lua-nginx-module#ngxutctime)
* [ngx.re.match](https://github.com/openresty/lua-nginx-module#ngxrematch)
* [ngx.re.find](https://github.com/openresty/lua-nginx-module#ngxrefind)
* [ngx.re.gmatch](https://github.com/openresty/lua-nginx-module#ngxregmatch)
* [ngx.re.sub](https://github.com/openresty/lua-nginx-module#ngxresub)
* [ngx.re.gsub](https://github.com/openresty/lua-nginx-module#ngxregsub)
* [ngx.shared.DICT](https://github.com/openresty/lua-nginx-module#ngxshareddict)
* [ngx.socket.tcp](https://github.com/openresty/lua-nginx-module#ngxsockettcp)
* [ngx.socket.udp](https://github.com/openresty/lua-nginx-module#ngxsocketudp)
* [ngx.socket.connect](https://github.com/openresty/lua-nginx-module#ngxsocketconnect)
* [ngx.get_phase](https://github.com/openresty/lua-nginx-module#ngxget_phase)
* [ngx.thread.spawn](https://github.com/openresty/lua-nginx-module#ngxthreadspawn)
* [ngx.thread.wait](https://github.com/openresty/lua-nginx-module#ngxthreadwait)
* [ngx.thread.kill](https://github.com/openresty/lua-nginx-module#ngxthreadkill)
* [ngx.on_abort](https://github.com/openresty/lua-nginx-module#ngxon_abort)
* [ngx.timer.at](https://github.com/openresty/lua-nginx-module#ngxtimerat)
* [ngx.timer.running_count](https://github.com/openresty/lua-nginx-module#ngxtimerrunning_count)
* [ngx.timer.pending_count](https://github.com/openresty/lua-nginx-module#ngxtimerpending_count)
* [ngx.config.debug](https://github.com/openresty/lua-nginx-module#ngxconfigdebug)
* [ngx.config.subsystem](https://github.com/openresty/lua-nginx-module#ngxconfigsubsystem)

    Always takes the Lua string value `"stream"` in this module.
* [ngx.config.prefix](https://github.com/openresty/lua-nginx-module#ngxconfigprefix)
* [ngx.config.nginx_version](https://github.com/openresty/lua-nginx-module#ngxconfignginx_version)
* [ngx.config.nginx_configure](https://github.com/openresty/lua-nginx-module#ngxconfignginx_configure)
* [ngx.config.ngx_lua_version](https://github.com/openresty/lua-nginx-module#ngxconfigngx_lua_version)
* [ngx.worker.exiting](https://github.com/openresty/lua-nginx-module#ngxworkerexiting)
* [ngx.worker.pid](https://github.com/openresty/lua-nginx-module#ngxworkerpid)
* [ngx.worker.pids](https://github.com/openresty/lua-nginx-module#ngxworkerpids)
* [ngx.worker.count](https://github.com/openresty/lua-nginx-module#ngxworkercount)
* [ngx.worker.id](https://github.com/openresty/lua-nginx-module#ngxworkerid)
* [coroutine.create](https://github.com/openresty/lua-nginx-module#coroutinecreate)
* [coroutine.resume](https://github.com/openresty/lua-nginx-module#coroutineresume)
* [coroutine.yield](https://github.com/openresty/lua-nginx-module#coroutineyield)
* [coroutine.wrap](https://github.com/openresty/lua-nginx-module#coroutinewrap)
* [coroutine.running](https://github.com/openresty/lua-nginx-module#coroutinerunning)
* [coroutine.status](https://github.com/openresty/lua-nginx-module#coroutinestatus)


## Nginx Compatibility

The latest version of this module is compatible with the following versions of Nginx:

* 1.19.x (last tested: 1.19.3)
* 1.17.x (last tested: 1.17.8)
* 1.15.x (last tested: 1.15.8)
* 1.13.x (last tested: 1.13.6)

Nginx cores older than 1.13.6 (exclusive) are *not* tested and may or may not
work. Use at your own risk!


## tell nginx's build system where to find LuaJIT 2.1:
export LUAJIT_LIB=/path/to/luajit/lib
export LUAJIT_INC=/path/to/luajit/include/luajit-2.1

## Here we assume Nginx is to be installed under /opt/nginx/.
./configure --prefix=/opt/nginx \
        --with-ld-opt="-Wl,-rpath,/path/to/luajit-or-lua/lib" \
        --with-stream \
        --with-stream_ssl_module \
        --add-module=/path/to/stream-lua-nginx-module

## Build and install
make -j4
make install
```

You may use `--without-http` if you do not wish to use this module with the
"http" subsystem. ngx_stream_lua will work perfectly fine without the "http"
subsystem.


## Code Repository

The code repository of this project is hosted on GitHub at
[openresty/stream-lua-nginx-module](https://github.com/openresty/stream-lua-nginx-module).


## See Also

* [ngx_http_lua_module](https://github.com/openresty/lua-nginx-module)
* [ngx_stream_echo_module](https://github.com/openresty/stream-echo-nginx-module)
* [OpenResty](https://openresty.org/)


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-stream-lua](https://github.com/openresty/stream-lua-nginx-module){target=_blank}.

# *stream-sts*: Nginx stream server traffic status core module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-stream-sts
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_stream_server_traffic_status_module.so;
```


This document describes nginx-module-stream-sts [v0.1.1](https://github.com/vozlt/nginx-module-stream-sts/releases/tag/v0.1.1){target=_blank} 
released on Jul 04 2018.

<hr />

[![License](http://img.shields.io/badge/license-BSD-brightgreen.svg)](https://github.com/vozlt/nginx-module-stream-sts/blob/master/LICENSE)

Nginx stream server traffic status core module

## Screenshots
![nginx-module-sts screenshot](https://cloud.githubusercontent.com/assets/3648408/23112117/e8c56cda-f770-11e6-9c68-f57cbf4dd542.png "screenshot with deault")

## Synopsis

```Nginx
http {
    stream_server_traffic_status_zone;

    ...

    server {

        ...

        location /status {
            stream_server_traffic_status_display;
            stream_server_traffic_status_display_format html;
        }
    }
}

stream {
    server_traffic_status_zone;

    ...

    server {
        ...
    }
}
```

## Description
This is an Nginx module that provides access to stream server traffic status information.
This is a porting version of the [nginx-module-vts](https://github.com/vozlt/nginx-module-vts) to the NGINX "stream" subsystem so as to support the same features in [nginx-module-vts](https://github.com/vozlt/nginx-module-vts).
It contains the current status such as servers, upstreams, user-defined filter.
This module is the core module of two modules([nginx-module-sts](https://github.com/vozlt/nginx-module-sts), [nginx-module-stream-sts](https://github.com/vozlt/nginx-module-stream-sts)).

The functions of each module are as follows:

* [nginx-module-stream-sts](https://github.com/vozlt/nginx-module-stream-sts)
  * Support for implementing stream server stats.
  * Support for implementing stream filter.
  * Support for implementing stream limit.
  * Support for implementing stream embedded variables.
* [nginx-module-sts](https://github.com/vozlt/nginx-module-sts)
  * Support for implementing display of stream server stats.
  * Support for implementing control of stream server stats.

## See Also
* [nginx-module-sts](https://github.com/vozlt/nginx-module-sts)
* [nginx-module-vts](https://github.com/vozlt/nginx-module-vts)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-stream-sts](https://github.com/vozlt/nginx-module-stream-sts){target=_blank}.

# *stream-upsync*: NGINX module for syncing stream backends from consul or etcd


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-stream-upsync
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_stream_upsync_module.so;
```


This document describes nginx-module-stream-upsync [v1.2.2](https://github.com/xiaokai-wang/nginx-stream-upsync-module/releases/tag/v1.2.2){target=_blank} 
released on Jan 02 2020.

<hr />

nginx-stream-upsync-module - Nginx C module, sync upstreams from consul or others, dynamically modify backend-servers attribute(weight, max_fails,...), needn't reload nginx.

It may not always be convenient to modify configuration files and restart NGINX. For example, if you are experiencing large amounts of traffic and high load, restarting NGINX and reloading the configuration at that point further increases load on the system and can temporarily degrade performance.

The module can be more smoothly expansion and constriction, and will not influence the performance.

Another module, [nginx-upsync-module](https://github.com/weibocom/nginx-upsync-module) supports nginx http module(HTTP protocol), please be noticed.

If you want to use [nginx-upsync-module](https://github.com/weibocom/nginx-upsync-module) and [nginx-stream-upsync-module](https://github.com/xiaokai-wang/nginx-stream-upsync-module) both, please refer to [nginx-upsync](https://github.com/CallMeFoxie/nginx-upsync).

## Status

This module is still under active development and is considered production ready.

## Synopsis

nginx-consul:
```nginx-consul
stream {
    upstream test {
        upsync 127.0.0.1:8500/v1/kv/upstreams/test/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
        upsync_dump_path /usr/local/nginx/conf/servers/servers_test.conf;

        include /usr/local/nginx/conf/servers/servers_test.conf;
    }

    upstream bar {
        server 127.0.0.1:8090 weight=1 fail_timeout=10 max_fails=3;
    }

    server {
        listen 12345;

        proxy_connect_timeout 1s;
        proxy_timeout 3s;
        proxy_pass test;
    }

    server {
        listen 2345;

        upstream_show
    }

    server {
        listen 127.0.0.1:9091;

        proxy_responses 1;
        proxy_timeout 20s;
        proxy_pass bar;
    }
}
```
nginx-etcd:
```nginx-etcd
stream {
    upstream test {
        upsync 127.0.0.1:2379/v2/keys/upstreams/test upsync_timeout=6m upsync_interval=500ms upsync_type=etcd strong_dependency=off;
        upsync_dump_path /usr/local/nginx/conf/servers/servers_test.conf;

        include /usr/local/nginx/conf/servers/servers_test.conf;
    }

    upstream bar {
        server 127.0.0.1:8090 weight=1 fail_timeout=10 max_fails=3;
    }

    server {
        listen 12345;

        proxy_connect_timeout 1s;
        proxy_timeout 3s;
        proxy_pass test;
    }

    server {
        listen 2345;

        upstream_show
    }

    server {
        listen 127.0.0.1:9091;

        proxy_responses 1;
        proxy_timeout 20s;
        proxy_pass bar;
    }
}
```
upsync_lb:
```upsync_lb
stream {
    upstream test {
        least_conn; //hash $uri consistent;

        upsync 127.0.0.1:8500/v1/kv/upstreams/test/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
        upsync_dump_path /usr/local/nginx/conf/servers/servers_test.conf;
        upsync_lb least_conn; //hash_ketama;

        include /usr/local/nginx/conf/servers/servers_test.conf;
    }

    upstream bar {
        server 127.0.0.1:8090 weight=1 fail_timeout=10 max_fails=3;
    }

    server {
        listen 12345;

        proxy_connect_timeout 1s;
        proxy_timeout 3s;
        proxy_pass test;
    }

    server {
        listen 2345;

        upstream_show
    }

    server {
        listen 127.0.0.1:9091;

        proxy_responses 1;
        proxy_timeout 20s;
        proxy_pass bar;
    }
}
```

NOTE: upstream: include command is neccesary, first time the dumped file should include all the servers.


## Description

This module provides a method to discover backend servers. Supporting dynamicly adding or deleting backend server through consul/etcd and dynamicly adjusting backend servers weight, module will timely pull new backend server list from consul/etcd to upsync nginx ip router. Nginx needn't reload. Having some advantages than others:

* timely

      module send key to consul/etcd with index, consul/etcd will compare it with its index, if index doesn't change connection will hang five minutes, in the period any operation to the key-value, will feed back rightaway.

* performance

      Pulling from consul/etcd equal a request to nginx, updating ip router nginx needn't reload, so affecting nginx performance is little.

* stability

      Even if one pulling failed, it will pull next upsync_interval, so guaranteing backend server stably provides service. And support dumping the latest config to location, so even if consul/etcd hung up, and nginx can be reload anytime. 


## Directives

## upsync
```
syntax: upsync $consul/etcd.api.com:$port/v1/kv/upstreams/$upstream_name/ [upsync_type=consul/etcd] [upsync_interval=second/minutes] [upsync_timeout=second/minutes] [strong_dependency=off/on]
```
default: none, if parameters omitted, default parameters are upsync_interval=5s upsync_timeout=6m strong_dependency=off

context: upstream

description: Pull upstream servers from consul/etcd... .

The parameters' meanings are:

* upsync_interval

    pulling servers from consul/etcd interval time.

* upsync_timeout

    pulling servers from consul/etcd request timeout.

* upsync_type

    pulling servers from conf server type.

* strong_dependency

    when nginx start up if strong_dependency is on that means servers will be depended on consul/etcd and will pull servers from consul/etcd.


## upsync_dump_path
`syntax: upsync_dump_path $path`

default: /tmp/servers_$host.conf

context: upstream

description: dump the upstream backends to the $path.


## upsync_lb
`syntax: upsync_lb $load_balance`

default: round_robin/ip_hash/hash modula

context: upstream

description: mainly for least_conn and hash consistent, when using one of them, you must point out using upsync_lb.


## upsync_show
`syntax: upsync_show`

default: none

context: server

description: show all upstreams.

```request
curl http://localhost:2345/upstream_show

show all upstreams
```


## Consul_interface

Data can be taken from key/value store or service catalog. In the first case parameter upsync_type of directive must be *consul*. For example
 
```nginx-consul
    upsync 127.0.0.1:8500/v1/kv/upstreams/test upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
```

In the second case it must be *consul_services*.

```nginx-consul
    upsync 127.0.0.1:8500/v1/catalog/service/test upsync_timeout=6m upsync_interval=500ms upsync_type=consul_services strong_dependency=off;
```

you can add or delete backend server through consul_ui or http_interface. Below are examples for key/value store.

http_interface example:

* add
```
    curl -X PUT http://$consul_ip:$port/v1/kv/upstreams/$upstream_name/$backend_ip:$backend_port
```
    default: weight=1 max_fails=2 fail_timeout=10 down=0 backup=0;

```
    curl -X PUT -d "{\"weight\":1, \"max_fails\":2, \"fail_timeout\":10}" http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
or
    curl -X PUT -d '{"weight":1, "max_fails":2, "fail_timeout":10}' http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
```
    value support json format.

* delete
```
    curl -X DELETE http://$consul_ip:$port/v1/kv/upstreams/$upstream_name/$backend_ip:$backend_port
```

* adjust-weight
```
    curl -X PUT -d "{\"weight\":2, \"max_fails\":2, \"fail_timeout\":10}" http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
or
    curl -X PUT -d '{"weight":2, "max_fails":2, "fail_timeout":10}' http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
```

* mark server-down
```
    curl -X PUT -d "{\"weight\":2, \"max_fails\":2, \"fail_timeout\":10, \"down\":1}" http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
or
    curl -X PUT -d '{"weight":2, "max_fails":2, "fail_timeout":10, "down":1}' http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
```

* check
```
    curl http://$consul_ip:$port/v1/kv/upstreams/$upstream_name?recurse
```


## Etcd_interface

you can add or delete backend server through http_interface.

mainly like etcd, http_interface example:

* add
```
    curl -X PUT http://$etcd_ip:$port/v2/keys/upstreams/$upstream_name/$backend_ip:$backend_port
```
    default: weight=1 max_fails=2 fail_timeout=10 down=0 backup=0;

```
    curl -X PUT -d value="{\"weight\":1, \"max_fails\":2, \"fail_timeout\":10}" http://$etcd_ip:$port/v2/keys/$dir1/$upstream_name/$backend_ip:$backend_port
```
    value support json format.

* delete
```
    curl -X DELETE http://$etcd_ip:$port/v2/keys/upstreams/$upstream_name/$backend_ip:$backend_port
```

* adjust-weight
```
    curl -X PUT -d "{\"weight\":2, \"max_fails\":2, \"fail_timeout\":10}" http://$etcd_ip:$port/v2/keys/$dir1/$upstream_name/$backend_ip:$backend_port
```

* mark server-down
```
    curl -X PUT -d value="{\"weight\":2, \"max_fails\":2, \"fail_timeout\":10, \"down\":1}" http://$etcd_ip:$port/v2/keys/$dir1/$upstream_name/$backend_ip:$backend_port
```

* check
```
    curl http://$etcd_ip:$port/v2/keys/upstreams/$upstream_name
```


## Code style

Code style is mainly based on [style](http://tengine.taobao.org/book/appendix_a.html)


## see also
* the nginx_upstream_check_module: https://github.com/alibaba/tengine/blob/master/src/http/ngx_http_upstream_check_module.c
* the nginx_upstream_check_module patch: https://github.com/yaoweibin/nginx_upstream_check_module
* or based on https://github.com/xiaokai-wang/nginx_upstream_check_module


## source dependency
* Cjson: https://github.com/kbranigan/cJSON
* http-parser: https://github.com/nodejs/http-parser


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-stream-upsync](https://github.com/xiaokai-wang/nginx-stream-upsync-module){target=_blank}.

# *sts*: Nginx stream server traffic status module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-sts
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_stream_server_traffic_status_module.so;
```


This document describes nginx-module-sts [v0.1.1](https://github.com/vozlt/nginx-module-sts/releases/tag/v0.1.1){target=_blank} 
released on Jul 04 2018.

<hr />

[![License](http://img.shields.io/badge/license-BSD-brightgreen.svg)](https://github.com/vozlt/nginx-module-sts/blob/master/LICENSE)

Nginx stream server traffic status module

## Screenshots
![nginx-module-sts screenshot](https://user-images.githubusercontent.com/3648408/41891509-be1e373e-794f-11e8-85bd-22ea6ca29d4a.png "screenshot with deault")

## Synopsis

```Nginx
http {
    stream_server_traffic_status_zone;

    ...

    server {

        ...

        location /status {
            stream_server_traffic_status_display;
            stream_server_traffic_status_display_format html;
        }
    }
}

stream {
    server_traffic_status_zone;

    ...

    server {
        ...
    }
}
```

## Description
This is an Nginx module that provides access to stream server traffic status information.
This is a porting version of the [nginx-module-vts](https://github.com/vozlt/nginx-module-vts) to the NGINX "stream" subsystem so as to support the same features in [nginx-module-vts](https://github.com/vozlt/nginx-module-vts).
It contains the current status such as servers, upstreams, user-defined filter.


First of all, It is required both the directive `server_traffic_status_zone` in stream block and `stream_server_traffic_status_zone` in http block, and then if the directive `stream_server_traffic_status_display` is set, can be access to as follows:

* /status/format/json
  * If you request `/status/format/json`, will respond with a JSON document containing the current activity data for using in live dashboards and third-party monitoring tools.
* /status/format/html
  * If you request `/status/format/html`, will respond with the built-in live dashboard in HTML that requests internally to `/status/format/json`.
* /status/format/jsonp
  * If you request `/status/format/jsonp`, will respond with a JSONP callback function containing the current activity data for using in live dashboards and third-party monitoring tools. 
* /status/format/prometheus                                                                                                                                                                         
  * If you request `/status/format/prometheus`, will respond with a [prometheus](https://prometheus.io) document containing the current activity data.   
* /status/control
  * If you request `/status/control`, will respond with a JSON document after it reset or delete zones through a query string. See the [Control](#control).

JSON document contains as follows:

```Json
{
    "hostName": ...,
    "nginxVersion": ...,
    "loadMsec": ...,
    "nowMsec": ...,
    "connections": {
        "active":...,
        "reading":...,
        "writing":...,
        "waiting":...,
        "accepted":...,
        "handled":...,
        "requests":...
    },
    "sharedZones": {
        "name":...,
        "maxSize":...,
        "usedSize":...,
        "usedNode":...
    },
    "streamServerZones": {
        "...":{
            "port":...,
            "protocol":...,
            "connectCounter":...,
            "inBytes":...,
            "outBytes":...,
            "responses":{
                "1xx":...,
                "2xx":...,
                "3xx":...,
                "4xx":...,
                "5xx":...,
            },
            "sessionMsecCounter":...,
            "sessionMsec":...,
            "sessionMsecs":{
                "times":[...],
                "msecs":[...]
            },
            "sessionBuckets":{
                "msecs":[...],
                "counters":[...]
            }
        }
        ...
    },
    "streamFilterZones": {
        "...":{
            "...":{

                "port":...,
                "protocol":...,
                "connectCounter":...,
                "inBytes":...,
                "outBytes":...,
                "responses":{
                    "1xx":...,
                    "2xx":...,
                    "3xx":...,
                    "4xx":...,
                    "5xx":...,
                },
                "sessionMsecCounter":...,
                "sessionMsec":...,
                "sessionMsecs":{
                    "times":[...],
                    "msecs":[...]
                },
                "sessionBuckets":{
                    "msecs":[...],
                    "counters":[...]
                }
            },
            ...
        },
        ...
    },
    "streamUpstreamZones": {
        "...":[
            {
                "server":...,
                "connectCounter":...,
                "inBytes":...,
                "outBytes":...,
                "responses":{
                    "1xx":...,
                    "2xx":...,
                    "3xx":...,
                    "4xx":...,
                    "5xx":...
                },
                "sessionMsecCounter":...,
                "sessionMsec":...,
                "sessionMsecs":{
                    "times":[...],
                    "msecs":[...]
                },
                "sessionBuckets":{
                    "msecs":[...]
                    "counters":[...]
                },
                "uSessionMsecCounter":...,
                "uSessionMsec":...,
                "uSessionMsecs":{
                    "times":[...],
                    "msecs":[...]
                },
                "uSessionBuckets":{
                    "msecs":[...]
                    "counters":[...]
                },
                "uConnectMsecCounter":...,
                "uConnectMsec":...,
                "uConnectMsecs":{
                    "times":[...],
                    "msecs":[...]
                },
                "uConnectBuckets":{
                    "msecs":[...]
                    "counters":[...]
                },
                "uFirstByteMsecCounter":...,
                "uFirstByteMsec":...,
                "uFirstByteMsecs":{
                    "times":[...],
                    "msecs":[...]
                },
                "uFirstByteBuckets":{
                    "msecs":[...]
                    "counters":[...]
                },
                "weight":...,
                "maxFails":...,
                "failTimeout":...,
                "backup":...,
                "down":...
            }
            ...
        ],
        ...
    }
}
```

* main
  * Basic version, uptime((nowMsec - loadMsec)/1000)
  * nowMsec, loadMsec is a millisecond.
* connections
  * Total connections and requests(same as stub_status_module in NGINX)
* streamServerZones
  * Traffic(in/out) and request and response counts and status(1xx,2xx...) hit ratio per each server zone
  * Total traffic(In/Out) and request and response counts(It zone name is `*`) and hit ratio
* streamFilterZones
  * Traffic(in/out) and request and response counts and status(1xx,2xx...) hit ratio per each server zone filtered through the `server_traffic_status_filter_by_set_key` directive
  * Total traffic(In/Out) and request and response counts(It zone name is `*`) and hit ratio filtered through the `server_traffic_status_filter_by_set_key` directive
* streamUpstreamZones
  * Traffic(in/out) and request and response counts per server in each upstream group
  * Current settings(weight, maxfails, failtimeout...) in nginx.conf

The directive `stream_server_traffic_status_display_format` sets the default ouput format that is one of json,jsonp,html,prometheus. (Default: json)

Traffic calculation as follows:

* streamServerZones
  * in += requested_bytes
  * out += sent_bytes
* streamFilterZones
  * in += requested_bytes via the filter
  * out += sent_bytes via the filter
* streamUpstreamZones
  * in += requested_bytes via the ServerZones
  * out += sent_bytes via the ServerZones

All calculations are working in log processing phase of Nginx.

`Caveats:` this module relies on nginx logging system(NGX_STREAM_LOG_PHASE:last phase of the nginx stream), so the traffic may be
in certain cirumstances different that real bandwidth traffic.
Websocket, canceled downloads may be cause of inaccuracies.
The working of the module doesn't matter at all whether the access_log directive "on" or "off".
Again, this module works well on "access_log off".

## Control
It is able to reset or delete traffic zones through a query string.
The request responds with a JSON document.

* URI Syntax
  * /*`{status_uri}`*/control?cmd=*`{command}`*&group=*`{group}`*&zone=*`{name}`*

```Nginx
http {

    stream_server_traffic_status_zone;

    ...

    server {

        server_name example.org;

        ...


        location /status {
            stream_server_traffic_status_display;
            stream_server_traffic_status_display_format html;
        }
    }                                                                                                                                                                                           }
}

stream {
    geoip_country    /usr/share/GeoIP/GeoIP.dat;

    server_traffic_status_zone;

    server_traffic_status_filter_by_set_key $geoip_country_code country::*;

    server {

        ...

    }

    ...

}
```

If it set as above, then the control uri is like `example.org/status/control`.

The available request arguments are as follows:
* **cmd**=\<`status`\|`reset`\|`delete`\>
  * status
    * It returns status of traffic zones to json format like `status/format/json`.
  * reset
    * It reset traffic zones without deleting nodes in shared memory.(= init to 0)
  * delete
    * It delete traffic zones in shared memory. when re-request recreated. 
* **group**=\<`server`\|`filter`\|`upstream@alone`\|`upstream@group`\|`*`\>
  * server
  * filter
  * upstream@alone
  * upstream@group
  * \*
* **zone**=*name*
  * server
    * *name*
  * filter
    * *filter_group*@*name*
  * upstream@group
    * *upstream_group*@*name*
  * upstream@alone
    * @*name*

### To get status of traffic zones on the fly
This is similar to the `status/format/json` except that it can get each zones.

#### To get fully zones
* It is exactly the same with the `status/format/json`.
  * /status/control?cmd=status&group=*

#### To get group zones
* streamServerZones
  * /status/control?cmd=status&group=server&zone=*
* streamFilterZones
  * /status/control?cmd=status&group=filter&zone=*
* streamUpstreamZones
  * /status/control?cmd=status&group=upstream@group&zone=*
* streamUpstreamZones::nogroups
  * /status/control?cmd=status&group=upstream@alone&zone=*

#### To get each zones
* single zone in streamServerZones
  * /status/control?cmd=status&group=server&zone=*`name`*
* single zone in streamFilterZones
  * /status/control?cmd=status&group=filter&zone=*`filter_group`*@*`name`*
* single zone in streamUpstreamZones
  * /status/control?cmd=status&group=upstream@group&zone=*`upstream_group`*@*`name`*
* single zone in streamUpstreamZones::nogroups
  * /status/control?cmd=status&group=upstream@alone&zone=*`name`*

### To reset traffic zones on the fly
It reset the values of specified zones to 0.

#### To reset fully zones
* /status/control?cmd=reset&group=*

#### To reset group zones
* streamServerZones
  * /status/control?cmd=reset&group=server&zone=*
* streamFilterZones
  * /status/control?cmd=reset&group=filter&zone=*
* streamUpstreamZones
  * /status/control?cmd=reset&group=upstream@group&zone=*
* streamUpstreamZones::nogroups
  * /status/control?cmd=reset&group=upstream@alone&zone=*

#### To reset each zones
* single zone in streamServerZones
  * /status/control?cmd=reset&group=server&zone=*`name`*
* single zone in streamFilterZones
  * /status/control?cmd=reset&group=filter&zone=*`filter_group`*@*`name`*
* single zone in streamUpstreamZones
  * /status/control?cmd=reset&group=upstream@group&zone=*`upstream_group`*@*`name`*
* single zone in streamUpstreamZones::nogroups
  * /status/control?cmd=reset&group=upstream@alone&zone=*`name`*

### To delete traffic zones on the fly
It delete the specified zones in shared memory.

#### To delete fully zones
* /status/control?cmd=delete&group=*

#### To delete group zones
* streamServerZones
  * /status/control?cmd=delete&group=server&zone=*
* streamFilterZones
  * /status/control?cmd=delete&group=filter&zone=*
* streamUpstreamZones
  * /status/control?cmd=delete&group=upstream@group&zone=*
* streamUpstreamZones::nogroups
  * /status/control?cmd=delete&group=upstream@alone&zone=*

#### To delete each zones
* single zone in streamServerZones
  * /status/control?cmd=delete&group=server&zone=*`name`*
* single zone in streamFilterZones
  * /status/control?cmd=delete&group=filter&zone=*`filter_group`*@*`name`*
* single zone in streamUpstreamZones
  * /status/control?cmd=delete&group=upstream@group&zone=*`upstream_group`*@*`name`*
* single zone in streamUpstreamZones::nogroups
  * /status/control?cmd=delete&group=upstream@alone&zone=*`name`*

## JSON
The following status information is provided in the JSON format:

### Json used by status
/*`{status_uri}`*/format/json

/*`{status_uri}`*/control?cmd=status&...

* hostName
  * Host name.
* nginxVersion
  * Version of the provided.
* loadMsec
  * Loaded process time in milliseconds.
* nowMsec
  * Current time in milliseconds
* connections
  * active
    * The current number of active client connections.
  * reading
    * The total number of reading client connections.
  * writing
    * The total number of writing client connections.
  * waiting
    * The total number of wating client connections.
  * accepted
    * The total number of accepted client connections.
  * handled
    * The total number of handled client connections.
  * requests
    * The total number of requested client connections.
* sharedZones
  * name
    * The name of shared memory specified in the configuration.(default: `stream_server_traffic_status`)
  * maxSize
    * The limit on the maximum size of the shared memory specified in the configuration.
  * usedSize
    * The current size of the shared memory.
  * usedNode
    * The current number of node using in shared memory. It can get an approximate size for one node with the following formula: (*usedSize* / *usedNode*)
* streamServerZones
  * connectCounter
    * The total number of client requests received from clients.
  * inBytes
    * The total number of bytes received from clients.
  * outBytes
    * The total number of bytes sent to clients.
  * responses
    * 1xx, 2xx, 3xx, 4xx, 5xx
      * The number of responses with status codes 1xx, 2xx, 3xx, 4xx, and 5xx.
  * sessionMsecCounter
    * The number of accumulated request processing time in milliseconds.
  * sessionMsec
    * The average of request processing times in milliseconds.
  * sessionMsecs
    * times
      * The times in milliseconds at request processing times.
    * msecs
      * The request processing times in milliseconds.
  * sessionBuckets
    * msecs
      * The bucket values of histogram set by `server_traffic_status_histogram_buckets` directive.
    * counters
      * The cumulative values for the reason that each bucket value is greater than or equal to the request processing time.
* streamFilterZones
  * It provides the same fields with `streamServerZones` except that it included group names.
* streamUpstreamZones
  * server
    * An address of the server.
  * connectCounter
    * The total number of client connections forwarded to this server.
  * inBytes
    * The total number of bytes received from this server.
  * outBytes
    * The total number of bytes sent to this server.
  * responses
    * 1xx, 2xx, 3xx, 4xx, 5xx
      * The number of responses with status codes 1xx, 2xx, 3xx, 4xx, and 5xx.
  * sessionMsecCounter
    * The number of accumulated request processing times in milliseconds including upstream.
  * sessionMsec
    * The average of request processing times in millseconds including upstream.
  * sessionMsecs
    * times
      * The times in milliseconds at request processing times.
    * msecs
      * The request processing times in milliseconds including upstream.
  * sessionBuckets
    * msecs
      * The bucket values of histogram set by `server_traffic_status_histogram_buckets` directive.
    * counters
      * The cumulative values for the reason that each bucket value is greater than or equal to the request processing time.
  * uSessionMsecCounter
    * The number of accumulated the session duration time in milliseconds to the upstream server.
  * uSessionMsec
    * The average of the session duration times in milliseconds to the upstream server.
  * uSessionMsecs
    * times
      * The times in milliseconds at request processing times.
    * msecs
      * The session duration times in milliseconds to the upstream server.
  * uSessionBuckets
    * msecs
      * The bucket values of histogram set by `server_traffic_status_histogram_buckets` directive.
    * counters
      * The cumulative values for the reason that each bucket value is greater than or equal to the session duration time to the upstream server.
  * uConnectMsecCounter
    * The number of accumulated the time to connect to the upstream server.
  * uConnectMsec
    * The average of the times in milliseconds to connect to the upstream server (1.11.4).
  * uConnectMsecs
    * times
      * The times in milliseconds at request processing times.
    * msecs
      * The times in milliseconds to connect to the upstream server.
  * uConnectBuckets
    * msecs
      * The bucket values of histogram set by `server_traffic_status_histogram_buckets` directive.
    * counters
      * The cumulative values for the reason that each bucket value is greater than or equal to the time to connect to the upstream server.
  * uFirstByteMsecCounter
    * The number of accumulated the times in milliseconds to receive the first byte of data.
  * uFirstByteMsec
    * The average of the times in milliseconds to receive the first byte of data (1.11.4).
  * uFirstByteMsecs
    * times
      * The times in milliseconds at request processing times.
    * msecs
      * The times in milliseconds to receive the first byte of data (1.11.4).
  * uFirstByteBuckets
    * msecs
      * The bucket values of histogram set by `server_traffic_status_histogram_buckets` directive.
    * counters
      * The cumulative values for the reason that each bucket value is greater than or equal to the time to receive the first byte of data.
  * weight
    * Current `weight` setting of the server.
  * maxFails
    * Current `max_fails` setting of the server.
  * failTimeout
    * Current `fail_timeout` setting of the server.
  * backup
    * Current `backup` setting of the server.
  * down
    * Current `down` setting of the server.

### Json used by control
/*`{status_uri}`*/control?cmd=reset&...

/*`{status_uri}`*/control?cmd=delete&...

* processingReturn
  * The result of true or false.
* processingCommandString
  * The requested command string.
* processingGroupString
  * The requested group string.
* processingZoneString
  * The requested zone string.
* processingCounts
  * The actual processing number.

## Variables
The following embedded variables are provided in `stream` block:

* **$sts_connect_counter**
  * The total number of client requests received from clients.
* **$sts_in_bytes**
  * The total number of bytes received from clients.
* **$sts_out_bytes**
  * The total number of bytes sent to clients.
* **$sts_1xx_counter**
  * The number of responses with status codes 1xx.
* **$sts_2xx_counter**
  * The number of responses with status codes 2xx.
* **$sts_3xx_counter**
  * The number of responses with status codes 3xx.
* **$sts_4xx_counter**
  * The number of responses with status codes 4xx.
* **$sts_5xx_counter**
  * The number of responses with status codes 5xx.
* **$sts_session_time**
  * The average of request processing times.

## Limit

It is able to limit total traffic per each server by using the directive
[`server_traffic_status_limit_traffic`](#server_traffic_status_limit_traffic).
It also is able to limit all traffic by using the directive
[`server_traffic_status_limit_traffic_by_set_key`](#server_traffic_status_limit_traffic_by_set_key).
When the limit is exceeded, the server will return the 503
(Service Temporarily Unavailable) error in reply to a request. 
The return code can be changeable.

### To limit traffic for server
```Nginx
stream {

    server_traffic_status_zone;

    ...

    server {

        listen 1981;

        server_traffic_status_limit_traffic in:64G;
        server_traffic_status_limit_traffic out:1024G;

        ...
    }
}
```

* Limit in/out total traffic on the `1981/tcp` to 64G and 1024G respectively.

### To limit traffic for filter
```Nginx
stream {
    geoip_country /usr/share/GeoIP/GeoIP.dat;

    server_traffic_status_zone;

    ...

    server {

        listen 1981;

        server_traffic_status_filter_by_set_key $geoip_country_code country::$server_addr;
        server_traffic_status_limit_traffic_by_set_key FG@country::$server_addr@US out:1024G;
        server_traffic_status_limit_traffic_by_set_key FG@country::$server_addr@CN out:2048G;

        ...

    }
}

```

* Limit total traffic of going into US and CN on the `example.org` to 1024G and 2048G respectively.

### To limit traffic for upstream
```Nginx
stream {

    server_traffic_status_zone;

    ...

    upstream backend {
        server 10.10.10.17:80;
        server 10.10.10.18:80;
    }

    server {

        listen 1981;

        server_traffic_status_limit_traffic_by_set_key UG@backend@10.10.10.17:80 in:512G;
        server_traffic_status_limit_traffic_by_set_key UG@backend@10.10.10.18:80 in:1024G;
        proxy_pass backend;

        ...

    }
}

```

* Limit total traffic of going into upstream backend on the `1981/tcp` to 512G and 1024G per each peer.

`Caveats:` Traffic is the cumulative transfer or counter, not a bandwidth.

## Use cases

It is able to calculate the user defined individual stats by using the directive `server_traffic_status_filter_by_set_key`.

### To calculate traffic for individual country using GeoIP
```Nginx
stream {
    geoip_country /usr/share/GeoIP/GeoIP.dat;

    server_traffic_status_zone;
    server_traffic_status_filter_by_set_key $geoip_country_code country::*;

    ...

    server {

        ...

        server_traffic_status_filter_by_set_key $geoip_country_code country::$server_addr:$server_port;

    }
}
```

* Calculate traffic for individual country of total server groups.
* Calculate traffic for individual country of each server groups.

Basically, country flags image is built-in in HTML.
The country flags image is enabled if the `country` string is included
in group name which is second argument of `server_traffic_status_filter_by_set_key` directive.


## Customizing
### To customize after the module installed
1. You need to change the `{{uri}}` string to your status uri in status.template.html as follows:
 ```
 shell> vi share/status.template.html
 ```
 ```
 var vtsStatusURI = "yourStatusUri/format/json", vtsUpdateInterval = 1000;
 ```

2. And then, customizing and copy status.template.html to server root directory as follows:
 ```
 shell> cp share/status.template.html /usr/share/nginx/html/status.html
 ```

4. Configure `nginx.conf`
 ```Nginx
    server {
        server_name example.org;
        root /usr/share/nginx/html;

        # Redirect requests for / to /status.html
        location = / {
            return 301 /status.html;
        }

        location = /status.html {}

        # Everything beginning /status (except for /status.html) is
        # processed by the status handler
        location /status {
            stream_server_traffic_status_display;
            stream_server_traffic_status_display_format json;
        }
    }

 ```

4. Access to your html.
 ```
 http://example.org/status.html
 ```

### To customize before the module installed
1. Modify `share/status.template.html` (Do not change `{{uri}}` string)

2. Recreate the `ngx_http_stream_server_traffic_status_module_html.h` as follows:
 ```
 shell> cd util
 shell> ./tplToDefine.sh ../share/status.template.html > ../src/ngx_http_stream_server_traffic_status_module_html.h
 ```

3. Add the module to the build configuration by adding
  ```
  --add-module=/path/to/nginx-module-sts
  --add-module=/path/to/nginx-module-stream-sts
  ```

4. Build the nginx binary.

5. Install the nginx binary.


## Directives

### stream_server_traffic_status

| -   | - |
| --- | --- |
| **Syntax**  | **stream_server_traffic_status** \<on\|off\> |
| **Default** | off |
| **Context** | http, server, location |

`Description:` Enables or disables the module working.
If you set `stream_server_traffic_status_zone` directive, is automatically enabled.

### stream_server_traffic_status_zone

| -   | - |
| --- | --- |
| **Syntax**  | **stream_server_traffic_status_zone** [shared:*name*] |
| **Default** | shared:stream_server_traffic_status |
| **Context** | http |

`Description:` Sets parameters for a shared memory zone specified by `server_traffic_status_zone`
directive in stream block.
`Caveats:` The `name` must be same as specified by `server_traffic_status_zone`.

### stream_server_traffic_status_display

| -   | - |
| --- | --- |
| **Syntax**  | **stream_server_traffic_status_display** |
| **Default** | - |
| **Context** | http, server, location |

`Description:` Enables or disables the module display handler.

### stream_server_traffic_status_display_format

| -   | - |
| --- | --- |
| **Syntax**  | **stream_server_traffic_status_display_format** \<json\|html\|jsonp\|prometheus\> |
| **Default** | json |
| **Context** | http, server, location |

`Description:` Sets the display handler's output format.
If you set `json`, will respond with a JSON document.
If you set `html`, will respond with the built-in live dashboard in HTML.
If you set `jsonp`, will respond with a JSONP callback function(default: *ngx_http_stream_server_traffic_status_jsonp_callback*).
If you set `prometheus`, will respond with a [prometheus](https://prometheus.io) document.

### stream_server_traffic_status_display_jsonp

| -   | - |
| --- | --- |
| **Syntax**  | **stream_server_traffic_status_display_jsonp** *callback* |
| **Default** | ngx_http_stream_server_traffic_status_jsonp_callback |
| **Context** | http, server, location |

`Description:` Sets the callback name for the JSONP.

### stream_server_traffic_status_average_method

| -   | - |
| --- | --- |
| **Syntax**  | **stream_server_traffic_status_average_method** \<AMM\|WMA\> [*period*] |
| **Default** | AMM 60s |
| **Context** | http, server, location |

`Description:` Sets the method which is a formula that calculate the average of response processing times.
The *period* is an effective time of the values used for the average calculation.(Default: 60s)
If *period* set to 0, effective time is ignored.
In this case, the last average value is displayed even if there is no requests and after the elapse of time.
The corresponding values are `sessionMsec`, `uSessionMsec`, `uConnectMsec`, `uFirstByteMsec` in JSON.

* **AMM**
  * The AMM is the [arithmetic mean](https://en.wikipedia.org/wiki/Arithmetic_mean).
* **WMA**
  * THE WMA is the [weighted moving average](https://en.wikipedia.org/wiki/Moving_average#Weighted_moving_average).


### server_traffic_status

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status** \<on\|off\> |
| **Default** | off |
| **Context** | stream, server |

`Description:` Enables or disables the module working.
If you set `server_traffic_status_zone` directive, is automatically enabled.


### server_traffic_status_zone

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_zone** [shared:*name:size*] |
| **Default** | shared:stream_server_traffic_status:1m |
| **Context** | stream |

`Description:` Sets parameters for a shared memory zone that will keep states for various keys.
The cache is shared between all worker processes.


### server_traffic_status_filter

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_filter** \<on\|off\> |
| **Default** | on |
| **Context** | stream, server |

`Description:` Enables or disables the filter features.

### server_traffic_status_filter_by_set_key

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_filter_by_set_key** *key* [*name*] |
| **Default** | - |
| **Context** | stream, server |

`Description:` Enables the keys by user defined variable.
The *key* is a key string to calculate traffic.
The *name* is a group string to calculate traffic.
The *key* and *name* can contain variables such as $host, $server_addr, $server_port.
The *name*'s group belongs to `streamFilterZones` if specified.
The *key*'s group belongs to `streamServerZones` if not specified second argument *name*.
The example with geoip module is as follows:

```Nginx
stream {

      ...

      server {
          listen 1981;
          server_traffic_status_filter_by_set_key $geoip_country_code country::$server_addr:$server_port;

          ...

      }
}
```

```Json
  ...
  "streamServerZones": {
  ...
  },
  "streamFilterZones": {
      "country::example.org": {
          "KR": {
              "port":...,
              "protocol":...,
              "connectCounter":...,
              "inBytes":...,
              "outBytes":...,
              "responses":{
                  "1xx":...,
                  "2xx":...,
                  "3xx":...,
                  "4xx":...,
                  "5xx":...,
              },
              "sessionMsec":...
              "sessionMsecs":{
                  "times":[...],
                  "msecs":[...]
              },
            },
          },
          "US": {
          ...
          },
          ...
      },
      ...
  },
  ...

```

### server_traffic_status_filter_check_duplicate

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_filter_check_duplicate** \<on\|off\> |
| **Default** | on |
| **Context** | stream, server |

`Description:` Enables or disables the deduplication of server_traffic_status_filter_by_set_key.
It is processed only one of duplicate values(`key` + `name`) in each directives(stream, server) if this option is enabled.

### server_traffic_status_limit

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_limit** \<on\|off\> |
| **Default** | on |
| **Context** | stream, server |

`Description:` Enables or disables the limit features.

### server_traffic_status_limit_traffic

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_limit_traffic** *member*:*size* [*code*] |
| **Default** | - |
| **Context** | stream, server |

`Description:` Enables the traffic limit for specified *member*.
The *member* is a member string to limit traffic.
The *size* is a size(k/m/g) to limit traffic.
The *code* is a code to return in response to rejected requests.(Default: 503)

The available *`member`* strings are as follows:
* **connect**
  * The total number of client connects received from clients.
* **in**
  * The total number of bytes received from clients.
* **out**
  * The total number of bytes sent to clients.
* **1xx**
  * The number of responses with status codes 1xx.
* **2xx**
  * The number of responses with status codes 2xx.
* **3xx**
  * The number of responses with status codes 3xx.
* **4xx**
  * The number of responses with status codes 4xx.
* **5xx**
  * The number of responses with status codes 5xx.

### server_traffic_status_limit_traffic_by_set_key

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_limit_traffic_by_set_key** *key* *member*:*size* [*code*] |
| **Default** | - |
| **Context** | stream, server |

`Description:` Enables the traffic limit for specified *key* and *member*.
The *key* is a key string to limit traffic.
The *member* is a member string to limit traffic.
The *size* is a size(k/m/g) to limit traffic.
The *code* is a code to return in response to rejected requests.(Default: 503)


The *`key`* syntax is as follows:
* *`group`*@[*`subgroup`*@]*`name`*

The available *`group`* strings are as follows:
* **NO**
  * The group of server.
* **UA**
  * The group of upstream alone.
* **UG**
  * The group of upstream group.(use *`subgroup`*)
* **FG**
  * The group of filter.(use *`subgroup`*)

The available *`member`* strings are as follows:
* **connect**
  * The total number of client requests received from clients.
* **in**
  * The total number of bytes received from clients.
* **out**
  * The total number of bytes sent to clients.
* **1xx**
  * The number of responses with status codes 1xx.
* **2xx**
  * The number of responses with status codes 2xx.
* **3xx**
  * The number of responses with status codes 3xx.
* **4xx**
  * The number of responses with status codes 4xx.
* **5xx**
  * The number of responses with status codes 5xx.

The *member* is the same as `server_traffic_status_limit_traffic` directive.

### server_traffic_status_limit_check_duplicate

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_limit_check_duplicate** \<on\|off\> |
| **Default** | on |
| **Context** | stream, server |

`Description:` Enables or disables the deduplication of server_traffic_status_limit_by_set_key.
It is processed only one of duplicate values(`member` | `key` + `member`)
in each directives(stream, server) if this option is enabled.

### server_traffic_status_average_method

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_average_method** \<AMM\|WMA\> [*period*] |
| **Default** | AMM 60s |
| **Context** | stream, server |

`Description:` Sets the method which is a formula that calculate the average of response processing times.
The *period* is an effective time of the values used for the average calculation.(Default: 60s)
If *period* set to 0, effective time is ignored.
In this case, the last average value is displayed even if there is no requests and after the elapse of time.
The corresponding value is only *$sts_session_time* variable.

* **AMM**
  * The AMM is the [arithmetic mean](https://en.wikipedia.org/wiki/Arithmetic_mean).
* **WMA**
  * THE WMA is the [weighted moving average](https://en.wikipedia.org/wiki/Moving_average#Weighted_moving_average).


`Caveats`: The *$sts_session_time* variable is the value calculated at the time of the last request.
It is not calculated when using variables.

### server_traffic_status_histogram_buckets

| -   | - |
| --- | --- |
| **Syntax**  | **server_traffic_status_histogram_buckets** *second* ... |
| **Default** | - |
| **Context** | stream |

`Description:` Sets the observe buckets to be used in the histograms.
By default, if you do not set this directive, it will not work.
The *second* can be expressed in decimal places with a minimum value of 0.001(1ms).
The maximum size of the buckets is 32. If this value is insufficient for you,
change the `NGX_STREAM_SERVER_TRAFFIC_STATUS_DEFAULT_BUCKET_LEN` in the
`nginx-mdule-stream-sts/src/ngx_stream_server_traffic_status_node.h`
and the `NGX_HTTP_STREAM_SERVER_TRAFFIC_STATUS_DEFAULT_BUCKET_LEN` in the
`nginx-module-sts/src/ngx_http_stream_server_traffic_status_node.h`.

For examples:
* **server_traffic_status_histogram_buckets** `0.005` `0.01` `0.05` `0.1` `0.5` `1` `5` `10`
  * The observe buckets are [5ms 10ms 50ms 1s 5s 10s].
* **server_traffic_status_histogram_buckets** `0.005` `0.01` `0.05` `0.1`
  * The observe buckets are [5ms 10ms 50ms 1s].

`Caveats:` By default, if you do not set this directive, the histogram statistics does not work.

## See Also
* [nginx-module-stream-sts](https://github.com/vozlt/nginx-module-stream-sts)
* [nginx-module-vts](https://github.com/vozlt/nginx-module-vts)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-sts](https://github.com/vozlt/nginx-module-sts){target=_blank}.

# *substitutions*: String substitutions module for nginx


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-substitutions
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_subs_filter_module.so;
```


This document describes nginx-module-substitutions [v0.6.6](https://github.com/dvershinin/ngx_http_substitutions_filter_module/releases/tag/v0.6.6){target=_blank} 
released on Dec 30 2021.

<hr />
nginx_substitutions_filter
    *Note: this module is not distributed with the Nginx source.
    Installation instructions can be found below.*

  Description
    nginx_substitutions_filter is a filter module which can do both regular
    expression and fixed string substitutions on response bodies. This
    module is quite different from the Nginx's native Substitution Module.
    It scans the output chains buffer and matches string line by line, just
    like Apache's mod_substitute
    (<http://httpd.apache.org/docs/trunk/mod/mod_substitute.html>).

  Example
    location / {

        subs_filter_types text/html text/css text/xml;
        subs_filter st(\d*).example.com $1.example.com ir;
        subs_filter a.example.com s.example.com;
        subs_filter http://$host https://$host;
    }

  Directives
    *   subs_filter_types

    *   subs_filter

   subs_filter_types
    syntax: *subs_filter_types mime-type [mime-types] *

    default: *subs_filter_types text/html*

    context: *http, server, location*

    *subs_filter_types* is used to specify which content types should be
    checked for *subs_filter*, in addition to *text/html*. The default is
    only *text/html*.

    This module just works with plain text. If the response is compressed,
    it can't uncompress the response and will ignore this response. This
    module can be compatible with gzip filter module. But it will not work
    with proxy compressed response. You can disable the compressed response
    like this:

    proxy_set_header Accept-Encoding "";

   subs_filter
    syntax: *subs_filter source_str destination_str [gior] *

    default: *none*

    context: *http, server, location*

    *subs_filter* allows replacing source string(regular expression or
    fixed) in the nginx response with destination string. The variables 
    in matching text is only avaiable under fixed string mode, which means 
    the matching text could not contain variables if it is a regular 
    expression. Substitution text may contain variables. More than one 
    substitution rules per location is supported. 
    The meaning of the third flags are:

    *   *g*(default): Replace all the match strings.

    *   *i*: Perform a case-insensitive match.

    *   *o*: Just replace the first one.

    *   *r*: The pattern is treated as a regular expression, default is
        fixed string.

   subs_filter_bypass
    syntax: *subs_filter_bypass $variable1 ...*

    default: *none*

    context: *http, server, location*

    You can sepcify several variables with this directive. If at least one
    of the variable is not empty and is not equal to '0', this substitution
    filter will be disabled.

  Installation
    To install, get the source with subversion:

    git clone
    git://github.com/yaoweibin/ngx_http_substitutions_filter_module.git

    and then compile nginx with the following option:

    ./configure --add-module=/path/to/module

  Known issue
    *   Can't substitute the response header.

  CHANGES
    Changes with nginx_substitutions_filter 0.6.4 2014-02-15

    *   Now non-200 response will work

    *   added the subs_filter_bypass directive

    Changes with nginx_substitutions_filter 0.6.2 2012-08-26

    *   fixed a bug of buffer overlap

    *   fixed a bug with last zero buffer

    Changes with nginx_substitutions_filter 0.6.0 2012-06-30

    *   refactor this module

    Changes with nginx_substitutions_filter 0.5.2 2010-08-11

    *   do many optimizing for this module

    *   fix a bug of buffer overlap

    *   fix a segment fault bug when output chain return NGX_AGAIN.

    *   fix a bug about last buffer with no linefeed. This may cause segment
        fault. Thanks for Josef Fröhle

    Changes with nginx_substitutions_filter 0.5 2010-04-15

    *   refactor the source structure, create branches of dev

    *   fix a bug of small chunk of buffers causing lose content

    *   fix the bug of last_buf and the nginx's compatibility above 0.8.25

    *   fix a bug with unwanted capture config error in fix string
        substitution

    *   add feature of regex captures

    Changes with nginx_substitutions_filter 0.4 2009-12-23

    *   fix many bugs

    Changes with nginx_substitutions_filter 0.3 2009-02-04

    *   Initial public release

  Reporting a bug
    Questions/patches may be directed to Weibin Yao, yaoweibin@gmail.com.

  Copyright & License
    This module is licensed under the BSD license.

    Copyright (C) 2014 by Weibin Yao <yaoweibin@gmail.com>.

    All rights reserved.

    Redistribution and use in source and binary forms, with or without
    modification, are permitted provided that the following conditions are
    met:

    *
          Redistributions of source code must retain the above copyright

        notice, this list of conditions and the following disclaimer.

    *
          Redistributions in binary form must reproduce the above copyright

        notice, this list of conditions and the following disclaimer in the
        documentation and/or other materials provided with the distribution.

    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
    IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
    TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
    PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
    HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
    TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
    PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
    LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
    NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
    SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-substitutions](https://github.com/dvershinin/ngx_http_substitutions_filter_module){target=_blank}.

# *sysguard*: NGINX sysguard module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-sysguard
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_sysguard_module.so;
```


This document describes nginx-module-sysguard [v0.0.1](https://github.com/dvershinin/nginx-module-sysguard/releases/tag/v0.0.1){target=_blank} 
released on Feb 29 2020.

<hr />

[![License](http://img.shields.io/badge/license-BSD-brightgreen.svg)](https://github.com/vozlt/nginx-module-sysguard/blob/master/LICENSE)

Nginx sysguard module

## Synopsis

```Nginx
http {

    ...

    server {

        ...

        sysguard on;
        sysguard_mode or;

        sysguard_load load=10.5 action=/loadlimit;
        sysguard_mem swapratio=20% action=/swaplimit;
        sysguard_mem free=100M action=/freelimit;
        sysguard_rt rt=0.01 period=5s method=AMM:10 action=/rtlimit;

        location /loadlimit {
            return 503;
        }

        location /swaplimit {
            return 503;
        }

        location /freelimit {
            return 503;
        }

        location /rtlimit {
            return 503;
        }
    }

    ...

    server {

        ...

        location /api {
            sysguard on;
            sysguard_mode or;
            sysguard_load load=20 action=/limit;
            sysguard_mem swapratio=10% action=/limit;
            sysguard_rt rt=2.01 period=5s method=WMA:10 action=/limit;

            ... 

        }

        location /images {
            sysguard on;
            sysguard_mode and;
            sysguard_load load=20 action=/limit;
            sysguard_mem swapratio=10% action=/limit;
            sysguard_rt rt=2.01 period=5s method=WMA:10 action=/limit;

            ...

        }

        location /limit {
            return 503;
        }
    }

}
```

## Description
This module can be used to protect your server in case system load, memory use goes too high or requests are responded too slow.
This is a porting version of the [sysguard](http://tengine.taobao.org/document/http_sysguard.html) in [tengine](https://github.com/alibaba/tengine) to the pure NGINX so as to support the same features.

`Caveats:` Note this module requires the sysinfo(2) system call, or getloadavg(3) function in glibc. It also requires the /proc file system to get memory information.

## Embedded Variables
The following embedded variables are provided:

* **$sysguard_load**
  * The load of system. If `$sysguard_load`'s value is 100, then load is 0.1(100/1000). (/msec)
* **$sysguard_swapstat**
  * The ratio of using swap. (/per)
* **$sysguard_free**
  * The real free space of memory. (/byte)
* **$sysguard_rt**
  * The average of request processing times. If `$sysguard_rt`'s value is 100, then response time is 0.1sec(100/1000). (/msec)
* **$sysguard_meminfo_totalram**
  * The total memory of meminfo. (/byte)
* **$sysguard_meminfo_freeram**
  * The free memory of meminfo. (/byte)
* **$sysguard_meminfo_bufferram**
  * The buffer memory of meminfo. (/byte)
* **$sysguard_meminfo_cachedram**
  * The cached memory of meminfo. (/byte)
* **$sysguard_meminfo_totalswap**
  * The total swap of meminfo. (/byte)
* **$sysguard_meminfo_freeswap**
  * The free swap of meminfo. (/byte)


## Directives

### sysguard

| -   | - |
| --- | --- |
| **Syntax**  | **sysguard** \<on\|off\> |
| **Default** | off |
| **Context** | http, server, location |

`Description:` Enables or disables the module working.

### sysguard_load

| -   | - |
| --- | --- |
| **Syntax**  | **sysguard_load** load=*number* [action=*/url*] |
| **Default** | - |
| **Context** | http, server, location |

`Description:` Specify the load threshold. When the system load exceeds this threshold, all subsequent requests will be redirected to the URL specified by the 'action' parameter. It will return 503 if there's no 'action' URL defined. This directive also support using ncpuratio to instead of the fixed threshold, 'ncpu' means the number of cpu's cores, you can use this directive like this: load=ncpu1.5

### sysguard_mem

| -   | - |
| --- | --- |
| **Syntax**  | **sysguard_mem** swapratio=*ratio*% free=*size* [action=*/url*] |
| **Default** | - |
| **Context** | http, server, location |

`Description:` Specify the used swap memory or free memory threshold. When the swap memory use ratio exceeds this threshold or memory free less than the size, all subsequent requests will be redirected to the URL specified by the 'action' parameter. It will return 503 if there's no 'action' URL. Sysguard uses this strategy to calculate memory free: "memfree = free + buffered + cached"

### sysguard_rt

| -   | - |
| --- | --- |
| **Syntax**  | **sysguard_rt** rt=*second* period=*time* [method=\<AMM\|WMA\>:*number*] [action=*/url*] |
| **Default** | - |
| **Context** | http, server, location |

`Description:` Specify the response time threshold.
Parameter rt is used to set a threshold of the average response time, in second.
Parameter period is used to specifiy the period of the statistics cycle.
If the average response time of the system exceeds the threshold specified by the user,
the incoming request will be redirected to a specified url which is defined by parameter 'action'.
If no 'action' is presented, the request will be responsed with 503 error directly.
The `method` is a formula that calculate the average of response processing times.
The `number` in method is the number of samples to calculate the average.
The default method is set to be `method=AMM:period`.

* **AMM**
  * The AMM is the [arithmetic mean](https://en.wikipedia.org/wiki/Arithmetic_mean).
* **WMA**
  * THE WMA is the [weighted moving average](https://en.wikipedia.org/wiki/Moving_average#Weighted_moving_average).

### sysguard_mode

| -   | - |
| --- | --- |
| **Syntax**  | **sysguard_mode** \<and\|or\> |
| **Default** | or |
| **Context** | http, server, location |

`Description:` If there are more than one type of monitor, this directive is used to specified the relations among all the monitors which are: 'and' for all matching and 'or' for any matching.

### sysguard_interval

| -   | - |
| --- | --- |
| **Syntax**  | **sysguard_interval** *time* |
| **Default** | 1s |
| **Context** | http, server, location |

`Description:` Specify the time interval to update your system information.
The default value is one second, which means sysguard updates the server status once a second.

### sysguard_log_level

| -   | - |
| --- | --- |
| **Syntax**  | **sysguard_log_level** \<info\|notice\|warn\|error\> |
| **Default** | error |
| **Context** | http, server, location |

`Description:` Specify the log level of sysguard.

## See Also
* [nginx-module-vts](https://github.com/vozlt/nginx-module-vts)
* [nginx-module-sts](https://github.com/vozlt/nginx-module-sts)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-sysguard](https://github.com/dvershinin/nginx-module-sysguard){target=_blank}.

# *testcookie*: NGINX testcookie robot mitigation module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-testcookie
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_testcookie_access_module.so;
```


This document describes nginx-module-testcookie [v1.28](https://github.com/dvershinin/testcookie-nginx-module/releases/tag/v1.28){target=_blank} 
released on Jul 19 2022.

<hr />

**testcookie-nginx-module** is a simple robot mitigation module using cookie based challenge/response.

Challenge cookies can be set using different methods:

*   "Set-Cookie" + 302/307 HTTP Location redirect
*   "Set-Cookie" + HTML meta refresh redirect
*   Custom template, JavaScript can be used here.

To prevent automatic parsing, challenge cookie value can be encrypted with AES-128 in CBC mode using custom/random key and iv, and then decrypted at client side with JavaScript.


## Directives

## testcookie
**syntax:** *testcookie (on|off|var);*

**default:** *off*

**context:** *http, server, location, if*

on - Enable module

off - Disable module

var - Don't intercept requests, only set module variables.


## testcookie_name
**syntax:** *testcookie_name &lt;string&gt;*

**default:** *TCK*

**context:** *http, server, location*

Sets cookie name.

## testcookie_domain
**syntax:** *testcookie_domain &lt;string&gt;*

**default:** *none, set by browser*

**context:** *http, server, location*

Sets cookie domain.


## testcookie_expires
**syntax:** *testcookie_expires &lt;string&gt;*

**default:** *31 Dec 2037 23:55:55 GMT*

**context:** *http, server, location*

Sets cookie expiration value.

## testcookie_path
**syntax:** *testcookie_path &lt;string&gt;*

**default:** */*

**context:** *http, server, location*

Sets cookie path, useful if you plan to use different keys for locations.

## testcookie_samesite
**syntax:** *testcookie_samesite &lt;string&gt;*

**default:** *None*

**context:** *http, server, location*

Sets cookie attribute, allows you to declare if your cookie should be restricted to a first-party or same-site context.
Default is None (Cookies will be sent in all contexts, i.e sending cross-origin is allowed.)
Accepts values: Lax, Strict, None.

## testcookie_secret
**syntax:** *testcookie_secret &lt;string&gt;*

**default:** *required configuration directive*

**context:** *http, server, location*

Secret string, used in challenge cookie computation, should be 32 bytes or more,
better to be long but static to prevent cookie reset for legitimate users every server restart.
If set to *"random"* - new secret will be generated every server restart, not recomended(all cookies with previous key will be invalid),

## testcookie_session
**syntax:** *testcookie_session &lt;variable&gt;*

**default:** *required configuration directive*

**context:** *http, server, location*

Sets the challenge generation function input,
*   $remote_addr - clients IP address will be used as an user unique identifier
*   $remote_addr$http_user_agent - clients IP + User-Agent

## testcookie_arg
**syntax:** *testcookie_arg &lt;string&gt;*

**default:** *none*

**context:** *http, server, location*

Sets GET parameter name, used for cookie setting attempts computation,

If not set - server will try to set cookie infinitely.

## testcookie_max_attempts
**syntax:** *testcookie_max_attempts &lt;integer&gt;*

**default:** *5*

**context:** *http, server, location*

Sets maximum number of redirects before user will be sent to fallback URL, according to RFC1945 can't be more than 5.

If set to 0 - server will try to set cookie infinitely(actually, browser will show the error page).


## testcookie_p3p
**syntax:** *testcookie_p3p &lt;string&gt;*

**default:** *none*

**context:** *http, server, location*

Sets P3P policy.

## testcookie_fallback
**syntax:** *testcookie_fallback &lt;script&gt;*

**default:** *none*

**context:** *http, server, location*

Sets the fallback URL, user will be redirected to after maximum number of attempts, specified by directive *testcookie_max_attempts* exceded.
Nginx scripting variables can be used here. If not set - client will get 403 after max attempts reached.

## testcookie_whitelist
**syntax:** *testcookie_whitelist &lt;network list&gt;*

**default:** *none*

**context:** *http, server*

Sets the networks for which the testing will not be used, add search engine networks here. Currently IPv4 CIDR only.

## testcookie_pass
**syntax:** *testcookie_pass $variable;*

**default:** *none*

**context:** *http, server*

Sets the variable name to test if cookie check should be bypassed.
If variable value set to *1* during the request - cookie check will not be performed.
Can be used for more complex whitelisting.

## testcookie_redirect_via_refresh
**syntax:** *testcookie_redirect_via_refresh (on|off);*

**default:** *off*

**context:** *http, server, location*

Set cookie and redirect using HTTP meta refresh, required if *testcookie_refresh_template* used.

## testcookie_refresh_template
**syntax:** *testcookie_refresh_template &lt;string&gt;*

**default:** *none*

**context:** *http, server, location*

Use custom html instead of simple HTTP meta refresh, you need to set cookie manually from the template
Available all the nginx variables and

    $testcookie_nexturl - URL the client should be redirected to, if max_attempts exceeded *testcookie_fallback* value will be here
    $testcookie_got - cookie value received from client, empty if no cookie or it does not match format
    $testcookie_set - correct cookie value we're expecting from client
    $testcookie_ok - user passed test (1 - passed, 0 - not passed) Note: changed from "yes"/"no" in v1.10

also, if testcookie_refresh_encrypt_cookie enabled there are three more variables:

    $testcookie_enc_key - encryption key (32 hex digits)
    $testcookie_enc_iv - encryption iv (32 hex digits)
    $testcookie_enc_sec - encrypted cookie value (32 hex digits)

## testcookie_refresh_status
**syntax:** *testcookie_refresh_status &lt;code&gt;*

**default:** *200*

**context:** *http, server, location*

Use custom HTTP status code when serving html.


## testcookie_deny_keepalive
**syntax:** *testcookie_deny_keepalive (on|off);*

**default:** *off*

**context:** *http, server, location*

Close connection just after setting the cookie, no reason to keep connections with bots.

## testcookie_get_only
**syntax:** *testcookie_get_only (on|off);*

**default:** *off*

**context:** *http, server, location*

Process only GET requests, POST requests will be bypassed.

## testcookie_https_location
**syntax:** *testcookie_https_location (on|off);*

**default:** *off*

**context:** *http, server, location*

Redirect client to https protocol after setting the cookie, also affects *$testcookie_nexturl*, useful with 3dparty SSL offload.

## testcookie_refresh_encrypt_cookie
**syntax:** *testcookie_refresh_encrypt_cookie (on|off);*

**default:** *off*

**context:** *http, server, location*

Encrypt cookie variable, used with *testcookie_refresh_template* to force client-side decryption with AES-128 CBC.

## testcookie_refresh_encrypt_cookie_key
**syntax:** *testcookie_refresh_encrypt_cookie_key &lt;32 hex digits|random&gt;*

**default:** *required directive if encryption enabled*

**context:** *http, server, location*

Sets encryption key.

Possible values:

    random - new key generated every nginx restart
    32 hex digits - static key, useful if you plan to obfuscate it deep in client-side javascript.

## testcookie_refresh_encrypt_iv
**syntax:** *testcookie_refresh_encrypt_iv &lt;32 hex digits|random|random2&gt;*

**default:** *random*

**context:** *http, server, location*

Sets encryption iv.

Possible values:
    random - new iv generated for every client request
    random2 - new iv generated for every nginx restart
    32 hex digits - static iv, useful if you plan to obfuscate it deep in client-side javascript

## testcookie_internal
**syntax:** *testcookie_internal (on|off);*

**default:** *off*

**context:** *http, server, location*

Enable testcookie check for internal redirects (disabled by default for optimization purposes!), useful for this type of configs:

    rewrite ^/(.*)$ /index.php?$1 last;

## testcookie_httponly_flag
**syntax:** *testcookie_httponly_flag (on|off);*

**default:** *off*

**context:** *http, server, location*

Enable HttpOnly flag for cookie.

## testcookie_secure_flag
**syntax:** *testcookie_secure_flag (on|off|$variable);*

**default:** *on*

**context:** *http, server, location*

Enable Secure flag for cookie.
Any variable value except "on" interpreted as False.

## testcookie_port_in_redirect
**syntax:** *testcookie_port_in_redirect (on|off);*

**default:** *off*

**context:** *http, server, location*

Expose port in redirect.


## Example configuration

    http {
        #default config, module disabled
        testcookie off;

        #setting cookie name
        testcookie_name BPC;

        #setting secret
        testcookie_secret keepmesecret;

        #setting session key
        testcookie_session $remote_addr;

        #setting argument name
        testcookie_arg ckattempt;

        #setting maximum number of cookie setting attempts
        testcookie_max_attempts 3;

        #setting p3p policy
        testcookie_p3p 'CP="CUR ADM OUR NOR STA NID", policyref="/w3c/p3p.xml"';

        #setting fallback url
        testcookie_fallback http://google.com/cookies.html?backurl=http://$host$request_uri;

        #configuring whitelist
        testcookie_whitelist {
            8.8.8.8/32;
        }


        #setting redirect via html code
        testcookie_redirect_via_refresh on;

        #enable encryption
        testcookie_refresh_encrypt_cookie on;

        #setting encryption key
        testcookie_refresh_encrypt_cookie_key deadbeefdeadbeefdeadbeefdeadbeef;

        #setting encryption iv
        testcookie_refresh_encrypt_cookie_iv deadbeefdeadbeefdeadbeefdeadbeef;

        #setting response template
        testcookie_refresh_template '<html><body>setting cookie...<script type=\"text/javascript\" src=\"/aes.min.js\" ></script><script>function toNumbers(d){var e=[];d.replace(/(..)/g,function(d){e.push(parseInt(d,16))});return e}function toHex(){for(var d=[],d=1==arguments.length&&arguments[0].constructor==Array?arguments[0]:arguments,e="",f=0;f<d.length;f++)e+=(16>d[f]?"0":"")+d[f].toString(16);return e.toLowerCase()}var a=toNumbers("$testcookie_enc_key"),b=toNumbers("$testcookie_enc_iv"),c=toNumbers("$testcookie_enc_set");document.cookie="BPC="+toHex(slowAES.decrypt(c,2,a,b))+"; expires=Thu, 31-Dec-37 23:55:55 GMT; path=/";location.href="$testcookie_nexturl";</script></body></html>';

        server {
            listen 80;
            server_name test.com;


            location = /aes.min.js {
                gzip  on;
                gzip_min_length 1000;
                gzip_types      text/plain;
                root /var/www/public_html;
            }

            location = /w3c/p3p.xml {
                root /var/www/public_html;
            }

            location / {
                #enable module for specific location
                testcookie on;
                proxy_set_header   Host             $host;
                proxy_set_header   X-Real-IP        $remote_addr;
                proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
                proxy_pass http://127.0.0.1:80;
            }
        }
    }

See more cases in "docs" directory of the project.

## Test suite

This module comes with a Perl-driven test suite. Thanks to the [Test::Nginx](http://search.cpan.org/perldoc?Test::Nginx) module in the Perl world.

## Sources

Available on github at [kyprizel/testcookie-nginx-module](http://github.com/kyprizel/testcookie-nginx-module).

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-testcookie](https://github.com/dvershinin/testcookie-nginx-module){target=_blank}.

# *traffic-accounting*: Monitor the incoming and outgoing traffic metrics in realtime for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-traffic-accounting
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_accounting_module.so;
```


This document describes nginx-module-traffic-accounting [v2.0.4](https://github.com/dvershinin/traffic-accounting-nginx-module/releases/tag/v2.0.4){target=_blank} 
released on May 16 2022.

<hr />

Monitor the incoming and outgoing traffic metrics in realtime for `NGINX`.

A realtime traffic and status code monitor solution for NGINX,
which needs less memory and cpu than other realtime log analyzing solutions.
Useful for traffic accounting based on NGINX config logic (by location / server / user-defined-variables).

[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2FLax%2Ftraffic-accounting-nginx-module.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2FLax%2Ftraffic-accounting-nginx-module?ref=badge_shield)
[![Financial Contributors on Open Collective](https://opencollective.com/traffic-acctiong-nginx-module/all/badge.svg?label=financial+contributors)](https://opencollective.com/traffic-acctiong-nginx-module) 

## Why?

Realtime log analysis solutions,
which requires multiple machines for storage and analysis,
are too heavy for application monitoring.

An cost-effective solution is in need to monitor the traffic metrics/status of application requests.
This solution should be accurate, sensitive, robust, light weight enough, and not affected by traffic peaks.

## How it works?

This module keeps a list of **metrics** identified by `accounting_id` in its context.

When a new **request** hits the server, the module will try to find its `accounting_id`, calculate statistics, and **aggregate** them into the corresponding metrics by `accounting_id`.

For each time period (defined by `interval`), a timer event is triggered, those metrics are rotated and exported to log files or sent to remote log servers.

## 

## Dashboard

**Dashboard - Visualize with Grafana**
![Accounting Dashboard](http://lax.github.io/traffic-accounting-nginx-module/images/accounting-dashboard.png)
## 

## Configuration

Edit your nginx.conf.

Example:

```nginx
http {
    # turn on accounting function
    accounting  on;
    accounting_log  logs/http-accounting.log;
    ...
    server {
        server_name example.com;

        accounting_id  $http_host;  # set accounting_id string by variable

        location / {
            accounting_id  accounting_id_str;  # set accounting_id string by location

            ...
        }

        location /api {
            accounting_id  API_PC;   # for pc

            if ($http_user_agent ~* '(Android|webOS|iPhone|iPod|BlackBerry)') {
                accounting_id  API_MOBILE;   # for mobile
            }

            ...
        }
    }

}
```

## Directives

## accounting
**syntax:** *accounting on | off*

**default:** *accounting off*

**context:** *http*

## accounting_log
**syntax:** *accounting_log \</path/to/log/file> \[level]*

**default:** *-*

**context:** *http*

Configures logging.

Support both local `file` path, or `stderr`, or `syslog:`.
The second parameter is the log level.
For more details of supported params, refer to [this page from nginx.org](http://nginx.org/en/docs/ngx_core_module.html#error_log).

If not specified, accounting log will be written to `/dev/log`.

## accounting_id
**syntax:** *accounting_id \<accounting_id>*

**default:** *accounting_id default*

**context:** *http, server, location, if in location*

Sets the `accounting_id` string by user defined variable.

This string is used to determine which `metrics` a request/session should be aggregated to.

## accounting_interval
**syntax:** *accounting_interval \<seconds>*

**default:** *accounting_interval 60*

**context:** *http*

Specifies the reporting interval.  Defaults to 60 seconds.

## accounting_perturb
**syntax:** *accounting_perturb on | off*

**default:** *accounting_perturb off*

**context:** *http*

Randomly staggers the reporting interval by 20% from the usual time.

## Usage

This module can be configured to writes metrics to local file, remote log server or local syslog device.

Open-source log-aggregation software such as logstash also support syslog input, which will help you establish a central log server.
See [samples/logstash/](samples/logstash/) for examples. [**Recommended**]

To collect logs with local syslog,
refer [Lax/ngx_http_accounting_module-utils](http://github.com/Lax/ngx_http_accounting_module-utils) to for sample configuration / utils.

## docker / docker-compose
To demonstrate with docker-compose, run

```
docker-compose build
docker-compose up -d
```

Open Grafana (address: `http://localhost:3000`) in your browser.

Create and configurate elasticsearch datasource with options:
```
Type: elasticsearch
URL: http://elasticsearch:9200
Version: 5.6+
Min time interval: 1m
```

Then import accounting dashboard from  [`samples/accounting-dashboard-grafana.json`](samples/accounting-dashboard-grafana.json).


## Metrics log format

```
## HTTP
2018/05/14 14:18:18 [notice] 5#0: pid:5|from:1526278638|to:1526278659|accounting_id:HTTP_ECHO_HELLO|requests:4872|bytes_in:438480|bytes_out:730800|latency_ms:0|upstream_latency_ms:0|200:4872
2018/05/14 14:18:18 [notice] 5#0: pid:5|from:1526278638|to:1526278659|accounting_id:INDEX|requests:4849|bytes_in:421863|bytes_out:1857167|latency_ms:0|upstream_latency_ms:0|301:4849

## Stream
2018/05/14 14:18:22 [notice] 5#0: pid:5|from:1526278642|to:1526278659|accounting_id:TCP_PROXY_ECHO|sessions:9723|bytes_in:860343|bytes_out:2587967|latency_ms:4133|upstream_latency_ms:3810|200:9723
```

Each line of the log output contains `metrics` for a particular `accounting_id`,
which contains a list of key-values.

|  key name       |  meanings of values |
|-----------------|---------------------|
| `pid`           | pid of nginx worker process |
| `from` / `to`   | metric was collected from the `period` between these timestamps |
| `accounting_id` | identify for the accounting unit, set by `accounting_id` directive |
| `requests`      | count of total requests processed in current period (HTTP module only) |
| `sessions`      | count of total sessions processed in current period (Stream module only) |
| `bytes_in`      | total bytes received by the server |
| `bytes_out`     | total bytes send out by the server |
| `latency_ms`    | sum of all requests/sessions' `$session_time`, in `millisecond` |
| `upstream_latency_ms`  | sum of `$upstream_response_time`, in `millisecond` |
| `200` / `302` / `400` / `404` / `500` ... | count of requests/sessions with status code `200`/`302`/`400`/`404`/`500`, etc. Notice the differences between http codes and stream codes |

## 


### Configuration example

```nginx
http {
  accounting        on;
  accounting_log    logs/http-accounting.log;
  accounting_id     $hostname;

  ...
}

stream {
  accounting        on;
  accounting_log    logs/stream-accounting.log;
  accounting_id     $hostname;

  ...
}
```

## Visualization

Visualization with `Kibana` or `Grafana` is easy.
See [samples/](samples/) for examples.
## 

## Branches

* master : main development branch.
* tag v0.1 or v2-freeze-20110526 : legacy release. works with nginx version(0.7.xx, 0.8.xx), nginx 0.9 is not tested. didn't work with nginx above 1.0.x.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-traffic-accounting](https://github.com/dvershinin/traffic-accounting-nginx-module){target=_blank}.

# *ts*: NGINX MPEG-TS Live Module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-ts
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_ts_module.so;
```


This document describes nginx-module-ts [v0.1.1](https://github.com/arut/nginx-ts-module/releases/tag/v0.1.1){target=_blank} 
released on Jul 14 2017.

<hr />

## Features

-   receives MPEG-TS over HTTP
-   produces and manages live
    [HLS](https://tools.ietf.org/html/draft-pantos-http-live-streaming-23)
-   produces and manages live
    [MPEG-DASH](https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-ts](https://github.com/arut/nginx-ts-module){target=_blank}.

# *untar*: NGINX HTTP Untar Module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-untar
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_untar_module.so;
```


This document describes nginx-module-untar [v1.1](https://github.com/ajax16384/ngx_http_untar_module/releases/tag/v1.1){target=_blank} 
released on Mar 21 2022.

<hr />
This [nginx](https://nginx.org/) module can serve static file content directly from tar archives.
Inspired by [nginx-unzip-module](https://github.com/youzee/nginx-unzip-module).

## Features
* Zero-copy: outputs content directly from archive file (no temporary files)
* Caching parsed archive file entries: reduce archive scan-search time
* Supported tar item types: normal file, long file name data

## Configuration example
```nginx
  location ~ ^/(.+?\.tar)/(.*)$ {
      untar_archive "$document_root/$1";
      untar_file "$2";
      untar;
  }
```
## Module directives
***
**untar_archive** `string`

**context:** `http, server, location`

Specifies tar archive name.
***
**untar_file** `string`

**context:** `http, server, location`

Specifies file to be extracted from **untar_archive**.
***
**untar**

**context:** `location`

Invokes untar of **untar_file** from **untar_archive**
***
## Known limitations
* only GET,HEAD verbs supported
* no archive entries listing
* base tar format support (only normal files: no symlink, sparse e.t.c)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-untar](https://github.com/ajax16384/ngx_http_untar_module){target=_blank}.

# *upload*: NGINX module for handling file uploads


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-upload
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_upload_module.so;
```


This document describes nginx-module-upload [v2.3.0](https://github.com/fdintino/nginx-upload-module/releases/tag/2.3.0){target=_blank} 
released on Aug 02 2018.

<hr />


A module for [nginx](https://www.nginx.com/) for handling file uploads using
multipart/form-data encoding ([RFC 1867](http://www.ietf.org/rfc/rfc1867.txt))
and resumable uploads according to
[this](https://github.com/fdintino/nginx-upload-module/upload-protocol.md)
protocol.

* [Description](#description)
* [Directives](#directives)
    * [upload_pass](#upload_pass)
    * [upload_resumable](#upload_resumable)
    * [upload_store](#upload_store)
    * [upload_state_store](#upload_state_store)
    * [upload_store_access](#upload_store_access)
    * [upload_set_form_field](#upload_set_form_field)
    * [upload_aggregate_form_field](#upload_aggregate_form_field)
    * [upload_pass_form_field](#upload_pass_form_field)
    * [upload_cleanup](#upload_cleanup)
    * [upload_buffer_size](#upload_buffer_size)
    * [upload_max_part_header_len](#upload_max_part_header_len)
    * [upload_max_file_size](#upload_max_file_size)
    * [upload_limit_rate](#upload_limit_rate)
    * [upload_max_output_body_len](#upload_max_output_body_len)
    * [upload_tame_arrays](#upload_tame_arrays)
    * [upload_pass_args](#upload_pass_args)
* [Example configuration](#example-configuration)
* [License](#license)

## Description

The module parses request body storing all files being uploaded to a
directory specified by [`upload_store`](#upload_store) directive. The
files are then being stripped from body and altered request is then
passed to a location specified by [`upload_pass`](#upload_pass)
directive, thus allowing arbitrary handling of uploaded files. Each of
file fields are being replaced by a set of fields specified by
[`upload_set_form_field`](#upload_set_form_field) directive. The
content of each uploaded file then could be read from a file specified
by $upload_tmp_path variable or the file could be simply moved to
ultimate destination. Removal of output files is controlled by directive
[`upload_cleanup`](#upload_cleanup). If a request has a method other than
POST, the module returns error 405 (Method not allowed). Requests with
such methods could be processed in alternative location via
[`error_page`](http://nginx.org/en/docs/http/ngx_http_core_module.html#error_page)
directive.

## Directives

### upload_pass

**Syntax:** <code><b>upload_pass</b> <i>location</i></code><br>
**Default:** —<br>
**Context:** `server,location`

Specifies location to pass request body to. File fields will be stripped
and replaced by fields, containing necessary information to handle
uploaded files.

### upload_resumable

**Syntax:** <code><b>upload_resumable</b> on | off</code><br>
**Default:** `upload_resumable off`<br>
**Context:** `main,server,location`

Enables resumable uploads.

### upload_store

**Syntax:** <code><b>upload_store</b> <i>directory</i> [<i>level1</i> [<i>level2</i>]] ...</code><br>
**Default:** —<br>
**Context:** `server,location`

Specifies a directory to which output files will be saved to. The
directory could be hashed. In this case all subdirectories should exist
before starting nginx.

### upload_state_store

**Syntax:** <code><b>upload_state_store</b> <i>directory</i> [<i>level1</i> [<i>level2</i>]] ...</code><br>
**Default:** —<br>
**Context:** `server,location`

Specifies a directory that will contain state files for resumable
uploads. The directory could be hashed. In this case all subdirectories
should exist before starting nginx.

### upload_store_access

**Syntax:** <code><b>upload_store_access</b> <i>mode</i></code><br>
**Default:** `upload_store_access user:rw`<br>
**Context:** `server,location`

Specifies access mode which will be used to create output files.

### upload_set_form_field

**Syntax:** <code><b>upload_set_form_field</b> <i>name</i> <i>value</i></code><br>
**Default:** —<br>
**Context:** `server,location`

Specifies a form field(s) to generate for each uploaded file in request
body passed to backend. Both `name` and `value` could contain following
special variables:

  - `$upload_field_name`: the name of original file field
  - `$upload_content_type`: the content type of file uploaded
  - `$upload_file_name`: the original name of the file being uploaded
    with leading path elements in DOS and UNIX notation stripped. I.e.
    "D:\\Documents And Settings\\My Dcouments\\My Pictures\\Picture.jpg"
    will be converted to "Picture.jpg" and "/etc/passwd" will be
    converted to "passwd".
  - `$upload_tmp_path`: the path where the content of original file is
    being stored to. The output file name consists 10 digits and
    generated with the same algorithm as in `proxy_temp_path`
    directive.

These variables are valid only during processing of one part of original
request body.

Usage example:

```nginx
upload_set_form_field $upload_field_name.name "$upload_file_name";
upload_set_form_field $upload_field_name.content_type "$upload_content_type";
upload_set_form_field $upload_field_name.path "$upload_tmp_path";
```

### upload_aggregate_form_field

**Syntax:** <code><b>upload_aggregate_form_field</b> <i>name</i> <i>value</i></code><br>
**Default:** —<br>
**Context:** `server,location`

Specifies a form field(s) containing aggregate attributes to generate
for each uploaded file in request body passed to backend. Both name and
value could contain standard nginx variables, variables from
[upload_set_form_field](#upload_set_form_field) directive and
following additional special variables:

  - `$upload_file_md5`: MD5 checksum of the file
  - `$upload_file_md5_uc`: MD5 checksum of the file in uppercase letters
  - `$upload_file_sha1`: SHA1 checksum of the file
  - `$upload_file_sha1_uc`: SHA1 checksum of the file in uppercase letters
  - `$upload_file_crc32`: hexdecimal value of CRC32 of the file
  - `$upload_file_size`: size of the file in bytes
  - `$upload_file_number`: ordinal number of file in request body

The value of a field specified by this directive is evaluated after
successful upload of the file, thus these variables are valid only at
the end of processing of one part of original request body.

**Warning:**: variables `$upload_file_md5`, `$upload_file_md5_uc`,
`$upload_file_sha1`, and `$upload_file_sha1_uc` use additional
resources to calculate MD5 and SHA1 checksums.

Usage example:

```nginx
upload_aggregate_form_field $upload_field_name.md5 "$upload_file_md5";
upload_aggregate_form_field $upload_field_name.size "$upload_file_size";

```

### upload_pass_form_field

**Syntax:** <code><b>upload_pass_form_field</b> <i>regex</i></code><br>
**Default:** —<br>
**Context:** `server,location`

Specifies a regex pattern for names of fields which will be passed to
backend from original request body. This directive could be specified
multiple times per location. Field will be passed to backend as soon as
first pattern matches. For PCRE-unaware enviroments this directive
specifies exact name of a field to pass to backend. If directive is
omitted, no fields will be passed to backend from client.

Usage example:

```nginx
upload_pass_form_field "^submit$|^description$";
```

For PCRE-unaware environments:

```nginx
upload_pass_form_field "submit";
upload_pass_form_field "description";

```

### upload_cleanup

**Syntax:** <code><b>upload_cleanup</b> <i>status/range</i> ...</code><br>
**Default:** —<br>
**Context:** `server,location`

Specifies HTTP statuses after generation of which all file successfuly
uploaded in current request will be removed. Used for cleanup after
backend or server failure. Backend may also explicitly signal errornous
status if it doesn't need uploaded files for some reason. HTTP status
must be a numerical value in range 400-599, no leading zeroes are
allowed. Ranges of statuses could be specified with a dash.

Usage example:

```nginx
upload_cleanup 400 404 499 500-505;
```

### upload_buffer_size

**Syntax:** <code><b>upload_buffer_size</b> <i>size</i></code><br>
**Default:** size of memory page in bytes<br>
**Context:** `server,location`

Size in bytes of write buffer which will be used to accumulate file data
and write it to disk. This directive is intended to be used to
compromise memory usage vs. syscall rate.

### upload_max_part_header_len

**Syntax:** <code><b>upload_max_part_header_len</b> <i>size</i></code><br>
**Default:** `512`<br>
**Context:** `server,location`

Specifies maximal length of part header in bytes. Determines the size of
the buffer which will be used to accumulate part headers.

### upload_max_file_size

**Syntax:** <code><b>upload_max_file_size</b> <i>size</i></code><br>
**Default:** `0`<br>
**Context:** `main,server,location`

Specifies maximal size of the file. Files longer than the value of this
directive will be omitted. This directive specifies "soft" limit, in the
sense, that after encountering file longer than specified limit, nginx
will continue to process request body, trying to receive remaining
files. For "hard" limit `client_max_body_size` directive must be
used. The value of zero for this directive specifies that no
restrictions on file size should be applied.

### upload_limit_rate

**Syntax:** <code><b>upload_limit_rate</b> <i>rate</i></code><br>
**Default:** `0`<br>
**Context:** `main,server,location`

Specifies upload rate limit in bytes per second. Zero means rate is
unlimited.

### upload_max_output_body_len

**Syntax:** <code><b>upload_max_output_body_len</b> <i>size</i></code><br>
**Default:** `100k`<br>
**Context:** `main,server,location`

Specifies maximal length of the output body. This prevents piling up of
non-file form fields in memory. Whenever output body overcomes specified
limit error 413 (Request entity too large) will be generated. The value
of zero for this directive specifies that no restrictions on output body
length should be applied.

### upload_tame_arrays

**Syntax:** <code><b>upload_tame_arrays</b> on | off</code><br>
**Default:** `off`<br>
**Context:** `main,server,location`

Specifies whether square brackets in file field names must be dropped
(required for PHP arrays).

### upload_pass_args

**Syntax:** <code><b>upload_pass_args</b> on | off</code><br>
**Default:** `off`<br>
**Context:** `main,server,location`

Enables forwarding of query arguments to location, specified by
[upload_pass](#upload_pass). Ineffective with named locations. Example:

```html
<form action="/upload/?id=5">
<!-- ... -->
```

```nginx
location /upload/ {
    upload_pass /internal_upload/;
    upload_pass_args on;
}

## ...

location /internal_upload/ {
    # ...
    proxy_pass http://backend;
}
```

In this example backend gets request URI "/upload?id=5". In case of
`upload_pass_args off` backend gets "/upload".

## Example configuration

```nginx
server {
    client_max_body_size 100m;
    listen 80;

    # Upload form should be submitted to this location
    location /upload/ {
        # Pass altered request body to this location
        upload_pass @test;

        # Store files to this directory
        # The directory is hashed, subdirectories 0 1 2 3 4 5 6 7 8 9 should exist
        upload_store /tmp 1;

        # Allow uploaded files to be read only by user
        upload_store_access user:r;

        # Set specified fields in request body
        upload_set_form_field $upload_field_name.name "$upload_file_name";
        upload_set_form_field $upload_field_name.content_type "$upload_content_type";
        upload_set_form_field $upload_field_name.path "$upload_tmp_path";

        # Inform backend about hash and size of a file
        upload_aggregate_form_field "$upload_field_name.md5" "$upload_file_md5";
        upload_aggregate_form_field "$upload_field_name.size" "$upload_file_size";

        upload_pass_form_field "^submit$|^description$";

        upload_cleanup 400 404 499 500-505;
    }

    # Pass altered request body to a backend
    location @test {
        proxy_pass http://localhost:8080;
    }
}
```

```html
<form name="upload" method="POST" enctype="multipart/form-data" action="/upload/">
<input type="file" name="file1">
<input type="file" name="file2">
<input type="hidden" name="test" value="value">
<input type="submit" name="submit" value="Upload">
</form>
```

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-upload](https://github.com/fdintino/nginx-upload-module){target=_blank}.

# *upstream-fair*: The fair load balancer module for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-upstream-fair
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_upstream_fair_module.so;
```


This document describes nginx-module-upstream-fair [v0.1.3](https://github.com/itoffshore/nginx-upstream-fair/releases/tag/0.1.3){target=_blank} 
released on Oct 03 2017.

<hr />
Nginx Upstream Fair Proxy Load Balancer

## **( compatible with nginx 1.11.6+ & with dynamic module capability ) **

## Description:

The Nginx fair proxy balancer enhances the standard round-robin load balancer provided
with Nginx so that it will track busy back end servers (e.g. Thin, Ebb, Mongrel)
and balance the load to non-busy server processes.

Further information can be found on http://nginx.localdomain.pl/

Ezra Zygmuntowicz has a good writeup of the fair proxy load balancer and how to use it here:
http://brainspl.at/articles/2007/11/09/a-fair-proxy-balancer-for-nginx-and-mongrel


## Usage:

Change your Nginx config file's upstream block to include the 'fair' directive:

upstream mongrel {
    fair;
    server 127.0.0.1:5000;
    server 127.0.0.1:5001;
    server 127.0.0.1:5002;
  }


If you encounter any issues, please report them using the bugtracker at
http://nginx.localdomain.pl/

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-upstream-fair](https://github.com/itoffshore/nginx-upstream-fair){target=_blank}.

# *upstream-jdomain*: Asynchronous domain name resolution module for NGINX upstream


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-upstream-jdomain
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_upstream_jdomain_module.so;
```


This document describes nginx-module-upstream-jdomain [v1.4.0](https://github.com/nicholaschiasson/ngx_upstream_jdomain/releases/tag/1.4.0){target=_blank} 
released on May 29 2022.

<hr />

An asynchronous domain name resolution module for nginx upstream.

This module allows you to use a domain name in an upstream block and expect the
domain name to be dynamically resolved so your upstream may be resilient to DNS
entry updates.

The module does not perform DNS resolution automatically on some interval.
Instead, the DNS resolution needs to be prompted by a request for the given
upstream. If nginx serves a connection bound for a jdomain upstream, and the
configured `interval` has elapsed, then the module will perform a DNS lookup.

The module is compatible with other `upstream` scope directives. This means you
may populate an `upstream` block with multiple `jdomain` directives, multiple
`server` directives, `keepalive`, load balancing directives, etc. Note that
unless another load balancing method is specified in the `upstream` block, this
module makes use of the default round robin load balancing algorithm built into
nginx core.

**Important Note**: Should an alternate load balancing algorithm be specified,
**it must come _before_ the jdomain directive in the upstream block!** If this
is not followed, nginx **_will_** crash during runtime! This is because many
other load balancing modules explicitly extend the built in round robin, and
thus end up clobbering the jdomain initialization handlers, since jdomain is
technically a load balancer module as well. While this may not be the case with
all load balancer modules, it's better to stay on the safe side and place
jdomain after.

**Important Note**: Due to the non blocking nature of this module and the fact
that its DNS resolution is triggered by incoming requests, the request that
prompts a lookup will actually still be forwarded to the upstream that was
resolved and cached before the DNS lookup happens. Depending on the scenario,
this could result in a one off failure when changing the states of
upstreams. This is important to keep in mind to ensure graceful transitions of
your upstreams.

This repository is a fork of [a repository](https://github.com/wdaike/ngx_upstream_jdomain)
originally authored by [wdaike](https://github.com/wdaike). As that project is
no longer maintained, this repository aims to be its successor and is now
several features ahead.

## Usage

```nginx
resolver 8.8.8.8; # Your Local DNS Server

## Basic upstream using domain name defaulting to port 80.
upstream backend_01 {
	jdomain example.com;
}

## Basic upstream specifying different port.
upstream backend_02 {
	jdomain example.com port=8080;
}

## Upstream with a backup server to use in case of host not found or format
## errors on DNS resolution.
upstream backend_03 {
	server 127.0.0.2 backup;
	jdomain example.com;
}

## Upstream which will use backup for any and all DNS resolution errors.
upstream backend_04 {
	server 127.0.0.2 backup;
	jdomain example.com strict;
}

server {
	listen 127.0.0.2:80;
	return 502 'An error.';
}
```

## Synopsis

```
Syntax: jdomain <domain-name> [port=80] [max_ips=4] [interval=1] [strict]
Context: upstream
Attributes:
	port:       Backend's listening port.                                      (Default: 80)
	max_ips:    IP buffer size. Maximum number of resolved IPs to cache.       (Default: 4)
	interval:   How many seconds to resolve domain name.                       (Default: 1)
	ipver:      Only addresses of family IPv4 or IPv6 will be used if defined  (Default: 0)
	strict:     Require the DNS resolution to succeed and return addresses,
	            otherwise marks the underlying server and peers as down and
	            forces use of other servers in the upstream block if there
	            are any present. A failed resolution can be a timeout, DNS
	            server failure, connection refusals, response with no
	            addresses, etc.
```

See https://www.nginx.com/resources/wiki/modules/domain_resolve/ for details.

## Development

### Prerequisites

To facilitate local development and enable you to build and test the module,
you'll need some tools.

- **[Docker](https://docs.docker.com/get-docker/)**: to provide an environment
	to easily reproduce ideal conditions for building and testing.
- **[act](https://github.com/nektos/act#installation)**: to simulate executing
	github actions workflows locally to save you from pushing commits just to
	watch the CI fail.
- **[rust](https://www.rust-lang.org/tools/install)**: dependency of
	`cargo-make`.
- **[cargo-make](https://sagiegurari.github.io/cargo-make/#installation)**: to
	run common development tasks such as building, testing, and formatting code.

### Task Runner

`cargo-make` is an advanced task runner that will enabled you to easily perform
common development operations like formatting the code, building the module,
running the test suite, and running code analysis. You can see the task
definitions in the file `Makefile.toml`. Installing `cargo-make` will result in
a standalone executable called `makers` as well as a `cargo` extension which
can be executed via `cargo make`. As this project is not a `rust` crate, it is
recommended to simply use `makers`.

Also note that for simplicity's sake, the task runner uses docker to run all
tasks. This means the build binary is not targetting your host platform.

#### Default Task

To add value, the default task (ie. simply running `makers` alone) will begin
an interactive bash session inside the docker container used for this project.

This should help with debugging and general workflow.

#### Formatting

Incorrectly formatted code will cause the github actions linting job to fail.
To avoid this, you can run the format task before pushing new changes, like so:

```bash
makers format
```

This formatting is performed by a tool called `clang-format`. You can find the
config options for this defined in the file `./.clang-format`.

#### Static Code Analysis

You can run a static analysis on the code via the analyse task:

```bash
makers analyse
```

This analysis is performed by a tool called `clang-tidy`. You can find the
config options for this defined in the file `./.clang-tidy`.

#### Testing

You can run the test suite using the test task, like so:

```bash
makers test
```

### Debugging

We can use `valgrind` and `gdb` on nginx from inside the container.

First open an interactive shell in the container with:

```bash
$ makers
```

We'll use that session to run `valgrind`:

```bash
$ valgrind --vgdb=full --vgdb-error=0 /github/workspace/bin/static/nginx -p/github/workspace/t/servroot -cconf/nginx.conf
==15== Memcheck, a memory error detector
==15== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==15== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==15== Command: /github/workspace/bin/static/nginx -p/github/workspace/t/servroot -cconf/nginx.conf
==15==
==15== (action at startup) vgdb me ...
==15==
==15== TO DEBUG THIS PROCESS USING GDB: start GDB like this
==15==   /path/to/gdb /github/workspace/bin/static/nginx
==15== and then give GDB the following command
==15==   target remote | /usr/lib64/valgrind/../../bin/vgdb --pid=15
==15== --pid is optional if only one valgrind process is running
==15==
```

Next, find the container identifier so we can open another session inside it:

```bash
$ docker ps
CONTAINER ID        IMAGE                                     COMMAND             CREATED             STATUS              PORTS                    NAMES
55fab1e069ba        act-github-actions-nginx-module-toolbox   "bash"              4 seconds ago       Up 3 seconds        0.0.0.0:1984->1984/tcp   serene_newton
```

Use either the name or ID to execute a bash session inside the container:

```bash
$ docker exec -it serene_newton bash
```

We'll use this session to start `gdb` and target the valgrind gdb server we started in the other session:

```bash
$ gdb /github/workspace/bin/static/nginx
GNU gdb (GDB) Red Hat Enterprise Linux 8.0.1-30.amzn2.0.3
Copyright (C) 2017 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-redhat-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
<http://www.gnu.org/software/gdb/documentation/>.
For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from /github/workspace/bin/static/nginx...done.
(gdb)
```

From the gdb prompt, target the valgrind process and begin debugging:

```bash
(gdb) target remote | /usr/lib64/valgrind/../../bin/vgdb --pid=15
Remote debugging using | /usr/lib64/valgrind/../../bin/vgdb --pid=15
relaying data between gdb and process 15
warning: remote target does not support file transfer, attempting to access files from local filesystem.
Reading symbols from /lib64/ld-linux-x86-64.so.2...(no debugging symbols found)...done.
0x0000000004000ef0 in _start () from /lib64/ld-linux-x86-64.so.2
Missing separate debuginfos, use: debuginfo-install glibc-2.26-35.amzn2.x86_64
(gdb)
```

### Running GitHub Actions

With `act`, you can simulate the workflow that will run on GitHub servers once
you push changes.

There is more than one job in the main workflow, so you need to specify the
test job when you run `act`. For example, you can use this command to run the
code format validation:

```shell
act -vj lint
```

Note that the `lint` job does not format your code, it only checks that the
formatting is as expected.

Also note that `-v` is used to enable verbose mode to give more visibility on
everything `act` is doing.

The jobs you can (and should) run locally are `lint`, `build`, `analyse`, and
`test`. The `test` job depends on the output from the `build` job. To keep the
output from the build job, you can add the `-b` flag to `act`, or you may
simply use the task runner to build.

### Known Issues

At the moment? None! 🎉

If you discover a bug or have a question to raise, please
[open an issue](https://github.com/nicholaschiasson/ngx_upstream_jdomain/issues/new/choose).

## Original Author

wdaike <wdaike@163.com> (https://github.com/wdaike), Baidu Inc.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-upstream-jdomain](https://github.com/nicholaschiasson/ngx_upstream_jdomain){target=_blank}.

# *upsync*: NGINX module for syncing upstreams from consul or etcd


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-upsync
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_upsync_module.so;
```


This document describes nginx-module-upsync [v2.1.3](https://github.com/weibocom/nginx-upsync-module/releases/tag/v2.1.3){target=_blank} 
released on Nov 20 2020.

<hr />

Nginx C module, which can sync upstreams from Consul or others. It dynamically modifies backend-servers attributes (weight, max_fails,...), without need to reload NGINX.

It may not always be convenient to modify configuration files and restart NGINX. For example, if you are experiencing large amounts of traffic and high load, restarting NGINX and reloading the configuration at that point further increases load on the system and can temporarily degrade performance.

The module allows to expand and scale down without affecting performance.

Another module, [nginx-stream-upsync-module](https://github.com/xiaokai-wang/nginx-stream-upsync-module) supports NGINX stream module (TCP protocol), please be noticed.

## Status

This module is still under active development and is considered production ready.

## Synopsis

nginx-consul:
```nginx-consul
http {
    upstream test {
        upsync 127.0.0.1:8500/v1/kv/upstreams/test/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
        upsync_dump_path /usr/local/nginx/conf/servers/servers_test.conf;

        include /usr/local/nginx/conf/servers/servers_test.conf;
    }

    upstream bar {
        server 127.0.0.1:8090 weight=1 fail_timeout=10 max_fails=3;
    }

    server {
        listen 8080;

        location = /proxy_test {
            proxy_pass http://test;
        }

        location = /bar {
            proxy_pass http://bar;
        }

        location = /upstream_show {
            upstream_show;
        }

    }
}
```
nginx-etcd:
```nginx-etcd
http {
    upstream test {
        upsync 127.0.0.1:2379/v2/keys/upstreams/test upsync_timeout=6m upsync_interval=500ms upsync_type=etcd strong_dependency=off;
        upsync_dump_path /usr/local/nginx/conf/servers/servers_test.conf;

        include /usr/local/nginx/conf/servers/servers_test.conf;
    }

    upstream bar {
        server 127.0.0.1:8090 weight=1 fail_timeout=10 max_fails=3;
    }

    server {
        listen 8080;

        location = /proxy_test {
            proxy_pass http://test;
        }

        location = /bar {
            proxy_pass http://bar;
        }

        location = /upstream_show {
            upstream_show;
        }

    }
}
```
upsync_lb:
```upsync_lb
http {
    upstream test {
        least_conn; //hash $uri consistent;

        upsync 127.0.0.1:8500/v1/kv/upstreams/test/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
        upsync_dump_path /usr/local/nginx/conf/servers/servers_test.conf;
        upsync_lb least_conn; //hash_ketama;

        include /usr/local/nginx/conf/servers/servers_test.conf;
    }

    upstream bar {
        server 127.0.0.1:8090 weight=1 fail_timeout=10 max_fails=3;
    }

    server {
        listen 8080;

        location = /proxy_test {
            proxy_pass http://test;
        }

        location = /bar {
            proxy_pass http://bar;
        }

        location = /upstream_show {
            upstream_show;
        }

    }
}
```

NOTE: recomending strong_dependency is configed off and the first time included file include all the servers.

## Description

This module provides a method to discover backend servers. Supporting dynamicly adding or deleting backend server through consul or etcd and dynamically adjusting backend servers weight, module will timely pull new backend server list from consul or etcd to upsync nginx ip router. Nginx needn't reload. Having some advantages than others:

* timely

      module send key to consul/etcd with index, consul/etcd will compare it with its index, if index doesn't change connection will hang five minutes, in the period any operation to the key-value, will feed back rightaway.

* performance

      Pulling from consul/etcd equal a request to nginx, updating ip router nginx needn't reload, so affecting nginx performance is little.

* stability

      Even if one pulling failed, it will pull next upsync_interval, so guarantying backend server stably provides service. And support dumping the latest config to location, so even if consul/etcd hung up, and nginx can be reload anytime. 

* health_check

      nginx-upsync-module support adding or deleting servers health check, needing nginx_upstream_check_module. Recommending nginx-upsync-module + nginx_upstream_check_module.

## Directives

## upsync
```
syntax: upsync $consul/etcd.api.com:$port/v1/kv/upstreams/$upstream_name/ [upsync_type=consul/etcd] [upsync_interval=second/minutes] [upsync_timeout=second/minutes] [strong_dependency=off/on]
```
default: none, if parameters omitted, default parameters are upsync_interval=5s upsync_timeout=6m strong_dependency=off

context: upstream

description: Pull upstream servers from consul/etcd... .

The parameters' meanings are:

* upsync_interval

    pulling servers from consul/etcd interval time.

* upsync_timeout

    pulling servers from consul/etcd request timeout.

* upsync_type

    pulling servers from conf server type.

* strong_dependency

    when strong_dependency is on, nginx will pull servers from consul/etcd every time when nginx start up or reload.


## upsync_dump_path
`syntax: upsync_dump_path $path`

default: /tmp/servers_$host.conf

context: upstream

description: dump the upstream backends to the $path.


## upsync_lb
`syntax: upsync_lb $load_balance`

default: round_robin/ip_hash/hash modula

context: upstream

description: mainly for least_conn and hash consistent, when using one of them, you must point out using upsync_lb.


## upstream_show
`syntax: upstream_show`

default: none

context: upstream

description: Show specific upstream all backend servers.

```configure
     location /upstream_list {
         upstream_show;
     }
```

```request1
curl http://127.0.0.1:8500/upstream_list?test;
```

```request2
curl http://127.0.0.1:8500/upstream_list;

show all upstreams.
```


## Consul_interface

Data can be taken from key/value store or service catalog. In the first case parameter upsync_type of directive must be *consul*. For example

```nginx-consul
        upsync 127.0.0.1:8500/v1/kv/upstreams/test upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
```

In the second case it must be *consul_services*.

```nginx-consul
        upsync 127.0.0.1:8500/v1/catalog/service/test upsync_timeout=6m upsync_interval=500ms upsync_type=consul_services strong_dependency=off;
```

In the third case, it must be *consul_health*:

```nginx-consul
        upsync 127.0.0.1:8500/v1/health/service/test upsync_timeout=6m upsync_interval=500ms upsync_type=consul_health strong_dependency=off;
```

Services with failing health checks are marked as down with the health api.

You can add or delete backend server through consul_ui or http_interface. Below are examples for key/value store.

http_interface example:

* add
```
    curl -X PUT http://$consul_ip:$port/v1/kv/upstreams/$upstream_name/$backend_ip:$backend_port
```
    default: weight=1 max_fails=2 fail_timeout=10 down=0 backup=0;

```
    curl -X PUT -d "{\"weight\":1, \"max_fails\":2, \"fail_timeout\":10}" http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
or
    curl -X PUT -d '{"weight":1, "max_fails":2, "fail_timeout":10}' http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
```
    value support json format.

* delete
```
    curl -X DELETE http://$consul_ip:$port/v1/kv/upstreams/$upstream_name/$backend_ip:$backend_port
```

* adjust-weight
```
    curl -X PUT -d "{\"weight\":2, \"max_fails\":2, \"fail_timeout\":10}" http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
or
    curl -X PUT -d '{"weight":2, "max_fails":2, "fail_timeout":10}' http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
```

* mark server-down
```
    curl -X PUT -d "{\"weight\":2, \"max_fails\":2, \"fail_timeout\":10, \"down\":1}" http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
or
    curl -X PUT -d '{"weight":2, "max_fails":2, "fail_timeout":10, "down":1}' http://$consul_ip:$port/v1/kv/$dir1/$upstream_name/$backend_ip:$backend_port
```

* check
```
    curl http://$consul_ip:$port/v1/kv/upstreams/$upstream_name?recurse
```


## Etcd_interface

you can add or delete backend server through http_interface.

mainly like etcd, http_interface example:

* add
```
    curl -X PUT http://$etcd_ip:$port/v2/keys/upstreams/$upstream_name/$backend_ip:$backend_port
```
    default: weight=1 max_fails=2 fail_timeout=10 down=0 backup=0;

```
    curl -X PUT -d value="{\"weight\":1, \"max_fails\":2, \"fail_timeout\":10}" http://$etcd_ip:$port/v2/keys/$dir1/$upstream_name/$backend_ip:$backend_port
```
    value support json format.

* delete
```
    curl -X DELETE http://$etcd_ip:$port/v2/keys/upstreams/$upstream_name/$backend_ip:$backend_port
```

* adjust-weight
```
    curl -X PUT -d "{\"weight\":2, \"max_fails\":2, \"fail_timeout\":10}" http://$etcd_ip:$port/v2/keys/$dir1/$upstream_name/$backend_ip:$backend_port
```

* mark server-down
```
    curl -X PUT -d value="{\"weight\":2, \"max_fails\":2, \"fail_timeout\":10, \"down\":1}" http://$etcd_ip:$port/v2/keys/$dir1/$upstream_name/$backend_ip:$backend_port
```

* check
```
    curl http://$etcd_ip:$port/v2/keys/upstreams/$upstream_name
```


## Check_module

check module support.

check-conf:
```check-conf
http {
    upstream test {
        upsync 127.0.0.1:8500/v1/kv/upstreams/test/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
        upsync_dump_path /usr/local/nginx/conf/servers/servers_test.conf;

        check interval=1000 rise=2 fall=2 timeout=3000 type=http default_down=false;
        check_http_send "HEAD / HTTP/1.0\r\n\r\n";
        check_http_expect_alive http_2xx http_3xx;

    }

    upstream bar {
        server 127.0.0.1:8090 weight=1 fail_timeout=10 max_fails=3;
    }

    server {
        listen 8080;

        location = /proxy_test {
            proxy_pass http://test;
        }

        location = /bar {
            proxy_pass http://bar;
        }

        location = /upstream_show {
            upstream_show;
        }

        location = /upstream_status {
            check_status;
            access_log off;
        }

    }
}
```


## Code style

Code style is mainly based on [style](http://tengine.taobao.org/book/appendix_a.html)


## see also
* the nginx_upstream_check_module: https://github.com/alibaba/tengine/blob/master/src/http/ngx_http_upstream_check_module.c
* the nginx_upstream_check_module patch: https://github.com/yaoweibin/nginx_upstream_check_module
* or based on https://github.com/xiaokai-wang/nginx_upstream_check_module


## source dependency
* Cjson: https://github.com/kbranigan/cJSON
* http-parser: https://github.com/nodejs/http-parser


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-upsync](https://github.com/weibocom/nginx-upsync-module){target=_blank}.

# *vod*: NGINX-based VOD Packager


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-vod
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_vod_module.so;
```


This document describes nginx-module-vod [v1.32](https://github.com/kaltura/nginx-vod-module/releases/tag/1.32){target=_blank} 
released on Oct 17 2023.

<hr />
## nginx-vod-module [![Build Status](https://travis-ci.org/kaltura/nginx-vod-module.svg?branch=master)](https://travis-ci.org/kaltura/nginx-vod-module)

[Join the list of organizations using this video packager project](https://github.com/kaltura/nginx-vod-module/issues/730/).

For live video streaming, please use [Media-Framework](https://github.com/kaltura/media-framework/).

### Features

* On-the-fly repackaging of MP4 files to DASH, HDS, HLS, MSS

* Working modes:
  1. Local - serve locally accessible files (local disk/NFS mounted)
  2. Remote - serve files accessible via HTTP using range requests
  3. Mapped - serve files according to a specification encoded in JSON format. The JSON can pulled from a remote server, or read from a local file

* Adaptive bitrate support

* Playlist support (playing several different media files one after the other) - mapped mode only

* Simulated live support (generating a live stream from MP4 files) - mapped mode only

* Fallback support for file not found in local/mapped modes (useful in multi-datacenter environments)
  
* Video codecs: H264, H265 (DASH/HLS), AV1 (DASH/HLS), VP8 (DASH), VP9 (DASH)

* Audio codecs: AAC, MP3 (HLS/HDS/MSS), AC-3 (DASH/HLS), E-AC-3 (DASH/HLS), VORBIS (DASH), OPUS (DASH), FLAC (HLS), DTS (HLS)

* Captions support - 
  
  Input:
  1. WebVTT
  2. SRT
  3. DFXP/TTML
  4. CAP (Cheetah)
  
  Output:
  1. DASH - either a single WebVTT or SMPTE-TT segments (configurable)
  2. HLS - segmented WebVTT (m3u8)
  3. MSS - converted to TTML and packaged in fragmented MP4 (no support for styling)

* Audio only/video only files

* Alternative audio renditions - supporting both:
  1. Generation of manifest with different audio renditions, allowing selection on the client side
  2. Muxing together audio and video streams from separate files / tracks - provides the ability
	to serve different audio renditions of a single video, without the need for any special support
	on the client side.

* Track selection for multi audio/video MP4 files

* Playback rate change - 0.5x up to 2x (requires libavcodec and libavfilter)

* Source file clipping (only from I-Frame to P-frame)

* Support for variable segment lengths - enabling the player to select the optimal bitrate fast,
without the overhead of short segments for the whole duration of the video

* Clipping of MP4 files for progressive download playback

* Thumbnail capture (requires libavcodec) and resize (requires libswscale)

* Volume map (requires libavcodec) - returns a CSV containing the volume level in each interval

* Decryption of CENC-encrypted MP4 files (it is possible to create such files with MP4Box)

* DASH: common encryption (CENC) support

* MSS: PlayReady encryption support

* HLS: Generation of I-frames playlist (EXT-X-I-FRAMES-ONLY)

* HLS: support for AES-128 / SAMPLE-AES encryption

### Limitations

* Track selection and playback rate change are not supported in progressive download

* I-frames playlist generation is not supported when encryption is enabled

* Tested on Linux only

## rpm -ihv http://installrepo.kaltura.org/releases/kaltura-release.noarch.rpm
## yum install kaltura-nginx
```

#### Debian/Ubuntu deb package
*Ubuntu NOTE: before trying to install kaltura-nginx, you must also make sure the multiverse repo is enabled*

For Debian Wheezy [7], Debian Jessie [8], Ubuntu 14.04 and 14.10, add this repo:
```sh
## wget -O - http://installrepo.kaltura.org/repo/apt/debian/kaltura-deb-curr.gpg.key|apt-key add -
## echo "deb [arch=amd64] http://installrepo.kaltura.org/repo/apt/debian propus main" > /etc/apt/sources.list.d/kaltura.list
```

For Ubuntu 16.04, 16.10 add this repo:
```sh
## wget -O - http://installrepo.kaltura.org/repo/apt/xenial/kaltura-deb-curr-256.gpg.key|apt-key add -
## echo "deb [arch=amd64] http://installrepo.kaltura.org/repo/apt/xenial propus main" > /etc/apt/sources.list.d/kaltura.list
```

For Ubuntu 20.04 add this repo:
```sh
## wget -O - http://installrepo.kaltura.org/repo/aptn/focal/kaltura-deb-256.gpg.key|apt-key add -
## echo "deb [arch=amd64] http://installrepo.kaltura.org/repo/aptn/focal quasar main" > /etc/apt/sources.list.d/kaltura.list
```


Then install the kaltura-nginx package:
```sh
## apt-get update
## apt-get install kaltura-nginx
```


If you wish to make use of the following features:
- Thumbnail capture
- Playback rate change - 0.5x up to 2x

You will also need to install the kaltura-ffmpeg (>= 3.1) package.

### URL structure

#### Basic URL structure

The basic structure of an nginx-vod-module URL is:
`http://<domain>/<location>/<fileuri>/<filename>`

Where:
* domain - the domain of the nginx-vod-module server
* location - the location specified in the nginx conf
* fileuri - a URI to the mp4 file:
  * local mode - the full file path is determined according to the root / alias nginx.conf directives
  * mapped mode - the full file path is determined according to the JSON received from the upstream / local file
  * remote mode - the mp4 file is read from upstream in chunks
  * Note: in mapped & remote modes, the URL of the upstream request is `http://<upstream>/<location>/<fileuri>?<extraargs>`
  (extraargs is determined by the `vod_upstream_extra_args` parameter)
* filename - detailed below

#### Multi URL structure

Multi URLs are used to encode several URLs on a single URL. A multi URL can be used to specify
the URLs of several different MP4 files that should be included together in a DASH MPD for example.

The structure of a multi URL is:
`http://<domain>/<location>/<prefix>,<middle1>,<middle2>,<middle3>,<postfix>.urlset/<filename>`

The sample URL above represents 3 URLs:
* `http://<domain>/<location>/<prefix><middle1><postfix>/<filename>`
* `http://<domain>/<location>/<prefix><middle2><postfix>/<filename>`
* `http://<domain>/<location>/<prefix><middle3><postfix>/<filename>`

The suffix `.urlset` (can be changed using `vod_multi_uri_suffix`) indicates that the URL should be treated as a multi URL.
For example - the URL `http://example.com/hls/videos/big_buck_bunny_,6,9,15,00k.mp4.urlset/master.m3u8` will return a manifest containing:
* http://example.com/hls/videos/big_buck_bunny_600k.mp4/index.m3u8
* http://example.com/hls/videos/big_buck_bunny_900k.mp4/index.m3u8
* http://example.com/hls/videos/big_buck_bunny_1500k.mp4/index.m3u8

#### URL path parameters

The following parameters are supported on the URL path:
* clipFrom - an offset in milliseconds since the beginning of the video, where the generated stream should start. 
	For example, `.../clipFrom/10000/...` will generate a stream that starts 10 seconds into the video.
* clipTo - an offset in milliseconds since the beginning of the video, where the generated stream should end.
	For example, `.../clipTo/60000/...` will generate a stream truncated to 60 seconds.
* tracks - can be used to select specific audio/video tracks. The structure of the parameter is: `v<id1>-v<id2>-a<id1>-a<id2>...`
	For example, `.../tracks/v1-a1/...` will select the first video track and first audio track.
	The default is to include all tracks.
* shift - can be used to apply a timing shift to one or more streams. The structure of the parameter is: `v<vshift>-a<ashift>-s<sshift>`
	For example, `.../shift/v100/...` will apply a forward shift of 100ms to the video timestamps.

#### Filename structure

The structure of filename is:
`<basename>[<seqparams>][<fileparams>][<trackparams>][<langparams>].<extension>`

Where:
* basename + extension - the set of options is packager specific (the list below applies to the default settings):
  * dash - manifest.mpd
  * hds - manifest.f4m
  * hls master playlist - master.m3u8
  * hls media playlist - index.m3u8
  * mss - manifest
  * thumb - `thumb-<offset>[<resizeparams>].jpg` (offset is the thumbnail video offset in milliseconds)
  * volume_map - `volume_map.csv`
* seqparams - can be used to select specific sequences by id (provided in the mapping JSON), e.g. master-sseq1.m3u8.
* fileparams - can be used to select specific sequences by index when using multi URLs.
	For example, manifest-f1.mpd will return an MPD only from the first URL.
* trackparams - can be used to select specific audio/video tracks.
	For example, manifest-a1.f4m will return an F4M containing only the first audio stream of each sequence.
	The default is to include the first audio and first video tracks of each file.
	The tracks selected on the file name are AND-ed with the tracks selected with the /tracks/ path parameter.
	v0/a0 select all video/audio tracks respectively.
	The a/v parameters can be combined with f/s, e.g. f1-v1-f2-a1 = video1 of file1 + audio1 of file2, f1-f2-v1 = video1 of file1 + video1 of file2.
* langparams - can be used to filter audio tracks/subtitles according to their language (ISO639-3 code).
	For example, master-leng.m3u8 will return only english audio tracks.
* resizeparams - can be used to resize the returned thumbnail image. For example, thumb-1000-w150-h100.jpg captures a thumbnail
	1 second into the video, and resizes it to 150x100. If one of the dimensions is omitted, its value is set so that the 
	resulting image will retain the aspect ratio of the video frame.

### Mapping response format

When configured to run in mapped mode, nginx-vod-module issues an HTTP request to a configured upstream server 
in order to receive the layout of media streams it should generate.
The response has to be in JSON format. 

This section contains a few simple examples followed by a reference of the supported objects and fields. 
But first, a couple of definitions:

1. `Source Clip` - a set of audio and/or video frames (tracks) extracted from a single media file
2. `Generator` - a component that can generate audio/video frames. Currently, the only supported generator is the silence generator.
3. `Filter` - a manipulation that can be applied on audio/video frames. The following filters are supported: 
  * rate (speed) change - applies to both audio and video
  * audio volume change
  * mix - can be used to merge several audio tracks together, or to merge the audio of source A with the video of source B
4. `Clip` - the result of applying zero or more filters on a set of source clips
5. `Dynamic Clip` - a clip whose contents is not known in advance, e.g. targeted ad content
6. `Sequence` - a set of clips that should be played one after the other. 
7. `Set` - several sequences that play together as an adaptive set, each sequence must have the same number of clips.

#### Simple mapping

The JSON below maps the request URI to a single MP4 file:
```json
{
	"sequences": [
		{
			"clips": [
				{
					"type": "source",
					"path": "/path/to/video.mp4"
				}
			]
		}
	]
}
```

When using multi URLs, this is the only allowed JSON pattern. In other words, it is not
possible to combine more complex JSONs using multi URL.

#### Adaptive set

As an alternative to using multi URL, an adaptive set can be defined via JSON:
```json
{
	"sequences": [
		{
			"clips": [
				{
					"type": "source",
					"path": "/path/to/bitrate1.mp4"
				}
			]
		},
		{
			"clips": [
				{
					"type": "source",
					"path": "/path/to/bitrate2.mp4"
				}
			]
		}
	]
}
```

#### Playlist

The JSON below will play 35 seconds of video1 followed by 22 seconds of video2:
```json
{
	"durations": [ 35000, 22000 ],
	"sequences": [
		{
			"clips": [
				{
					"type": "source",
					"path": "/path/to/video1.mp4"
				},
				{
					"type": "source",
					"path": "/path/to/video2.mp4"
				}
			]
		}
	]
}
```

#### Filters

The JSON below takes video1, plays it at x1.5 and mixes the audio of the result with the audio of video2,
after reducing it to 50% volume:
```json
{
	"sequences": [
		{
			"clips": [
				{
					"type": "mixFilter",
					"sources": [
						{
							"type": "rateFilter",
							"rate": 1.5,
							"source": {
								"type": "source",
								"path": "/path/to/video1.mp4"
							}
						},
						{
							"type": "gainFilter",
							"gain": 0.5,
							"source": {
								"type": "source",
								"path": "/path/to/video2.mp4",
								"tracks": "a1"
							}
						}
					]
				}
			]
		}
	]
}
```

#### Continuous live

The JSON below is a sample of a continuous live stream (=a live stream in which all videos have exactly the same encoding parameters).
In practice, this JSON will have to be generated by some script, since it is time dependent.
(see test/playlist.php for a sample implementation)
```json
{
	"playlistType": "live",
	"discontinuity": false,
	"segmentBaseTime": 1451904060000,
	"firstClipTime": 1451917506000,
	"durations": [83000, 83000],
	"sequences": [
		{
			"clips": [
				{
					"type": "source",
					"path": "/path/to/video1.mp4"
				},
				{
					"type": "source",
					"path": "/path/to/video2.mp4"
				}
			]
		}
	]
}
```

#### Non-continuous live

The JSON below is a sample of a non-continuous live stream (=a live stream in which the videos have different encoding parameters).
In practice, this JSON will have to be generated by some script, since it is time dependent 
(see test/playlist.php for a sample implementation)
```json
{
	"playlistType": "live",
	"discontinuity": true,
	"initialClipIndex": 171,
	"initialSegmentIndex": 153,
	"firstClipTime": 1451918170000,
	"durations": [83000, 83000],
	"sequences": [
		{
			"clips": [
				{
					"type": "source",
					"path": "/path/to/video1.mp4"
				},
				{
					"type": "source",
					"path": "/path/to/video2.mp4"
				}
			]
		}
	]
}
```

### Mapping reference

#### Set (top level object in the mapping JSON)

Mandatory fields:
* `sequences` - array of Sequence objects. 
	The mapping has to contain at least one sequence and up to 32 sequences.
	
Optional fields:
* `id` - a string that identifies the set. The id can be retrieved by `$vod_set_id`.
* `playlistType` - string, can be set to `live`, `vod` or `event` (only supported for HLS playlists), default is `vod`.
* `durations` - an array of integers representing clip durations in milliseconds.
	This field is mandatory if the mapping contains more than a single clip per sequence.
	If specified, this array must contain at least one element and up to 128 elements.
* `discontinuity` - boolean, indicates whether the different clips in each sequence have
	different media parameters. This field has different manifestations according to the 
	delivery protocol - a value of true will generate `#EXT-X-DISCONTINUITY` in HLS, 
	and a multi period MPD in DASH. The default value is true, set to false only if the media
	files were transcoded with exactly the same parameters (in AVC for example, 
	the clips should have exactly the same SPS/PPS).
* `segmentDuration` - integer, sets the segment duration in milliseconds. This field, 
	if specified, takes priority over the value set in `vod_segment_duration`.
* `consistentSequenceMediaInfo` - boolean, currently affects only DASH. When set to true (default)
	the MPD will report the same media parameters in each period element. Setting to false
	can have severe performance implications for long sequences (nginx-vod-module has 
	to read the media info of all clips included in the mapping in order to generate the MPD)
* `referenceClipIndex` - integer, sets the (1-based) index of the clip that should be used 
	to retrieve the video metadata for manifest requests (codec, width, height etc.)
	If `consistentSequenceMediaInfo` is set to false, this parameter has no effect -
	all clips are parsed. If this parameter is not specified, nginx-vod-module uses the last clip 
	by default.
* `notifications` - array of notification objects (see below), when a segment is requested,
	all the notifications that fall between the start/end times of the segment are fired.
	the notifications must be ordered in an increasing offset order.
* `clipFrom` - integer, contains a timestamp indicating where the returned stream should start.
	Setting this parameter is equivalent to passing /clipFrom/ on the URL.
* `clipTo` - integer, contains a timestamp indicating where the returned stream should end.
	Setting this parameter is equivalent to passing /clipTo/ on the URL.
* `cache` - boolean, if set to false, the mapping response will not be saved to cache (vod_mapping_cache).
	The default value is true.
* `closedCaptions` - array of closed captions objects (see below), containing languages and ids
	of any embedded CEA-608 / CEA-708 captions. If an empty array is provided, the module will output
	`CLOSED-CAPTIONS=NONE` on each `EXT-X-STREAM-INF` tag. If the list does not appear in the JSON, the 
	module will not output any `CLOSED-CAPTIONS` fields in the playlist.
	
Live fields:
* `firstClipTime` - integer, mandatory for all live playlists unless `clipTimes` is specified.
	Contains the absolute time of the first clip in the playlist, in milliseconds since the epoch (unixtime x 1000)
* `clipTimes` - array of integers, sets the absolute time of all the clips in the playlist, 
	in milliseconds since the epoch (unixtime x 1000). This field can be used only when 
	`discontinuity` is set to true. The timestamps may contain gaps, but they are not allowed to overlap
	(`clipTimes[n + 1] >= clipTimes[n] + durations[n]`)
* `segmentBaseTime` - integer, mandatory for continuous live streams, contains the absolute
	time of the first segment of the stream, in milliseconds since the epoch (unixtime x 1000).
	This value must not change during playback.
	For discontinuous live streams, this field is optional:
	* if not set, sequential segment indexes will be used throughout the playlist.
		In this case, the upstream server generating the mapping json has to maintain state,
		and update initialSegmentIndex every time a clip is removed from the playlist.
	* if set, the timing gaps between clips must not be lower than `vod_segment_duration`.
* `firstClipStartOffset` - integer, optional, measured in milliseconds. This field contains the
	difference between first clip time, and the original start time of the first clip -
	the time it had when it was initially added (before the live window shifted)
* `initialClipIndex` - integer, mandatory for non-continuous live streams that mix videos having
	different encoding parameters (SPS/PPS), contains the index of the first clip in the playlist. 
	Whenever a clip is pushed out of the head of the playlist, this value must be incremented by one.
* `initialSegmentIndex` - integer, mandatory for live streams that do not set `segmentBaseTime`, 
	contains the index of the first segment in the playlist. Whenever a clip is pushed out of the head of
	the playlist, this value must be incremented by the number of segments in the clip.
* `presentationEndTime` - integer, optional, measured in milliseconds since the epoch.
	when supplied, the module will compare the current time to the supplied value, 
	and signal the end of the live presentation if `presentationEndTime` has passed. 
	In HLS, for example, this parameter controls whether an `#EXT-X-ENDLIST` tag should be 
	included in the media playlist.
	When the parameter is not supplied, the module will not signal live presentation end.
* `expirationTime` - integer, optional, measured in milliseconds since the epoch.
	when supplied, the module will compare the current time to the supplied value, 
	and if `expirationTime` has passed, the module will return a 404 error for manifest requests 
	(segment requests will continue to be served).
	when both presentationEndTime and expirationTime have passed, presentationEndTime takes
	priority, i.e. manifest requests will be served and signal presentation end.
* `liveWindowDuration` - integer, optional, provides a way to override `vod_live_window_duration`
	specified in the configuration. If the value exceeds the absolute value specified in 
	`vod_live_window_duration`, it is ignored.
* `timeOffset` - integer, sets an offset that should be applied to the server clock when serving
	live requests. This parameter can be used to test future/past events.
	
#### Sequence

Mandatory fields:
* `clips` - array of Clip objects (mandatory). The number of elements must match the number
	the durations array specified on the set. If the durations array is not specified,
	the clips array must contain a single element.
	
Optional fields:
* `id` - a string that identifies the sequence. The id can be retrieved by `$vod_sequence_id`.
* `language` - a 3-letter (ISO-639-2) language code, this field takes priority over any language
	specified on the media file (MP4 mdhd atom)
* `label` - a friendly string that identifies the sequence. If a language is specified,
	a default label will be automatically derived by it - e.g. if language is `ita`, 
	by default `italiano` will be used as the label.
* `bitrate` - an object that can be used to set the bitrate for the different media types,
	in bits per second. For example, `{"v": 900000, "a": 64000}`. If the bitrate is not supplied,
	nginx-vod-module will estimate it based on the last clip in the sequence.
* `avg_bitrate` - an object that can be used to set the average bitrate for the different media types,
	in bits per second. See `bitrate` above for a sample object. If specified, the module will use
	the value to populate the AVERAGE-BANDWIDTH attribute of `#EXT-X-STREAM-INF` in HLS.

#### Clip (abstract)

Mandatory fields:
* `type` - a string that defines the type of the clip. Allowed values are:
	* source
	* rateFilter
	* mixFilter
	* gainFilter
	* silence
	* concat
	* dynamic

Optional fields:
* `keyFrameDurations` - array of integers, containing the durations in milliseconds of the video key frames
	in the clip. This property can only be supplied on the top level clips of each sequence,
	supplying this property on nested clips has no effect.
	Supplying the key frame durations enables the module to both:
	1. align the segments to key frames 
	2. report the correct segment durations in the manifest - providing an alternative to setting
		`vod_manifest_segment_durations_mode` to `accurate`, which is not supported for multi clip
		media sets (for performance reasons).
* `firstKeyFrameOffset` - integer, offset of the first video key frame in the clip, 
	measured in milliseconds relative to `firstClipTime`. Defaults to 0 if not supplied.

#### Source clip

Mandatory fields:
* `type` - a string with the value `source`
* `path` - a string containing the path of the MP4 file. The string `"empty"` can be used to represent
	an empty captions file (useful in case only some videos in a playlist have captions)

Optional fields:
* `sourceType` - sets the interface that should be used to read the MP4 file, allowed values are:
	`file` and `http`. By default, the module uses `http` if `vod_remote_upstream_location` is set,
	and `file` otherwise.
* `tracks` - a string that specifies the tracks that should be used, the default is "v1-a1",
	which means the first video track and the first audio track
* `clipFrom` - an integer that specifies an offset in milliseconds, from the beginning of the 
	media file, from which to start loading frames
* `encryptionKey` - a base64 encoded string containing the key (128/192/256 bit) that should be used
	to decrypt the file.
* `encryptionIv` - a base64 encoded string containing the iv (128 bit) that should be used
	to decrypt the file.
* `encryptionScheme` - the encryption scheme that was used to encrypt the file. Currently,
	only two schemes are supported - `cenc` for MP4 files, `aes-cbc` for caption files.

#### Rate filter clip

Mandatory fields:
* `type` - a string with the value `rateFilter`
* `rate` - a float that specified the acceleration factor, e.g. a value of 2 means double speed.
	Allowed values are in the range 0.5 - 2 with up to two decimal points
* `source` - a clip object on which to perform the rate filtering

#### Gain filter clip

Mandatory fields:
* `type` - a string with the value `gainFilter`
* `gain` - a float that specified the amplification factor, e.g. a value of 2 means twice as loud.
	The gain must be positive with up to two decimal points
* `source` - a clip object on which to perform the gain filtering

#### Mix filter clip

Mandatory fields:
* `type` - a string with the value `mixFilter`
* `sources` - an array of Clip objects to mix. This array must contain at least one clip and
	up to 32 clips.

#### Concat clip

Mandatory fields:
* `type` - a string with the value `concat`
* `durations` - an array of integers representing MP4 durations in milliseconds,
	this array must match the `paths` array in count and order.

Optional fields:
* `paths` - an array of strings, containing the paths of the MP4 files. Either `paths` or `clipIds` must be specified.
* `clipIds` - an array of strings, containing the ids of source clips. 
	The ids are translated to paths by issuing a request to the uri specified in `vod_source_clip_map_uri`.
	Either `paths` or `clipIds` must be specified.
* `tracks` - a string that specifies the tracks that should be used, the default is "v1-a1",
	which means the first video track and the first audio track
* `offset` - an integer in milliseconds that indicates the timestamp offset of the 
	first frame in the concatenated stream relative to the clip start time
* `basePath` - a string that should be added as a prefix to all the paths
* `notifications` - array of notification objects (see below), when a segment is requested,
	all the notifications that fall between the start/end times of the segment are fired.
	the notifications must be ordered in an increasing offset order.

#### Dynamic clip

Mandatory fields:
* `type` - a string with the value `dynamic`
* `id` - a string that uniquely identifies the dynamic clip, used for mapping the clip to its content

#### Notification

Mandatory fields:
* `offset` - an integer in milliseconds that indicates the time in which the notification should be fired.
	when the notification object is contained in the media set, `offset` is relative to `firstClipTime`
	(0 for vod). when the notification object is contained in a concat clip, `offset` is relative to
	the beginning of the concat clip.
* `id` - a string that identifies the notification, this id can be referenced by `vod_notification_uri`
	using the variable `$vod_notification_id`

#### Closed Captions

Mandatory fields:
* `id` - a string that identifies the embedded captions. This will become the `INSTREAM-ID` field and must
have one of the following values: `CC1`, `CC3`, `CC3`, `CC4`, or `SERVICEn`, where `n` is between 1 and 63.
* `label` - a friendly string that indicates the language of the closed caption track.

Optional fields:
* `language` - a 3-letter (ISO-639-2) language code that indicates the language of the closed caption track.


### Security

#### Authorization

##### CDN-based delivery

Media packaged by nginx-vod-module can be protected using CDN tokens, this works as follows:
* Some application authenticates the user and decides whether the user should be allowed 
	to watch a specific video. If the user is allowed, the application generates a tokenized
	URL for the manifest of the video.
* The CDN validates the token, and if found to be valid, forwards the request to nginx-vod-module 
	on the origin. 
* The nginx server builds the manifest response and generates tokens for the segment URLs
	contained inside it. The module https://github.com/kaltura/nginx-secure-token-module can
	be used to accomplish this task, it currently support Akamai tokens and CloudFront tokens.
	See the readme of this module for more details.
* The CDN validates the token on each segment that is requested.

In this setup it also highly recommended to block direct access to the origin server by
authenticating the CDN requests. Without this protection, a user who somehow gets the address
of the origin will be able to bypass the CDN token enforcement. If using Akamai, this can
be accomplished using https://github.com/refractalize/nginx_mod_akamai_g2o.
For other CDNs, it may be possible to configure the CDN to send a secret header to the origin
and then simply enforce the header using an nginx if statement:
```c
		if ($http_x_secret_origin_header != "secret value") {
			return 403;
		}
```

In addition to the above, most CDNs support other access control settings, such as geo-location.
These restrictions are completely transparent to the origin and should work well. 

##### Direct delivery

Deployments in which the media is pulled directly from nginx-vod-module can protect the media
using nginx access control directives, such `allow`, `deny`, or `access_by_lua` (for more complex
scenarios).

In addition, it is possible to build a token based solution (as detailed in the previous section) 
without a CDN, by having the nginx server validate the token. 
The module https://github.com/kaltura/nginx-akamai-token-validate-module can be used
to validate Akamai tokens. Locations on which the module is enabled will return 403 unless the 
request contains a valid Akamai token. See the readme of this module for more details.

#### URL encryption

As an alternative to tokenization, URL encryption can be used to prevent an attacker from being
able to craft a playable URL. URL encryption can be implemented with 
https://github.com/kaltura/nginx-secure-token-module, and is supported for HLS and DASH (with 
manifest format set to segmentlist). 

In terms of security, the main advantage of CDN tokens over URL encryption is that CDN tokens
usually expire, while encrypted URLs do not (someone who obtains a playable URL will be able to
use it indefinitely)

#### Media encryption

Nginx-vod-module supports AES-128 and SAMPLE-AES HLS encryption schemes. The main difference between
media encryption and DRM (detailed below) is the mechanism used to transfer the encryption key to 
the client. With media encryption the key is fetched by the client by performing a simple GET request
to nginx-vod-module, while with DRM the key is returned inside a vendor specific license response.

Media encryption reduces the problem of securing the media to the need to secure the encryption key. 
The media segment URLs (which compose the vast majority of the traffic) can be completely unprotected, 
and easily cacheable by any proxies between the client and servers (unlike tokenization). 
The encryption key request can then be protected using one of the methods mentioned above (CDN tokens,
nginx access rules etc.). 

In addition, it is possible to configure nginx-vod-module to return the encryption key over HTTPS
while having the segments delivered over HTTP. The way to configure this is to set `vod_segments_base_url`
to `http://nginx-vod-host` and set `vod_base_url` to `https://nginx-vod-host`.

#### DRM

Nginx-vod-module has the ability to perform on-the-fly encryption for MPEG DASH (CENC), MSS Play Ready and FairPlay HLS.
As in the case of media encryption, the encryption is performed while serving a video/audio segment to the client, 
therefore, when working with DRM it is recommended not to serve the content directly from nginx-vod-module to end-users.
A more scalable architecture would be to use proxy servers or a CDN in order to cache the encrypted segments.

In order to perform the encryption, nginx-vod-module needs several parameters, including key & key_id, these parameters
are fetched from an external server via HTTP GET requests.
The `vod_drm_upstream_location` parameter specifies an nginx location that is used to access the DRM server,
and the request uri is configured using `vod_drm_request_uri` (this parameter can include nginx variables). 
The response of the DRM server is a JSON, with the following format:

```json
[{
	"pssh": [{
			"data": "CAESEGMyZjg2MTczN2NjNGYzODIaB2thbHR1cmEiCjBfbmptaWlwbXAqBVNEX0hE", 
			"uuid": "edef8ba9-79d6-4ace-a3c8-27dcd51d21ed"
		}], 
	"key": "GzoNU9Dfwc//Iq3/zbzMUw==", 
	"key_id": "YzJmODYxNzM3Y2M0ZjM4Mg=="
}]
```

* `pssh.data` - base64 encoded binary data, the format of this data is drm vendor specific
* `pssh.uuid` - the drm system UUID, in this case, edef8ba9-79d6-4ace-a3c8-27dcd51d21ed stands for Widevine
* `key` - base64 encoded encryption key (128 bit)
* `key_id` - base64 encoded key identifier (128 bit)
* `iv` - optional base64 encoded initialization vector (128 bit). The IV is currently used only in HLS (FairPlay), 
	in the other protocols an IV is generated automatically by nginx-vod-module.

##### Sample configurations

Apple FairPlay HLS:
```nginx
location ~ ^/fpshls/p/\d+/(sp/\d+/)?serveFlavor/entryId/([^/]+)/(.*) {
	vod hls;
	vod_hls_encryption_method sample-aes;
	vod_hls_encryption_key_uri "skd://entry-$2";
	vod_hls_encryption_key_format "com.apple.streamingkeydelivery";
	vod_hls_encryption_key_format_versions "1";

	vod_drm_enabled on;
	vod_drm_request_uri "/udrm/system/ovp/$vod_suburi";

	vod_last_modified_types *;
	add_header Access-Control-Allow-Headers '*';
	add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
	add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
	add_header Access-Control-Allow-Origin '*';
	expires 100d;
}
```

Common Encryption HLS:
```nginx
location ~ ^/cenchls/p/\d+/(sp/\d+/)?serveFlavor/entryId/([^/]+)/(.*) {
	vod hls;
	vod_hls_encryption_method sample-aes-cenc;
	vod_hls_encryption_key_format "urn:uuid:edef8ba9-79d6-4ace-a3c8-27dcd51d21ed";
	vod_hls_encryption_key_format_versions "1";

	vod_drm_enabled on;
	vod_drm_request_uri "/udrm/system/ovp/$vod_suburi";

	vod_last_modified_types *;
	add_header Access-Control-Allow-Headers '*';
	add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
	add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
	add_header Access-Control-Allow-Origin '*';
	expires 100d;
}
```

##### Verified configurations

Following is a list of configurations that were tested and found working:
* DASH/CENC with PlayReady & Widevine PSSH together
* MSS PlayReady
* HLS FairPlay

### Performance recommendations

1. For medium/large scale deployments, don't have users play the videos directly from nginx-vod-module.
	Since all the different streaming protocols supported by nginx vod are HTTP based, they can be cached by standard HTTP proxies / CDNs. 
	For medium scale add a layer of caching proxies between the vod module and the end users 
	(can use standard nginx servers with proxy_pass & proxy_cache). 
	For large scale deployments, it is recommended to use a CDN (such as Akamai, Level3 etc.). 
	
	In general, it's best to have nginx vod as close as possible to where the mp4 files are stored, 
	and have the caching proxies as close as possible to the end users.
2. Enable nginx-vod-module caches:
	* `vod_metadata_cache` - saves the need to re-read the video metadata for each segment. This cache should be rather large, in the order of GBs.
	* `vod_response_cache` - saves the responses of manifest requests. This cache may not be required when using a second layer of caching servers before nginx vod. 
		No need to allocate a large buffer for this cache, 128M is probably more than enough for most deployments.
	* `vod_mapping_cache` - for mapped mode only, few MBs is usually enough.
	* nginx's open_file_cache - caches open file handles.

	The hit/miss ratios of these caches can be tracked by enabling performance counters (`vod_performance_counters`)
	and setting up a status page for nginx vod (`vod_status`)
3. In local & mapped modes, enable aio. - nginx has to be compiled with aio support, and it has to be enabled in nginx conf (aio on). 
	You can verify it works by looking at the performance counters on the vod status page - read_file (aio off) vs. async_read_file (aio on)
4. In local & mapped modes, enable asynchronous file open - nginx has to be compiled with threads support, and `vod_open_file_thread_pool`
	has to be specified in nginx.conf. You can verify it works by looking at the performance counters on the vod status page - 
	open_file vs. async_open_file. Note that open_file may be nonzero with vod_open_file_thread_pool enabled, due to the open file cache - 
	open requests that are served from cache will be counted as synchronous open_file.
5. When using DRM enabled DASH/MSS, if the video files have a single nalu per frame, set `vod_min_single_nalu_per_frame_segment` to non-zero.
6. The muxing overhead of the streams generated by this module can be reduced by changing the following parameters:
	* HDS - set `vod_hds_generate_moof_atom` to off
	* HLS - set `vod_hls_mpegts_align_frames` to off and `vod_hls_mpegts_interleave_frames` to on
7. Enable gzip compression on manifest responses - 

	`gzip_types application/vnd.apple.mpegurl video/f4m application/dash+xml text/xml`
8. Apply common nginx performance best practices, such as tcp_nodelay=on, client_header_timeout etc.

### Configuration directives - base

#### vod
* **syntax**: `vod segmenter`
* **default**: `n/a`
* **context**: `location`

Enables the nginx-vod module on the enclosing location. 
The allowed values for `segmenter` are:

1. `none` - serves the MP4 files as is / clipped
2. `dash` - Dynamic Adaptive Streaming over HTTP packager
3. `hds` - Adobe HTTP Dynamic Streaming packager
4. `hls` - Apple HTTP Live Streaming packager
5. `mss` - Microsoft Smooth Streaming packager
6. `thumb` - thumbnail capture
7. `volume_map` - audio volume map

#### vod_mode
* **syntax**: `vod_mode mode`
* **default**: `local`
* **context**: `http`, `server`, `location`

Sets the file access mode - local, remote or mapped (see the features section above for more details)

#### vod_status
* **syntax**: `vod_status`
* **default**: `n/a`
* **context**: `location`

Enables the nginx-vod status page on the enclosing location. 
The following query params are supported:
* `?reset=1` - resets the performance counters and cache stats.
* `?format=prom` - returns the output in format compatible with Prometheus (the default format is XML).

### Configuration directives - segmentation

#### vod_segment_duration
* **syntax**: `vod_segment_duration duration`
* **default**: `10s`
* **context**: `http`, `server`, `location`

Sets the segment duration in milliseconds. It is highly recommended to use a segment duration that is a multiple of the GOP duration.
If the segment duration is not a multiple of GOP duration, and `vod_align_segments_to_key_frames` is enabled, there could be significant
differences between the segment duration that is reported in the manifest and the actual segment duration. This could also lead to
the appearance of empty segments within the stream.

#### vod_live_window_duration
* **syntax**: `vod_live_window_duration duration`
* **default**: `30000`
* **context**: `http`, `server`, `location`

Sets the total duration in milliseconds of the segments that should be returned in a live manifest.
If the value is positive, nginx vod returns a range of maximum `vod_live_window_duration` milliseconds, ending at the current server time.
If the value is negative, nginx vod returns a range of maximum `-vod_live_window_duration` milliseconds from the end of the mapping json.
If the value is set to zero, the live manifest will contain all the segments that are fully contained in the mapping json time frame.

#### vod_force_playlist_type_vod
* **syntax**: `vod_force_playlist_type_vod on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

Generate a vod stream even when the media set has `playlistType=live`. 
Enabling this setting has the following effects:
1. Frame timestamps will be continuous and start from zero
2. Segment indexes will start from one
3. In case of HLS, the returned manifest will have both `#EXT-X-PLAYLIST-TYPE:VOD` and `#EXT-X-ENDLIST`

This can be useful for clipping vod sections out of a live stream.

#### vod_force_continuous_timestamps
* **syntax**: `vod_force_continuous_timestamps on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

Generate continuous timestamps even when the media set has gaps (gaps can created by the use of `clipTimes`)
If ID3 timestamps are enabled (`vod_hls_mpegts_output_id3_timestamps`), they contain the original timestamps that were set in `clipTimes`.

#### vod_bootstrap_segment_durations
* **syntax**: `vod_bootstrap_segment_durations duration`
* **default**: `none`
* **context**: `http`, `server`, `location`

Adds a bootstrap segment duration in milliseconds. This setting can be used to make the first few segments
shorter than the default segment duration, thus making the adaptive bitrate selection kick-in earlier without 
the overhead of short segments throughout the video.

#### vod_align_segments_to_key_frames
* **syntax**: `vod_align_segments_to_key_frames on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module forces all segments to start with a key frame. Enabling this setting can lead to differences
between the actual segment durations and the durations reported in the manifest (unless `vod_manifest_segment_durations_mode` is set to accurate).

#### vod_segment_count_policy
* **syntax**: `vod_segment_count_policy last_short/last_long/last_rounded`
* **default**: `last_short`
* **context**: `http`, `server`, `location`

Configures the policy for calculating the segment count, for segment_duration = 10 seconds:
* last_short - a file of 33 sec is partitioned as - 10, 10, 10, 3
* last_long - a file of 33 sec is partitioned as - 10, 10, 13
* last_rounded - a file of 33 sec is partitioned as - 10, 10, 13, a file of 38 sec is partitioned as 10, 10, 10, 8

#### vod_manifest_duration_policy
* **syntax**: `vod_manifest_duration_policy min/max`
* **default**: `max`
* **context**: `http`, `server`, `location`

Configures the policy for calculating the duration of a manifest containing multiple streams:
* max - uses the maximum stream duration (default)
* min - uses the minimum non-zero stream duration

#### vod_manifest_segment_durations_mode
* **syntax**: `vod_manifest_segment_durations_mode estimate/accurate`
* **default**: `estimate`
* **context**: `http`, `server`, `location`

Configures the calculation mode of segment durations within manifest requests:
* estimate - reports the duration as configured in nginx.conf, e.g. if `vod_segment_duration` has the value 10000,
an HLS manifest will contain #EXTINF:10
* accurate - reports the exact duration of the segment, taking into account the frame durations, e.g. for a 
frame rate of 29.97 and 10 second segments it will report the first segment as 10.01. accurate mode also
takes into account the key frame alignment, in case `vod_align_segments_to_key_frames` is on

#### vod_media_set_override_json
* **syntax**: `vod_media_set_override_json json`
* **default**: `{}`
* **context**: `http`, `server`, `location`

This parameter provides a way to override portions of the media set JSON (mapped mode only).
For example, `vod_media_set_override_json '{"clipTo":20000}'` clips the media set to 20 sec.
The parameter value can contain variables.

### Configuration directives - upstream

#### vod_upstream_location
* **syntax**: `vod_upstream_location location`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets an nginx location that is used to read the MP4 file (remote mode) or mapping the request URI (mapped mode).

#### vod_remote_upstream_location
* **syntax**: `vod_remote_upstream_location location`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets an nginx location that is used to read the MP4 file on remote or mapped mode. If this directive is set on mapped mode, the module reads 
the MP4 files over HTTP, treating the paths in the mapping JSON as URIs (the default behavior is to read from local files)

#### vod_max_upstream_headers_size
* **syntax**: `vod_max_upstream_headers_size size`
* **default**: `4k`
* **context**: `http`, `server`, `location`

Sets the size that is allocated for holding the response headers when issuing upstream requests (to vod_xxx_upstream_location).

#### vod_upstream_extra_args
* **syntax**: `vod_upstream_extra_args "arg1=value1&arg2=value2&..."`
* **default**: `empty`
* **context**: `http`, `server`, `location`

Extra query string arguments that should be added to the upstream request (remote/mapped modes only).
The parameter value can contain variables.

#### vod_media_set_map_uri
* **syntax**: `vod_media_set_map_uri uri`
* **default**: `$vod_suburi`
* **context**: `http`, `server`, `location`

Sets the uri of media set mapping requests, the parameter value can contain variables.
In case of multi url, `$vod_suburi` will be the current sub uri (a separate request is issued per sub URL)

#### vod_path_response_prefix
* **syntax**: `vod_path_response_prefix prefix`
* **default**: `{"sequences":[{"clips":[{"type":"source","path":"`
* **context**: `http`, `server`, `location`

Sets the prefix that is expected in URI mapping responses (mapped mode only).

#### vod_path_response_postfix
* **syntax**: `vod_path_response_postfix postfix`
* **default**: `"}]}]}`
* **context**: `http`, `server`, `location`

Sets the postfix that is expected in URI mapping responses (mapped mode only).

#### vod_max_mapping_response_size
* **syntax**: `vod_max_mapping_response_size length`
* **default**: `1K`
* **context**: `http`, `server`, `location`

Sets the maximum length of a path returned from upstream (mapped mode only).

### Configuration directives - fallback

#### vod_fallback_upstream_location
* **syntax**: `vod_fallback_upstream_location location`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets an nginx location to which the request is forwarded after encountering a file not found error (local/mapped modes only).

#### vod_proxy_header_name
* **syntax**: `vod_proxy_header_name name`
* **default**: `X-Kaltura-Proxy`
* **context**: `http`, `server`, `location`

Sets the name of an HTTP header that is used to prevent fallback proxy loops (local/mapped modes only).

#### vod_proxy_header_value
* **syntax**: `vod_proxy_header_value name`
* **default**: `dumpApiRequest`
* **context**: `http`, `server`, `location`

Sets the value of an HTTP header that is used to prevent fallback proxy loops (local/mapped modes only).

### Configuration directives - performance

#### vod_metadata_cache
* **syntax**: `vod_metadata_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the video metadata cache. For MP4 files, this cache holds the moov atom.

#### vod_mapping_cache
* **syntax**: `vod_mapping_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the mapping cache for vod (mapped mode only).

#### vod_live_mapping_cache
* **syntax**: `vod_live_mapping_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the mapping cache for live (mapped mode only).

#### vod_response_cache
* **syntax**: `vod_response_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the response cache. The response cache holds manifests
and other non-video content (like DASH init segment, HLS encryption key etc.). Video segments are not cached.

#### vod_live_response_cache
* **syntax**: `vod_live_response_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the response cache for time changing live responses. 
This cache holds the following types of responses for live: DASH MPD, HLS index M3U8, HDS bootstrap, MSS manifest.

#### vod_initial_read_size
* **syntax**: `vod_initial_read_size size`
* **default**: `4K`
* **context**: `http`, `server`, `location`

Sets the size of the initial read operation of the MP4 file.

#### vod_max_metadata_size
* **syntax**: `vod_max_metadata_size size`
* **default**: `128MB`
* **context**: `http`, `server`, `location`

Sets the maximum supported video metadata size (for MP4 - moov atom size)

#### vod_max_frames_size
* **syntax**: `vod_max_frames_size size`
* **default**: `16MB`
* **context**: `http`, `server`, `location`

Sets the limit on the total size of the frames of a single segment

#### vod_max_frame_count
* **syntax**: `vod_max_frame_count count`
* **default**: `1048576`
* **context**: `http`, `server`, `location`

Sets the limit on the total count of the frames read to serve non segment (e.g. playlist) request.

#### vod_segment_max_frame_count
* **syntax**: `vod_segment_max_frame_count count`
* **default**: `65536`
* **context**: `http`, `server`, `location`

Sets the limit on the total count of the frames read to serve segment request.

#### vod_cache_buffer_size
* **syntax**: `vod_cache_buffer_size size`
* **default**: `256K`
* **context**: `http`, `server`, `location`

Sets the size of the cache buffers used when reading MP4 frames.

#### vod_open_file_thread_pool
* **syntax**: `vod_open_file_thread_pool pool_name`
* **default**: `off`
* **context**: `http`, `server`, `location`

Enables the use of asynchronous file open via thread pool.
The thread pool must be defined with a thread_pool directive, if no pool name is specified the default pool is used.
This directive is supported only on nginx 1.7.11 or newer when compiling with --add-threads.
Note: this directive currently disables the use of nginx's open_file_cache by nginx-vod-module

#### vod_output_buffer_pool
* **syntax**: `vod_output_buffer_pool size count`
* **default**: `off`
* **context**: `http`, `server`, `location`

Pre-allocates buffers for generating response data, saving the need allocate/free the buffers on every request.

#### vod_performance_counters
* **syntax**: `vod_performance_counters zone_name`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the shared memory object name of the performance counters

### Configuration directives - url structure

#### vod_base_url
* **syntax**: `vod_base_url url`
* **default**: `see below`
* **context**: `http`, `server`, `location`

Sets the base URL (scheme + domain) that should be returned in manifest responses.
The parameter value can contain variables, if the parameter evaluates to an empty string, relative URLs will be used.
If the parameter evaluates to a string ending with /, it is assumed to be a full URL - the module only appends the
file name to it, instead of a full URI.
If not set, the base URL is determined as follows:
1. If the request did not contain a host header (HTTP/1.0) relative URLs will be returned
2. Otherwise, the base URL will be `$scheme://$http_host`
The setting currently affects only HLS and DASH. In MSS and HDS, relative URLs are always returned.

#### vod_segments_base_url
* **syntax**: `vod_segments_base_url url`
* **default**: `see below`
* **context**: `http`, `server`, `location`

Sets the base URL (scheme + domain) that should be used for delivering video segments.
The parameter value can contain variables, if the parameter evaluates to an empty string, relative URLs will be used.
If not set, vod_base_url will be used.
The setting currently affects only HLS.

#### vod_multi_uri_suffix
* **syntax**: `vod_multi_uri_suffix suffix`
* **default**: `.urlset`
* **context**: `http`, `server`, `location`

A URL suffix that is used to identify multi URLs. A multi URL is a way to encode several different URLs
that should be played together as an adaptive streaming set, under a single URL. When the default suffix is
used, an HLS set URL may look like: 
http://host/hls/common-prefix,bitrate1,bitrate2,common-suffix.urlset/master.m3u8

#### vod_clip_to_param_name
* **syntax**: `vod_clip_to_param_name name`
* **default**: `clipTo`
* **context**: `http`, `server`, `location`

The name of the clip to request parameter.

#### vod_clip_from_param_name
* **syntax**: `vod_clip_from_param_name name`
* **default**: `clipFrom`
* **context**: `http`, `server`, `location`

The name of the clip from request parameter.

#### vod_tracks_param_name
* **syntax**: `vod_tracks_param_name name`
* **default**: `tracks`
* **context**: `http`, `server`, `location`

The name of the tracks request parameter.

#### vod_time_shift_param_name
* **syntax**: `vod_time_shift_param_name name`
* **default**: `shift`
* **context**: `http`, `server`, `location`

The name of the shift request parameter.

#### vod_speed_param_name
* **syntax**: `vod_speed_param_name name`
* **default**: `speed`
* **context**: `http`, `server`, `location`

The name of the speed request parameter.

#### vod_lang_param_name
* **syntax**: `vod_lang_param_name name`
* **default**: `lang`
* **context**: `http`, `server`, `location`

The name of the language request parameter.

#### vod_force_sequence_index
* **syntax**: `vod_force_sequence_index on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

Use sequence index in segment uris even if there is only one sequence

### Configuration directives - response headers

#### vod_expires
* **syntax**: `vod_expires time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the "Expires" and "Cache-Control" response headers for successful requests.
This directive is similar to nginx's built-in `expires` directive, except that it only supports the expiration interval scenario
(epoch, max, off, day time are not supported)
Main motivation for using this directive instead of the built-in `expires` is to have different expiration for VOD and dynamic live content.
If this directive is not specified, nginx-vod-module will not set the "Expires" / "Cache-Control" headers.
This setting affects all types of requests in VOD playlists and segment requests in live playlists.

#### vod_expires_live
* **syntax**: `vod_expires_live time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Same as `vod_expires` (above) for live requests that are not time dependent and not segments (e.g. HLS - master.m3u8, HDS - manifest.f4m).

#### vod_expires_live_time_dependent
* **syntax**: `vod_expires_live_time_dependent time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Same as `vod_expires` (above) for live requests that are time dependent (HLS - index.m3u8, HDS - bootstrap.abst, MSS - manifest, DASH - manifest.mpd).

#### vod_last_modified
* **syntax**: `vod_last_modified time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the Last-Modified header returned on the response, by default the module does not return a Last-Modified header.
The reason for having this parameter here is in order to support If-Modified-Since / If-Unmodified-Since.
Since nginx's builtin ngx_http_not_modified_filter_module runs before any other header filter module, it will not see any headers set by add_headers / more_set_headers.
This makes nginx always reply as if the content changed (412 for If-Unmodified-Since / 200 for If-Modified-Since)
For live requests that are not segments (e.g. live DASH MPD), Last-Modified is set to the current server time.

#### vod_last_modified_types
* **syntax**: `vod_last_modified_types mime-type1 mime-type2 ...`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the MIME types for which the Last-Modified header should be set.
The special value "*" matches any MIME type.

### Configuration directives - ad stitching (mapped mode only)

#### vod_dynamic_mapping_cache
* **syntax**: `vod_dynamic_mapping_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the cache that stores the mapping of dynamic clips.

#### vod_dynamic_clip_map_uri
* **syntax**: `vod_dynamic_clip_map_uri uri`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the uri that should be used to map dynamic clips. 
The parameter value can contain variables, specifically, `$vod_clip_id` contains the id of the clip that should be mapped.
The expected response from this uri is a JSON containing a concat clip object.

#### vod_source_clip_map_uri
* **syntax**: `vod_source_clip_map_uri uri`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the uri that should be used to map source clips defined using the clipIds property of concat. 
The parameter value can contain variables, specifically, `$vod_clip_id` contains the id of the clip that should be mapped.
The expected response from this uri is a JSON containing a source clip object.

#### vod_redirect_segments_url
* **syntax**: `vod_redirect_segments_url url`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets a url to which requests for segments should be redirected.
The parameter value can contain variables, specifically, `$vod_dynamic_mapping` contains a serialized representation of the mapping of dynamic clips.

#### vod_apply_dynamic_mapping
* **syntax**: `vod_apply_dynamic_mapping mapping`
* **default**: `none`
* **context**: `http`, `server`, `location`

Maps dynamic clips to concat clips using the given expression, previously generated by `$vod_dynamic_mapping`.
The parameter value can contain variables.

#### vod_notification_uri
* **syntax**: `vod_notification_uri uri`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the uri that should be used to issue notifications. 
The parameter value can contain variables, specifically, `$vod_notification_id` contains the id of the notification that is being fired.
The response from this uri is ignored.

### Configuration directives - DRM / encryption

#### vod_secret_key
* **syntax**: `vod_secret_key string`
* **default**: `empty`
* **context**: `http`, `server`, `location`

Sets the seed that is used to generate the TS encryption key and DASH/MSS encryption IVs.
The parameter value can contain variables, and will usually have the structure "secret-$vod_filepath".
See the list of nginx variables added by this module below.

#### vod_encryption_iv_seed
* **syntax**: `vod_encryption_iv_seed string`
* **default**: `empty`
* **context**: `http`, `server`, `location`

Sets the seed that is used to generate the encryption IV, currently applies only to HLS/fMP4 with AES-128 encryption.
The parameter value can contain variables.

#### vod_drm_enabled
* **syntax**: `vod_drm_enabled on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module encrypts the media segments according to the response it gets from the drm upstream.
Currently supported only for dash and mss (play ready).

#### vod_drm_single_key
* **syntax**: `vod_drm_single_key on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module requests the drm info only for the first sequence and applies it to all sequences.
When disabled, the drm info is requested for each sequence separately.
In addition, in DASH, enabling this setting makes the module place the ContentProtection tag under AdaptationSet,
otherwise, it is placed under Representation.

#### vod_drm_clear_lead_segment_count
* **syntax**: `vod_drm_clear_lead_segment_count count`
* **default**: `1`
* **context**: `http`, `server`, `location`

Sets the number of clear (unencrypted) segments in the beginning of the stream. A clear lead enables the player to start playing without having to wait for the license response.

#### vod_drm_max_info_length
* **syntax**: `vod_drm_max_info_length length`
* **default**: `4K`
* **context**: `http`, `server`, `location`

Sets the maximum length of a drm info returned from upstream.

#### vod_drm_upstream_location
* **syntax**: `vod_drm_upstream_location location`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the nginx location that should be used for getting the DRM info for the file.

#### vod_drm_info_cache
* **syntax**: `vod_drm_info_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the drm info cache.

#### vod_drm_request_uri
* **syntax**: `vod_drm_request_uri uri`
* **default**: `$vod_suburi`
* **context**: `http`, `server`, `location`

Sets the uri of drm info requests, the parameter value can contain variables.
In case of multi url, `$vod_suburi` will be the current sub uri (a separate drm info request is issued per sub URL)

#### vod_min_single_nalu_per_frame_segment
* **syntax**: `vod_min_single_nalu_per_frame_segment index`
* **default**: `0`
* **context**: `http`, `server`, `location`

Sets the minimum segment index (1-based) that should be assumed to have a single h264 nalu per frame.
If the value is 0, no assumption is being made on the number of nal units per frame.
This setting only affects DASH and MSS configurations that have DRM enabled.

When transcoding videos using libx264, by default, all frames have a single nal unit, except the first frame
that contains an additional nalu with the libx264 copyright information. Setting this parameter to a value
greater than 0 can provide a significant performance improvement, since the layout of the segment can be
calculated in advance, allowing the module to:
* Output segment buffers as they are generated (it doesn't have to wait for the whole segment to complete)
* Avoid frame processing for requests that do not need the segment data (e.g. HEAD, range 0-0, etc.)

### Configuration directives - DASH

#### vod_dash_absolute_manifest_urls
* **syntax**: `vod_dash_absolute_manifest_urls on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the server returns absolute URLs in MPD requests

#### vod_dash_manifest_file_name_prefix
* **syntax**: `vod_dash_manifest_file_name_prefix name`
* **default**: `manifest`
* **context**: `http`, `server`, `location`

The name of the MPD file (an mpd extension is implied).

#### vod_dash_profiles
* **syntax**: `vod_dash_profiles profiles`
* **default**: `urn:mpeg:dash:profile:isoff-main:2011`
* **context**: `http`, `server`, `location`

Sets the profiles that are returned in the MPD tag in manifest responses.

#### vod_dash_init_file_name_prefix
* **syntax**: `vod_dash_init_file_name_prefix name`
* **default**: `init`
* **context**: `http`, `server`, `location`

The name of the MP4 initialization file (an mp4 extension is implied).

#### vod_dash_fragment_file_name_prefix
* **syntax**: `vod_dash_fragment_file_name_prefix name`
* **default**: `frag`
* **context**: `http`, `server`, `location`

The name of the fragment files (an m4s extension is implied).

#### vod_dash_manifest_format
* **syntax**: `vod_dash_manifest_format format`
* **default**: `segmenttimeline`
* **context**: `http`, `server`, `location`

Sets the MPD format, available options are:
* `segmentlist` - uses SegmentList and SegmentURL tags, in this format the URL of each fragment is explicitly set in the MPD
* `segmenttemplate` - uses SegmentTemplate, reporting a single duration for all fragments
* `segmenttimeline` - uses SegmentTemplate and SegmentTimeline to explicitly set the duration of the fragments

#### vod_dash_subtitle_format
* **syntax**: `vod_dash_subtitle_format format`
* **default**: `webvtt`
* **context**: `http`, `server`, `location`

Sets the format of the subtitles returned in the MPD, available options are:
* `webvtt` - WebVTT
* `smpte-tt` - SMPTE Timed Text

#### vod_dash_init_mp4_pssh
* **syntax**: `vod_dash_init_mp4_pssh on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled, the DRM pssh boxes are returned in the DASH init segment and in the manifest.
When disabled, the pssh boxes are returned only in the manifest.

#### vod_dash_duplicate_bitrate_threshold
* **syntax**: `vod_dash_duplicate_bitrate_threshold threshold`
* **default**: `4096`
* **context**: `http`, `server`, `location`

The bitrate threshold for removing identical bitrates, streams whose bitrate differences are less than
this value will be considered identical.

#### vod_dash_use_base_url_tag
* **syntax**: `vod_dash_use_base_url_tag on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, a BaseURL tag will be used to specify the fragments/init segment base url.
Otherwise, the media/initialization attributes under SegmentTemplate will contain absolute URLs. 

### Configuration directives - HDS

#### vod_hds_absolute_manifest_urls
* **syntax**: `vod_hds_absolute_manifest_urls on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the server returns the base URL in the F4M manifest

#### vod_hds_manifest_file_name_prefix
* **syntax**: `vod_hds_manifest_file_name_prefix name`
* **default**: `manifest`
* **context**: `http`, `server`, `location`

The name of the HDS manifest file (an f4m extension is implied).

#### vod_hds_fragment_file_name_prefix
* **syntax**: `vod_hds_fragment_file_name_prefix name`
* **default**: `frag`
* **context**: `http`, `server`, `location`

The prefix of fragment file names, the actual file name is `frag-f<file-index>-v<video-track-index>-a<audio-track-index>-Seg1-Frag<index>`.

#### vod_hds_generate_moof_atom
* **syntax**: `vod_hds_generate_moof_atom on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the module generates a moof atom in the HDS fragments, when disabled only an mdat atom is generated.
Turning this parameter off reduces the packaging overhead, however the default is on since Adobe tools are generating this atom.

### Configuration directives - HLS

#### vod_hls_encryption_method
* **syntax**: `vod_hls_encryption_method method`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the encryption method of HLS segments, allowed values are: none (default), aes-128, sample-aes, sample-aes-cenc.

#### vod_hls_force_unmuxed_segments
* **syntax**: `vod_hls_force_unmuxed_segments on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled the server returns the audio stream in separate segments than the ones used by the video stream (using EXT-X-MEDIA)

#### vod_hls_container_format
* **syntax**: `vod_hls_container_format mpegts/fmp4/auto`
* **default**: `auto`
* **context**: `http`, `server`, `location`

Sets the container format of the HLS segments. 
The default behavior is to use fmp4 for HEVC, and mpegts otherwise (Apple does not support HEVC over MPEG TS).

#### vod_hls_absolute_master_urls
* **syntax**: `vod_hls_absolute_master_urls on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the server returns absolute playlist URLs in master playlist requests

#### vod_hls_absolute_index_urls
* **syntax**: `vod_hls_absolute_index_urls on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the server returns absolute segment URLs in media playlist requests

#### vod_hls_absolute_iframe_urls
* **syntax**: `vod_hls_absolute_iframe_urls on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled the server returns absolute segment URLs in iframe playlist requests

#### vod_hls_output_iframes_playlist
* **syntax**: `vod_hls_output_iframes_playlist on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When disabled iframe playlists are not returned as part of master playlists

#### vod_hls_master_file_name_prefix
* **syntax**: `vod_hls_master_file_name_prefix name`
* **default**: `master`
* **context**: `http`, `server`, `location`

The name of the HLS master playlist file (an m3u8 extension is implied).

#### vod_hls_index_file_name_prefix
* **syntax**: `vod_hls_index_file_name_prefix name`
* **default**: `index`
* **context**: `http`, `server`, `location`

The name of the HLS media playlist file (an m3u8 extension is implied).

#### vod_hls_iframes_file_name_prefix
* **syntax**: `vod_hls_iframes_file_name_prefix name`
* **default**: `iframes`
* **context**: `http`, `server`, `location`

The name of the HLS I-frames playlist file (an m3u8 extension is implied).

#### vod_hls_segment_file_name_prefix
* **syntax**: `vod_hls_segment_file_name_prefix name`
* **default**: `seg`
* **context**: `http`, `server`, `location`

The prefix of segment file names, the actual file name is `seg-<index>-v<video-track-index>-a<audio-track-index>.ts`.

#### vod_hls_init_file_name_prefix
* **syntax**: `vod_hls_init_file_name_prefix name`
* **default**: `init`
* **context**: `http`, `server`, `location`

The name of the init segment file name, only relevant when using fmp4 container.

#### vod_hls_encryption_key_file_name
* **syntax**: `vod_hls_encryption_key_file_name name`
* **default**: `encryption.key`
* **context**: `http`, `server`, `location`

The name of the encryption key file name, only relevant when encryption method is not `none`.

#### vod_hls_encryption_key_uri
* **syntax**: `vod_hls_encryption_key_uri uri`
* **default**: `a url pointing to encryption.key`
* **context**: `http`, `server`, `location`

Sets the value of the URI attribute of EXT-X-KEY, only relevant when encryption method is not `none`.
The parameter value can contain variables.

#### vod_hls_encryption_key_format
* **syntax**: `vod_hls_encryption_key_format format`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the KEYFORMAT attribute of EXT-X-KEY, only relevant when encryption method is not `none`.

#### vod_hls_encryption_key_format_versions
* **syntax**: `vod_hls_encryption_key_format_versions versions`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the KEYFORMATVERSIONS attribute of EXT-X-KEY, only relevant when encryption method is not `none`.

#### vod_hls_mpegts_interleave_frames
* **syntax**: `vod_hls_mpegts_interleave_frames on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the HLS muxer interleaves frames of different streams (audio / video).
When disabled, on every switch between audio / video the muxer flushes the MPEG TS packet.

#### vod_hls_mpegts_align_frames
* **syntax**: `vod_hls_mpegts_align_frames on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled, every video / audio frame is aligned to MPEG TS packet boundary,
padding is added as needed.

#### vod_hls_mpegts_output_id3_timestamps
* **syntax**: `vod_hls_mpegts_output_id3_timestamps on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, an ID3 TEXT frame will be outputted in each TS segment, containing a JSON with the absolute segment timestamp.
The timestamp is measured in milliseconds since the epoch (unixtime x 1000), the JSON structure is: `{"timestamp":1459779115000}`

#### vod_hls_mpegts_align_pts
* **syntax**: `vod_hls_mpegts_align_pts on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module will shift back the dts timestamps by the pts delay of the initial frame.
This can help keep the pts timestamps aligned across multiple renditions.

### Configuration directives - MSS

#### vod_mss_manifest_file_name_prefix
* **syntax**: `vod_mss_manifest_file_name_prefix name`
* **default**: `manifest`
* **context**: `http`, `server`, `location`

The name of the manifest file (has no extension).

#### vod_mss_duplicate_bitrate_threshold
* **syntax**: `vod_mss_duplicate_bitrate_threshold threshold`
* **default**: `4096`
* **context**: `http`, `server`, `location`

The bitrate threshold for removing identical bitrates, streams whose bitrate differences are less than
this value will be considered identical.

### Configuration directives - thumbnail capture

#### vod_thumb_file_name_prefix
* **syntax**: `vod_thumb_file_name_prefix name`
* **default**: `thumb`
* **context**: `http`, `server`, `location`

The name of the thumbnail file (a jpg extension is implied).

#### vod_thumb_accurate_positioning
* **syntax**: `vod_thumb_accurate_positioning on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled, the module grabs the frame that is closest to the requested offset.
When disabled, the module uses the keyframe that is closest to the requested offset.
Setting this parameter to off can result in faster thumbnail capture, since the module 
always decodes a single video frame per request.

#### vod_gop_look_behind
* **syntax**: `vod_gop_look_behind millis`
* **default**: `10000`
* **context**: `http`, `server`, `location`

Sets the interval (in milliseconds) before the thumbnail offset that should be loaded.
This setting should be set to the maximum GOP size, setting it to a lower value may result in capture failure.
Note that the metadata of all frames between `offset - vod_gop_look_behind` and `offset + vod_gop_look_ahead`
is loaded, however only the frames of the minimum GOP containing `offset` will be read and decoded.

#### vod_gop_look_ahead
* **syntax**: `vod_gop_look_ahead millis`
* **default**: `1000`
* **context**: `http`, `server`, `location`

Sets the interval (in milliseconds) after the thumbnail offset that should be loaded.

### Configuration directives - volume map

#### vod_volume_map_file_name_prefix
* **syntax**: `vod_volume_map_file_name_prefix name`
* **default**: `volume_map`
* **context**: `http`, `server`, `location`

The name of the volume map file (a csv extension is implied).

#### vod_volume_map_interval
* **syntax**: `vod_volume_map_interval millis`
* **default**: `1000`
* **context**: `http`, `server`, `location`

Sets the interval/resolution (in milliseconds) of the volume map.

### Configuration directives - misc

#### vod_ignore_edit_list
* **syntax**: `vod_ignore_edit_list on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module ignores any edit lists (elst) in the MP4 file.

#### vod_parse_hdlr_name
* **syntax**: `vod_parse_hdlr_name on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module parses the name field of the hdlr MP4 atom, and uses it as the stream label.

#### vod_parse_udta_name
* **syntax**: `vod_parse_udta_name on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module parses the name atom child of the udta MP4 atom, and uses it as the stream label.

### Nginx variables

The module adds the following nginx variables:
* `$vod_suburi` - the current sub uri. For example, if the url is:
  `http://<domain>/<location>/<prefix>,<middle1>,<middle2>,<middle3>,<postfix>.urlset/<filename>`
  `$vod_suburi` will have the value `http://<domain>/<location>/<prefix><middle1><postfix>/<filename>` 
  when processing the first uri.
* `$vod_filepath` - in local / mapped modes, the file path of current sub uri. In remote mode, has the same value as `$vod_suburi`.
* `$vod_set_id` - contains the id of the set.
* `$vod_sequence_id` - contains the id of the current sequence, if no id was specified in the mapping json this variable will be the same as `$vod_suburi`.
* `$vod_clip_id` - the id of the current clip, this variable has a value during these phases:
  1. Mapping of dynamic clips to concat clips
  2. Mapping of source clip to paths
* `$vod_notification_id` - the id of the current notification, the value is non-empty only when referenced by `vod_notification_uri`
* `$vod_dynamic_mapping` - a serialized representation of the mapping of dynamic clips to concat clips.
* `$vod_request_params` - a serialized representation of the request params, e.g. 12-f2-v1-a1. The variable contains:
  1. The segment index (for a segment request)
  2. The sequence index
  3. A selection of audio/video tracks
* `$vod_status` - the internal error code of the module, provides a more fine grained classification of errors than http status.
	the following values are defined:
	`BAD_REQUEST` - the request is invalid, for example, `clipFrom` is larger than the video duration
	`NO_STREAMS` - an invalid segment index was requested
	`EMPTY_MAPPING` - the mapping response is empty
	`BAD_MAPPING` - the mapping json is invalid, for example, the `sequences` element is missing
	`BAD_DATA` - the video file is corrupt
	`EXPIRED` - the current server time is larger than `expirationTime`
	`ALLOC_FAILED` - the module failed to allocate memory
	`UNEXPECTED` - a scenario that is not supposed to happen, most likely a bug in the module
* `$vod_segment_duration` - for segment requests, contains the duration of the segment in milliseconds
* `$vod_frames_bytes_read` - for segment requests, total number of bytes read while processing media frames

Note: Configuration directives that can accept variables are explicitly marked as such.

### Sample configurations

#### Local configuration
```nginx
	http {
		upstream fallback {
			server fallback.kaltura.com:80;
		}

		server {
			# vod settings
			vod_mode local;
			vod_fallback_upstream_location /fallback;
			vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';
			vod_last_modified_types *;

			# vod caches
			vod_metadata_cache metadata_cache 512m;
			vod_response_cache response_cache 128m;
			
			# gzip manifests
			gzip on;
			gzip_types application/vnd.apple.mpegurl;

			# file handle caching / aio
			open_file_cache          max=1000 inactive=5m;
			open_file_cache_valid    2m;
			open_file_cache_min_uses 1;
			open_file_cache_errors   on;
			aio on;
			
			location ^~ /fallback/ {
				internal;
				proxy_pass http://fallback/;
				proxy_set_header Host $http_host;
			}

			location /content/ {
				root /web/;
				vod hls;
				
				add_header Access-Control-Allow-Headers '*';
				add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
				add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
				add_header Access-Control-Allow-Origin '*';
				expires 100d;
			}
		}
	}
```

#### Mapped configuration
```nginx
	http {
		upstream kalapi {
			server www.kaltura.com:80;
		}

		upstream fallback {
			server fallback.kaltura.com:80;
		}

		server {
			# vod settings
			vod_mode mapped;
			vod_upstream_location /kalapi;
			vod_upstream_extra_args "pathOnly=1";
			vod_fallback_upstream_location /fallback;
			vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';
			vod_last_modified_types *;

			# vod caches
			vod_metadata_cache metadata_cache 512m;
			vod_response_cache response_cache 128m;
			vod_mapping_cache mapping_cache 5m;
			
			# gzip manifests
			gzip on;
			gzip_types application/vnd.apple.mpegurl;

			# file handle caching / aio
			open_file_cache          max=1000 inactive=5m;
			open_file_cache_valid    2m;
			open_file_cache_min_uses 1;
			open_file_cache_errors   on;
			aio on;
			
			location ^~ /fallback/ {
				internal;
				proxy_pass http://fallback/;
				proxy_set_header Host $http_host;
			}

			location ^~ /kalapi/ {
				internal;
				proxy_pass http://kalapi/;
				proxy_set_header Host $http_host;
			}

			location ~ ^/p/\d+/(sp/\d+/)?serveFlavor/ {
				# encrypted hls
				vod hls;
				vod_secret_key "mukkaukk$vod_filepath";
				vod_hls_encryption_method aes-128;
				
				add_header Access-Control-Allow-Headers '*';
				add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
				add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
				add_header Access-Control-Allow-Origin '*';
				expires 100d;
			}
		}
	}
```

#### Mapped + Remote configuration
```nginx
	http {
		upstream jsonupstream {
			server jsonserver:80;
		}

		server {
			# vod settings
			vod_mode mapped;
			vod_upstream_location /json;
			vod_remote_upstream_location /proxy;
			vod_upstream_extra_args "pathOnly=1";
			vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';
			vod_last_modified_types *;

			# vod caches
			vod_metadata_cache metadata_cache 512m;
			vod_response_cache response_cache 128m;
			vod_mapping_cache mapping_cache 5m;

			# gzip manifests
			gzip on;
			gzip_types application/vnd.apple.mpegurl;

			# file handle caching / aio
			open_file_cache	  max=1000 inactive=5m;
			open_file_cache_valid    2m;
			open_file_cache_min_uses 1;
			open_file_cache_errors   on;
			aio on;

			location ^~ /json/hls/ {
				internal;
				proxy_pass http://jsonupstream/;
				proxy_set_header Host $http_host;
			}

			location ~ /proxy/([^/]+)/(.*) {
				internal;
				proxy_pass $1://$2;
				resolver 8.8.8.8;
			}

			location ~ ^/hls/ {
				vod hls;

				add_header Access-Control-Allow-Headers '*';
				add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
				add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
				add_header Access-Control-Allow-Origin '*';
				expires 100d;
			}
		}
	}
```

Set it up so that http://jsonserver:80/test.json returns the following JSON:
```json
	{
		"sequences": [{
			"clips": [{
				"type": "source",
				"path": "/http/commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4"
			}]
		}]
	}
```

And use this stream URL - http://nginx-vod-server/hls/test.json/master.m3u8

#### Remote configuration
```nginx
	http {
		upstream kalapi {
			server www.kaltura.com:80;
		}

		server {
			# vod settings
			vod_mode remote;
			vod_upstream_location /kalapi;
			vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';
			vod_last_modified_types *;

			# vod caches
			vod_metadata_cache metadata_cache 512m;
			vod_response_cache response_cache 128m;
			
			# gzip manifests
			gzip on;
			gzip_types application/vnd.apple.mpegurl;
			
			location ^~ /kalapi/ {
				internal;
				proxy_pass http://kalapi/;
				proxy_set_header Host $http_host;
			}

			location ~ ^/p/\d+/(sp/\d+/)?serveFlavor/ {
				vod hls;
				
				add_header Access-Control-Allow-Headers '*';
				add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
				add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
				add_header Access-Control-Allow-Origin '*';
				expires 100d;
			}
		}
	}
```

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-vod](https://github.com/kaltura/nginx-vod-module){target=_blank}.

# *vts*: NGINX virtual host traffic status module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-vts
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_vhost_traffic_status_module.so;
```


This document describes nginx-module-vts [v0.2.2](https://github.com/vozlt/nginx-module-vts/releases/tag/v0.2.2){target=_blank} 
released on May 26 2023.

<hr />

[![CI](https://github.com/vozlt/nginx-module-vts/actions/workflows/ci.yml/badge.svg)](https://github.com/vozlt/nginx-module-vts/actions/workflows/ci.yml)
[![License](http://img.shields.io/badge/license-BSD-brightgreen.svg)](https://github.com/vozlt/nginx-module-vts/blob/master/LICENSE)

Nginx virtual host traffic status module

## Test
Run `sudo prove -r t` after you have installed this module. The `sudo` is required because
the test requires Nginx to listen on port 80.

## Screenshots
![screenshot-vts-0](https://cloud.githubusercontent.com/assets/3648408/23890539/a4c0de18-08d5-11e7-9a8b-448662454854.png "screenshot with default")
## 
![screenshot-vts-1](https://cloud.githubusercontent.com/assets/3648408/23890545/a9d5b504-08d5-11e7-88c2-eb55f39233db.png "screenshot with filter")

## Synopsis

```Nginx
http {
    vhost_traffic_status_zone;

    ...

    server {

        ...

        location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

## Description
This is an Nginx module that provides access to virtual host status information.
It contains the current status such as servers, upstreams, caches.
This is similar to the live activity monitoring of nginx plus.
The built-in html is also taken from the demo page of old version.

First of all, the directive `vhost_traffic_status_zone` is required,
and then if the directive `vhost_traffic_status_display` is set, can be access to as follows:

* /status/format/json
  * If you request `/status/format/json`, will respond with a JSON document containing the current activity data for using in live dashboards and third-party monitoring tools.
* /status/format/html
  * If you request `/status/format/html`, will respond with the built-in live dashboard in HTML that requests internally to `/status/format/json`.
* /status/format/jsonp
  * If you request `/status/format/jsonp`, will respond with a JSONP callback function containing the current activity data for using in live dashboards and third-party monitoring tools. 
* /status/format/prometheus
  * If you request `/status/format/prometheus`, will respond with a [prometheus](https://prometheus.io) document containing the current activity data.
* /status/control
  * If you request `/status/control`, will respond with a JSON document after it reset or delete zones through a query string. See the [Control](#control).

JSON document contains as follows:

```Json
{
    "hostName": ...,
    "moduleVersion": ...,
    "nginxVersion": ...,
    "loadMsec": ...,
    "nowMsec": ...,
    "connections": {
        "active":...,
        "reading":...,
        "writing":...,
        "waiting":...,
        "accepted":...,
        "handled":...,
        "requests":...
    },
    "sharedZones": {
        "name":...,
        "maxSize":...,
        "usedSize":...,
        "usedNode":...
    },
    "serverZones": {
        "...":{
            "requestCounter":...,
            "inBytes":...,
            "outBytes":...,
            "responses":{
                "1xx":...,
                "2xx":...,
                "3xx":...,
                "4xx":...,
                "5xx":...,
                "miss":...,
                "bypass":...,
                "expired":...,
                "stale":...,
                "updating":...,
                "revalidated":...,
                "hit":...,
                "scarce":...
            },
            "requestMsecCounter":...,
            "requestMsec":...,
            "requestMsecs":{
                "times":[...],
                "msecs":[...]
            },
            "requestBuckets":{
                "msecs":[...],
                "counters":[...]
            },
        }
        ...
    },
    "filterZones": {
        "...":{
            "...":{
                "requestCounter":...,
                "inBytes":...,
                "outBytes":...,
                "responses":{
                    "1xx":...,
                    "2xx":...,
                    "3xx":...,
                    "4xx":...,
                    "5xx":...,
                    "miss":...,
                    "bypass":...,
                    "expired":...,
                    "stale":...,
                    "updating":...,
                    "revalidated":...,
                    "hit":...,
                    "scarce":...
                },
                "requestMsecCounter":...,
                "requestMsec":...,
                "requestMsecs":{
                    "times":[...],
                    "msecs":[...]
                },
                "requestBuckets":{
                    "msecs":[...],
                    "counters":[...]
                },
            },
            ...
        },
        ...
    },
    "upstreamZones": {
        "...":[
            {
                "server":...,
                "requestCounter":...,
                "inBytes":...,
                "outBytes":...,
                "responses":{
                    "1xx":...,
                    "2xx":...,
                    "3xx":...,
                    "4xx":...,
                    "5xx":...
                },
                "requestMsecCounter":...,
                "requestMsec":...,
                "requestMsecs":{
                    "times":[...],
                    "msecs":[...]
                },
                "requestBuckets":{
                    "msecs":[...],
                    "counters":[...]
                },
                "responseMsecCounter":...,
                "responseMsec":...,
                "responseMsecs":{
                    "times":[...],
                    "msecs":[...]
                },
                "responseBuckets":{
                    "msecs":[...],
                    "counters":[...]
                },
                "weight":...,
                "maxFails":...,
                "failTimeout":...,
                "backup":...,
                "down":...
            }
            ...
        ],
        ...
    }
    "cacheZones": {
        "...":{
            "maxSize":...,
            "usedSize":...,
            "inBytes":...,
            "outBytes":...,
            "responses":{
                "miss":...,
                "bypass":...,
                "expired":...,
                "stale":...,
                "updating":...,
                "revalidated":...,
                "hit":...,
                "scarce":...
            }
        },
        ...
    }
}
```

* main
  * Basic version, uptime((nowMsec - loadMsec)/1000)
  * nowMsec, loadMsec is a millisecond.
* connections
  * Total connections and requests(same as stub_status_module in NGINX)
* sharedZones
  * The shared memory information using in nginx-module-vts.
* serverZones
  * Traffic(in/out) and request and response counts and cache hit ratio per each server zone
  * Total traffic(In/Out) and request and response counts(It zone name is `*`) and hit ratio
* filterZones
  * Traffic(in/out) and request and response counts and cache hit ratio per each server zone filtered through the `vhost_traffic_status_filter_by_set_key` directive
  * Total traffic(In/Out) and request and response counts(It zone name is `*`) and hit ratio filtered through the `vhost_traffic_status_filter_by_set_key` directive
* upstreamZones
  * Traffic(in/out) and request and response counts per server in each upstream group
  * Current settings(weight, maxfails, failtimeout...) in nginx.conf
* cacheZones
  * Traffic(in/out) and size(capacity/used) and hit ratio per each cache zone when using the proxy_cache directive.

The `overCounts` objects in JSON document are mostly for 32bit system and will be increment by 1 if its value is overflowed.
The directive `vhost_traffic_status_display_format` sets the default ouput format that is one of json, jsonp, html, prometheus. (Default: json)

Traffic calculation as follows:

* ServerZones
  * in += requested_bytes
  * out += sent_bytes
* FilterZones
  * in += requested_bytes via the filter
  * out += sent_bytes via the filter
* UpstreamZones
  * in += requested_bytes via the ServerZones
  * out += sent_bytes via the ServerZones
* cacheZones
  * in += requested_bytes via the ServerZones
  * out += sent_bytes via the ServerZones

All calculations are working in log processing phase of Nginx.
Internal redirects(X-Accel-Redirect or error_page) does not calculate in the UpstreamZones.

`Caveats:` this module relies on nginx logging system(NGX_HTTP_LOG_PHASE:last phase of the nginx http), so the traffic may be
in certain cirumstances different that real bandwidth traffic.
Websocket, canceled downloads may be cause of inaccuracies.
The working of the module doesn't matter at all whether the access_log directive "on" or "off".
Again, this module works well on "access_log off".
When using several domains it sets to be first domain(left) of server_name directive.
If you don't want it, see the [vhost_traffic_status_filter_by_host](#vhost_traffic_status_filter_by_host), [vhost_traffic_status_filter_by_set_key](#vhost_traffic_status_filter_by_set_key) directive.

See the following modules for the `stream` traffic statistics:
* [nginx-module-sts](https://github.com/vozlt/nginx-module-sts)
* [nginx-module-stream-sts](https://github.com/vozlt/nginx-module-stream-sts)

## Calculations and Intervals

### Averages

All averages are currently calculated as [AMM](https://en.wikipedia.org/wiki/Arithmetic_mean)(Arithmetic Mean) over the last [64](https://github.com/vozlt/nginx-module-vts/blob/master/src/ngx_http_vhost_traffic_status_node.h#L11) values.

## Control
It is able to reset or delete traffic zones through a query string.
The request responds with a JSON document.

* URI Syntax
  * /*`{status_uri}`*/control?cmd=*`{command}`*&group=*`{group}`*&zone=*`{name}`*

```Nginx
http {

    geoip_country /usr/share/GeoIP/GeoIP.dat;

    vhost_traffic_status_zone;
    vhost_traffic_status_filter_by_set_key $geoip_country_code country::*;

    ...

    server {

        server_name example.org;

        ...

        vhost_traffic_status_filter_by_set_key $geoip_country_code country::$server_name;

        location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

If it set as above, then the control uri is like `example.org/status/control`.

The available request arguments are as follows:
* **cmd**=\<`status`\|`reset`\|`delete`\>
  * status
    * It returns status of traffic zones to json format like `status/format/json`.
  * reset
    * It reset traffic zones without deleting nodes in shared memory.(= init to 0)
  * delete
    * It delete traffic zones in shared memory. when re-request recreated. 
* **group**=\<`server`\|`filter`\|`upstream@alone`\|`upstream@group`\|`cache`\|`*`\>
  * server
  * filter
  * upstream@alone
  * upstream@group
  * cache
  * \*
* **zone**=*name*
  * server
    * *name*
  * filter
    * *filter_group*@*name*
  * upstream@group
    * *upstream_group*@*name*
  * upstream@alone
    * @*name*
  * cache
    * *name*


### To get status of traffic zones on the fly
This is similar to the `status/format/json` except that it can get each zones.

#### To get fully zones
* It is exactly the same with the `status/format/json`.
  * /status/control?cmd=status&group=*

#### To get group zones
* mainZones
  * /status/control?cmd=status&group=server&zone=::main
* serverZones
  * /status/control?cmd=status&group=server&zone=*
* filterZones
  * /status/control?cmd=status&group=filter&zone=*
* upstreamZones
  * /status/control?cmd=status&group=upstream@group&zone=*
* upstreamZones::nogroups
  * /status/control?cmd=status&group=upstream@alone&zone=*
* cacheZones
  * /status/control?cmd=status&group=cache&zone=*

The **mainZones** values are default status values including `hostName`, `moduleVersion`, `nginxVersion`, `loadMsec`, `nowMsec`, `connections`.

#### To get each zones
* single zone in serverZones
  * /status/control?cmd=status&group=server&zone=*`name`*
* single zone in filterZones
  * /status/control?cmd=status&group=filter&zone=*`filter_group`*@*`name`*
* single zone in upstreamZones
  * /status/control?cmd=status&group=upstream@group&zone=*`upstream_group`*@*`name`*
* single zone in upstreamZones::nogroups
  * /status/control?cmd=status&group=upstream@alone&zone=*`name`*
* single zone in cacheZones
  * /status/control?cmd=status&group=cache&zone=*`name`*

### To reset traffic zones on the fly
It reset the values of specified zones to 0.

#### To reset fully zones
* /status/control?cmd=reset&group=*

#### To reset group zones
* serverZones
  * /status/control?cmd=reset&group=server&zone=*
* filterZones
  * /status/control?cmd=reset&group=filter&zone=*
* upstreamZones
  * /status/control?cmd=reset&group=upstream@group&zone=*
* upstreamZones::nogroups
  * /status/control?cmd=reset&group=upstream@alone&zone=*
* cacheZones
  * /status/control?cmd=reset&group=cache&zone=*

#### To reset each zones
* single zone in serverZones
  * /status/control?cmd=reset&group=server&zone=*`name`*
* single zone in filterZones
  * /status/control?cmd=reset&group=filter&zone=*`filter_group`*@*`name`*
* single zone in upstreamZones
  * /status/control?cmd=reset&group=upstream@group&zone=*`upstream_group`*@*`name`*
* single zone in upstreamZones::nogroups
  * /status/control?cmd=reset&group=upstream@alone&zone=*`name`*
* single zone in cacheZones
  * /status/control?cmd=reset&group=cache&zone=*`name`*

### To delete traffic zones on the fly
It delete the specified zones in shared memory.

#### To delete fully zones
* /status/control?cmd=delete&group=*

#### To delete group zones
* serverZones
  * /status/control?cmd=delete&group=server&zone=*
* filterZones
  * /status/control?cmd=delete&group=filter&zone=*
* upstreamZones
  * /status/control?cmd=delete&group=upstream@group&zone=*
* upstreamZones::nogroups
  * /status/control?cmd=delete&group=upstream@alone&zone=*
* cacheZones
  * /status/control?cmd=delete&group=cache&zone=*

#### To delete each zones
* single zone in serverZones
  * /status/control?cmd=delete&group=server&zone=*`name`*
* single zone in filterZones
  * /status/control?cmd=delete&group=filter&zone=*`filter_group`*@*`name`*
* single zone in upstreamZones
  * /status/control?cmd=delete&group=upstream@group&zone=*`upstream_group`*@*`name`*
* single zone in upstreamZones::nogroups
  * /status/control?cmd=delete&group=upstream@alone&zone=*`name`*
* single zone in cacheZones
  * /status/control?cmd=delete&group=cache&zone=*`name`*

## Set
It can get the status values in nginx configuration separately using `vhost_traffic_status_set_by_filter` directive.
It can acquire almost all status values and the obtained value is stored in user-defined-variable which is first argument.

* Directive Syntax
  * **vhost_traffic_status_set_by_filter** *$variable* *group*/*zone*/*name*

```Nginx
http {

    geoip_country /usr/share/GeoIP/GeoIP.dat;

    vhost_traffic_status_zone;
    vhost_traffic_status_filter_by_set_key $geoip_country_code country::*;

    ...
    upstream backend {
        10.10.10.11:80;
        10.10.10.12:80;
    }

    server {

        server_name example.org;

        ...

        vhost_traffic_status_filter_by_set_key $geoip_country_code country::$server_name;

        vhost_traffic_status_set_by_filter $requestCounter server/example.org/requestCounter;
        vhost_traffic_status_set_by_filter $requestCounterKR filter/country::example.org@KR/requestCounter;

        location /backend {
            vhost_traffic_status_set_by_filter $requestCounterB1 upstream@group/backend@10.10.10.11:80/requestCounter;
            proxy_pass http://backend;
        }
    }
}
```

The above settings are as follows:

* $requestCounter
  * serverZones -> example.org -> requestCounter
* $requestCounterKR
  * filterZones -> country::example.org -> KR -> requestCounter
* $requestCounterB1
  * upstreamZones -> backend -> 10.0.10.11:80 -> requestCounter

Please see the [vhost_traffic_status_set_by_filter](#vhost_traffic_status_set_by_filter) directive for detailed usage.

## JSON
The following status information is provided in the JSON format:

### Json used by status
/*`{status_uri}`*/format/json

/*`{status_uri}`*/control?cmd=status&...

* hostName
  * Host name.
* moduleVersion
  * Version of the module in *`{version}(|.dev.{commit})`* format.
* nginxVersion
  * Version of the provided.
* loadMsec
  * Loaded process time in milliseconds.
* nowMsec
  * Current time in milliseconds
* connections
  * active
    * The current number of active client connections.
  * reading
    * The total number of reading client connections.
  * writing
    * The total number of writing client connections.
  * waiting
    * The total number of wating client connections.
  * accepted
    * The total number of accepted client connections.
  * handled
    * The total number of handled client connections.
  * requests
    * The total number of requested client connections.
* sharedZones
  * name
    * The name of shared memory specified in the configuration.(default: `vhost_traffic_status`)
  * maxSize
    * The limit on the maximum size of the shared memory specified in the configuration.
  * usedSize
    * The current size of the shared memory.
  * usedNode
    * The current number of node using in shared memory. It can get an approximate size for one node with the following formula: (*usedSize* / *usedNode*)
* serverZones
  * requestCounter
    * The total number of client requests received from clients.
  * inBytes
    * The total number of bytes received from clients.
  * outBytes
    * The total number of bytes sent to clients.
  * responses
    * 1xx, 2xx, 3xx, 4xx, 5xx
      * The number of responses with status codes 1xx, 2xx, 3xx, 4xx, and 5xx.
    * miss
      * The number of cache miss.
    * bypass
      * The number of cache bypass.
    * expired
      * The number of cache expired.
    * stale
      * The number of cache stale.
    * updating
      * The number of cache updating.
    * revalidated
      * The number of cache revalidated.
    * hit
      * The number of cache hit.
    * scarce
      * The number of cache scare.
  * requestMsecCounter
    * The number of accumulated request processing time in milliseconds.
  * requestMsec
    * The average of request processing times in milliseconds.
  * requestMsecs
    * times
      * The times in milliseconds at request processing times.
    * msecs
      * The request processing times in milliseconds.
  * requestBuckets
    * msecs
      * The bucket values of histogram set by `vhost_traffic_status_histogram_buckets` directive.
    * counters
      * The cumulative values for the reason that each bucket value is greater than or equal to the request processing time. 
* filterZones
  * It provides the same fields with `serverZones` except that it included group names.
* upstreamZones
  * server
    * An address of the server.
  * requestCounter
    * The total number of client connections forwarded to this server.
  * inBytes
    * The total number of bytes received from this server.
  * outBytes
    * The total number of bytes sent to this server.
  * responses
    * 1xx, 2xx, 3xx, 4xx, 5xx
      * The number of responses with status codes 1xx, 2xx, 3xx, 4xx, and 5xx.
  * requestMsecCounter
    * The number of accumulated request processing time including upstream in milliseconds.
  * requestMsec
    * The average of request processing times including upstream in milliseconds.
  * requestMsecs
    * times
      * The times in milliseconds at request processing times.
    * msecs
      * The request processing times including upstream in milliseconds.
  * requestBuckets
    * msecs
      * The bucket values of histogram set by `vhost_traffic_status_histogram_buckets` directive.
    * counters
      * The cumulative values for the reason that each bucket value is greater than or equal to the request processing time including upstream.
  * responseMsecCounter
    * The number of accumulated only upstream response processing time in milliseconds.
  * responseMsec
    * The average of only upstream response processing times in milliseconds.
  * responseMsecs
    * times
      * The times in milliseconds at request processing times.
    * msecs
      * The only upstream response processing times in milliseconds.
  * responseBuckets
    * msecs
      * The bucket values of histogram set by `vhost_traffic_status_histogram_buckets` directive.
    * counters
      * The cumulative values for the reason that each bucket value is greater than or equal to the only upstream response processing time.
  * weight
    * Current `weight` setting of the server.
  * maxFails
    * Current `max_fails` setting of the server.
  * failTimeout
    * Current `fail_timeout` setting of the server.
  * backup
    * Current `backup` setting of the server.
  * down
    * Current `down` setting of the server. Basically, this is just a mark the [ngx_http_upstream_module](http://nginx.org/en/docs/http/ngx_http_upstream_module.html#server)'s server down(eg. `server backend3.example.com down`), not actual upstream server state. It will changed to actual state if you enabled the upstream zone directive.
* cacheZones
  * maxSize
    * The limit on the maximum size of the cache specified in the configuration. If `max_size` in `proxy_cache_path` directive is not specified, the system dependent value `NGX_MAX_OFF_T_VALUE` is assigned by default. In other words, this value is from nginx, not what I specified.
  * usedSize
    * The current size of the cache. This value is taken from nginx like the above `maxSize` value. 
  * inBytes
    * The total number of bytes received from the cache.
  * outBytes
    * The total number of bytes sent from the cache.
  * responses
    * miss
      * The number of cache miss.
    * bypass
      * The number of cache bypass.
    * expired
      * The number of cache expired.
    * stale
      * The number of cache stale.
    * updating
      * The number of cache updating.
    * revalidated
      * The number of cache revalidated.
    * hit
      * The number of cache hit.
    * scarce
      * The number of cache scare.

### Json used by control
/*`{status_uri}`*/control?cmd=reset&...

/*`{status_uri}`*/control?cmd=delete&...

* processingReturn
  * The result of true or false.
* processingCommandString
  * The requested command string.
* processingGroupString
  * The requested group string.
* processingZoneString
  * The requested zone string.
* processingCounts
  * The actual processing number.

## Variables
The following embedded variables are provided:

* **$vts_request_counter**
  * The total number of client requests received from clients.
* **$vts_in_bytes**
  * The total number of bytes received from clients.
* **$vts_out_bytes**
  * The total number of bytes sent to clients.
* **$vts_1xx_counter**
  * The number of responses with status codes 1xx.
* **$vts_2xx_counter**
  * The number of responses with status codes 2xx.
* **$vts_3xx_counter**
  * The number of responses with status codes 3xx.
* **$vts_4xx_counter**
  * The number of responses with status codes 4xx.
* **$vts_5xx_counter**
  * The number of responses with status codes 5xx.
* **$vts_cache_miss_counter**
  * The number of cache miss.
* **$vts_cache_bypass_counter**
  * The number of cache bypass.
* **$vts_cache_expired_counter**
  * The number of cache expired.
* **$vts_cache_stale_counter**
  * The number of cache stale.
* **$vts_cache_updating_counter**
  * The number of cache updating.
* **$vts_cache_revalidated_counter**
  * The number of cache revalidated.
* **$vts_cache_hit_counter**
  * The number of cache hit.
* **$vts_cache_scarce_counter**
  * The number of cache scare.
* **$vts_request_time_counter**
  * The number of accumulated request processing time.
* **$vts_request_time**
  * The average of request processing times.

## Limit

It is able to limit total traffic per each host by using the directive
[`vhost_traffic_status_limit_traffic`](#vhost_traffic_status_limit_traffic).
It also is able to limit all traffic by using the directive
[`vhost_traffic_status_limit_traffic_by_set_key`](#vhost_traffic_status_limit_traffic_by_set_key).
When the limit is exceeded, the server will return the 503
(Service Temporarily Unavailable) error in reply to a request. 
The return code can be changeable.

### To limit traffic for server
```Nginx
http {

    vhost_traffic_status_zone;

    ...

    server {

        server_name *.example.org;

        vhost_traffic_status_limit_traffic in:64G;
        vhost_traffic_status_limit_traffic out:1024G;

        ...
    }
}
```

* Limit in/out total traffic on the `*.example.org` to 64G and 1024G respectively.
It works individually per each domain if `vhost_traffic_status_filter_by_host` directive is enabled.

### To limit traffic for filter
```Nginx
http {
    geoip_country /usr/share/GeoIP/GeoIP.dat;

    vhost_traffic_status_zone;

    ...

    server {

        server_name example.org;

        vhost_traffic_status_filter_by_set_key $geoip_country_code country::$server_name;
        vhost_traffic_status_limit_traffic_by_set_key FG@country::$server_name@US out:1024G;
        vhost_traffic_status_limit_traffic_by_set_key FG@country::$server_name@CN out:2048G;

        ...

    }
}

```

* Limit total traffic of going into US and CN on the `example.org` to 1024G and 2048G respectively.

### To limit traffic for upstream
```Nginx
http {

    vhost_traffic_status_zone;

    ...

    upstream backend {
        server 10.10.10.17:80;
        server 10.10.10.18:80;
    }

    server {

        server_name example.org;

        location /backend {
            vhost_traffic_status_limit_traffic_by_set_key UG@backend@10.10.10.17:80 in:512G;
            vhost_traffic_status_limit_traffic_by_set_key UG@backend@10.10.10.18:80 in:1024G;
            proxy_pass http://backend;
        }

        ...

    }
}

```

* Limit total traffic of going into upstream backend on the `example.org` to 512G and 1024G per each peer.

`Caveats:` Traffic is the cumulative transfer or counter, not a bandwidth.

## Use cases

It is able to calculate the user defined individual stats by using the directive `vhost_traffic_status_filter_by_set_key`.

### To calculate traffic for individual country using GeoIP
```Nginx
http {
    geoip_country /usr/share/GeoIP/GeoIP.dat;

    vhost_traffic_status_zone;
    vhost_traffic_status_filter_by_set_key $geoip_country_code country::*;

    ...

    server {

        ...

        vhost_traffic_status_filter_by_set_key $geoip_country_code country::$server_name;

        location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

* Calculate traffic for individual country of total server groups.
* Calculate traffic for individual country of each server groups.

Basically, country flags image is built-in in HTML.
The country flags image is enabled if the `country` string is included
in group name which is second argument of `vhost_traffic_status_filter_by_set_key` directive.

### To calculate traffic for individual storage volume
```Nginx
http {
    vhost_traffic_status_zone;

    ...

    server {

        ...

        location ~ ^/storage/(.+)/.*$ {
            set $volume $1;
            vhost_traffic_status_filter_by_set_key $volume storage::$server_name;
        }

        location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

* Calculate traffic for individual storage volume matched by regular expression of location directive.

### To calculate traffic for individual user agent
```Nginx
http {
    vhost_traffic_status_zone;

    map $http_user_agent $filter_user_agent {
        default 'unknown';
        ~iPhone ios;
        ~Android android;
        ~(MSIE|Mozilla) windows;
    }

    vhost_traffic_status_filter_by_set_key $filter_user_agent agent::*;

    ...

    server {

        ...

        vhost_traffic_status_filter_by_set_key $filter_user_agent agent::$server_name;

        location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

* Calculate traffic for individual `http_user_agent`

### To calculate traffic for detailed http status code
```Nginx
http {
    vhost_traffic_status_zone;

    server {

        ...

        vhost_traffic_status_filter_by_set_key $status $server_name;

        location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

* Calculate traffic for detailed `http status code`

`Caveats:` [$status](http://nginx.org/en/docs/http/ngx_http_core_module.html#variables) variable is available in nginx-(1.3.2, 1.2.2).

### To calculate traffic for dynamic dns

If the domain has multiple DNS A records, you can calculate traffic for individual IPs
for the domain using the filter feature or a variable in proxy_pass.

```Nginx
http {
    vhost_traffic_status_zone;

    upstream backend {
        elb.example.org:80;
    }

    ...

    server {

        ...

        location /backend {
            vhost_traffic_status_filter_by_set_key $upstream_addr upstream::backend;
            proxy_pass backend;
        }
    }
}
```

* Calculate traffic for individual IPs for the domain `elb.example.org`.
If `elb.example.org` has multiple DNS A records, will be display all IPs in `filterZones`.
In the above settings, as NGINX starts up or reloads it configuration,
it queries a DNS server to resolve domain and DNS A records is cached in memory.
Therefore the DNS A records are not changed in memory even if
DNS A records are chagned by DNS administrator unless NGINX re-starts up or reloads.

```Nginx
http {
    vhost_traffic_status_zone;

    resolver 10.10.10.53 valid=10s

    ...

    server {

        ...

        location /backend {
            set $backend_server elb.example.org;
            proxy_pass http://$backend_server;
        }
    }
}
```

* Calculate traffic for individual IPs for the domain `elb.example.org`.
If `elb.example.org`'s DNS A record is changed,
will be display both the old IP and the new IP in `::nogroups`.
Unlike the first upstream group setting, the second setting works well
even if DNS A records are chagned by DNS administrator.

`Caveats:` Please more details about NGINX DNS see the
[dns-service-discovery-nginx-plus](https://www.nginx.com/blog/dns-service-discovery-nginx-plus).

### To calculate traffic except for status page

```Nginx
http {
    vhost_traffic_status_zone;

    ...

    server {

        ...

        location /status {
            vhost_traffic_status_bypass_limit on;
            vhost_traffic_status_bypass_stats on;
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

* The `/status` uri is excluded from the status traffic calculation and limit feature. 
See the following directives:
  * [vhost_traffic_status_bypass_limit](#vhost_traffic_status_bypass_limit)
  * [vhost_traffic_status_bypass_stats](#vhost_traffic_status_bypass_stats)


### To maintain statistics data permanently

```Nginx
http {
    vhost_traffic_status_zone;
    vhost_traffic_status_dump /var/log/nginx/vts.db;

    ...

    server {

        ...

    }
}
```

* The `vhost_traffic_status_dump` directive maintains statistics data permanently
even if system has been rebooted or nginx has been restarted.
Please see the [vhost_traffic_status_dump](#vhost_traffic_status_dump) directive for detailed usage.

## Customizing
### To customize after the module installed
1. You need to change the `{{uri}}` string to your status uri in status.template.html as follows:
 ```
 shell> vi share/status.template.html
 ```
 ```
 var vtsStatusURI = "yourStatusUri/format/json", vtsUpdateInterval = 1000;
 ```

2. And then, customizing and copy status.template.html to server root directory as follows:
 ```
 shell> cp share/status.template.html /usr/share/nginx/html/status.html
 ```

4. Configure `nginx.conf`
 ```Nginx
    server {
        server_name example.org;
        root /usr/share/nginx/html;

        # Redirect requests for / to /status.html
        location = / {
            return 301 /status.html;
        }

        location = /status.html {}

        # Everything beginning /status (except for /status.html) is
        # processed by the status handler
        location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format json;
        }
    }

 ```

4. Access to your html.
 ```
 http://example.org/status.html
 ```

### To customize before the module installed
1. Modify `share/status.template.html` (Do not change `{{uri}}` string)

2. Recreate the `ngx_http_vhost_traffic_status_module_html.h` as follows:
 ```
 shell> cd util
 shell> ./tplToDefine.sh ../share/status.template.html > ../src/ngx_http_vhost_traffic_status_module_html.h
 ```

3. Add the module to the build configuration by adding
  `--add-module=/path/to/nginx-module-vts`

4. Build the nginx binary.

5. Install the nginx binary.


## Directives

![draw_io_vts_diagram](https://user-images.githubusercontent.com/3648408/42613122-279cdb70-85da-11e8-940e-e348bd8ea861.png "The order of nginx-module-vts module directives")

### vhost_traffic_status

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status** \<on\|off\> |
| **Default** | off |
| **Context** | http, server, location |

`Description:` Enables or disables the module working.
If you set `vhost_traffic_status_zone` directive, is automatically enabled.

### vhost_traffic_status_zone

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_zone** [shared:*name:size*] |
| **Default** | shared:vhost_traffic_status:1m |
| **Context** | http |

`Description:` Sets parameters for a shared memory zone that will keep states for various keys.
The cache is shared between all worker processes.
In most cases, the shared memory size used by nginx-module-vts does not increase much.
The shared memory size is increased pretty when using `vhost_traffic_status_filter_by_set_key`
directive but if filter's keys are fixed(*eg. the total number of the country code is about 240*)
it does not continuously increase.

If you use `vhost_traffic_status_filter_by_set_key` directive, set it as follows:

* Set to more than 32M shared memory size by default.
(`vhost_traffic_status_zone shared:vhost_traffic_status:32m`)
* If the message(*`"ngx_slab_alloc() failed: no memory in vhost_traffic_status_zone"`*)
printed in error_log, increase to more than (usedSize * 2).

### vhost_traffic_status_dump

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_dump** *path* [*period*] |
| **Default** | - |
| **Context** | http |

`Description:` Enables the statistics data dump and restore.
The *path* is a location to dump the statistics data.(e.g. `/var/log/nginx/vts.db`)
The *period* is a backup cycle time.(Default: 60s)
It is backed up immediately regardless of the backup cycle if nginx is exited by signal(`SIGKILL`).

### vhost_traffic_status_display

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_display** |
| **Default** | - |
| **Context** | http, server, location |

`Description:` Enables or disables the module display handler.

### vhost_traffic_status_display_format

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_display_format** \<json\|html\|jsonp\|prometheus\> |
| **Default** | json |
| **Context** | http, server, location |

`Description:` Sets the display handler's output format.
If you set `json`, will respond with a JSON document.
If you set `html`, will respond with the built-in live dashboard in HTML.
If you set `jsonp`, will respond with a JSONP callback function(default: *ngx_http_vhost_traffic_status_jsonp_callback*).
If you set `prometheus`, will respond with a [prometheus](https://prometheus.io) document.

### vhost_traffic_status_display_jsonp

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_display_jsonp** *callback* |
| **Default** | ngx_http_vhost_traffic_status_jsonp_callback |
| **Context** | http, server, location |

`Description:` Sets the callback name for the JSONP.

### vhost_traffic_status_display_sum_key

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_display_sum_key** *name* |
| **Default** | * |
| **Context** | http, server, location |

`Description:` Sets the sum key string in serverZones field's JSON. The default sum key string is the "*".

### vhost_traffic_status_filter

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_filter** \<on\|off\> |
| **Default** | on |
| **Context** | http, server, location |

`Description:` Enables or disables the filter features.

### vhost_traffic_status_filter_by_host

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_filter_by_host** \<on\|off\> |
| **Default** | off |
| **Context** | http, server, location |

`Description:` Enables or disables the keys by Host header field.
If you set `on` and nginx's server_name directive set several or wildcard name starting with an asterisk, e.g. “*.example.org”
and requested to server with hostname such as (a|b|c).example.org or *.example.org
then json serverZones is printed as follows:

```Nginx
server {
  server_name *.example.org;
  vhost_traffic_status_filter_by_host on;

  ...

}
```

```Json
  ...
  "serverZones": {
      "a.example.org": {
      ...
      },
      "b.example.org": {
      ...
      },
      "c.example.org": {
      ...
      }
      ...
   },
   ...
```

It provides the same function that set `vhost_traffic_status_filter_by_set_key $host`.

### vhost_traffic_status_filter_by_set_key

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_filter_by_set_key** *key* [*name*] |
| **Default** | - |
| **Context** | http, server, location |

`Description:` Enables the keys by user defined variable.
The *key* is a key string to calculate traffic.
The *name* is a group string to calculate traffic.
The *key* and *name* can contain variables such as $host, $server_name.
The *name*'s group belongs to `filterZones` if specified.
The *key*'s group belongs to `serverZones` if not specified second argument *name*.
The example with geoip module is as follows:

```Nginx
server {
  server_name example.org;
  vhost_traffic_status_filter_by_set_key $geoip_country_code country::$server_name;

  ...

}
```

```Json
  ...
  "serverZones": {
  ...
  },
  "filterZones": {
      "country::example.org": {
          "KR": {
              "requestCounter":...,
              "inBytes":...,
              "outBytes":...,
              "responses":{
                  "1xx":...,
                  "2xx":...,
                  "3xx":...,
                  "4xx":...,
                  "5xx":...,
                  "miss":...,
                  "bypass":...,
                  "expired":...,
                  "stale":...,
                  "updating":...,
                  "revalidated":...,
                  "hit":...,
                  "scarce":...
              },
              "requestMsecCounter":...,
              "requestMsec":...,
              "requestMsecs":{
                  "times":[...],
                  "msecs":[...]
              },
          },
          "US": {
          ...
          },
          ...
      },
      ...
  },
  ...

```

### vhost_traffic_status_filter_check_duplicate

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_filter_check_duplicate** \<on\|off\> |
| **Default** | on |
| **Context** | http, server, location |

`Description:` Enables or disables the deduplication of vhost_traffic_status_filter_by_set_key.
It is processed only one of duplicate values(`key` + `name`) in each directives(http, server, location) if this option is enabled.

### vhost_traffic_status_filter_max_node

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_filter_max_node** *number* [*string* ...] |
| **Default** | 0 |
| **Context** | http |

`Description:` Enables the limit of filter size using the specified *number* and *string* values.
If the *number* is exceeded, the existing nodes are deleted by the [LRU](https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU) algorithm.
The *number* argument is the size of the node that will be limited.
The default value `0` does not limit filters.
The one node is an object in `filterZones` in JSON document.
The *string* arguments are the matching string values for the group string value set by `vhost_traffic_status_filter_by_set_key` directive. 
Even if only the first part matches, matching is successful like the regular expression `/^string.*/`.
By default, If you do not set *string* arguments then it applied for all filters.


For examples:

`$ vi nginx.conf`

```Nginx
http {

    geoip_country /usr/share/GeoIP/GeoIP.dat;

    vhost_traffic_status_zone;

    # The all filters are limited to a total of 16 nodes.
    # vhost_traffic_status_filter_max_node 16

    # The `/^uris.*/` and `/^client::ports.*/` group string patterns are limited to a total of 64 nodes.
    vhost_traffic_status_filter_max_node 16 uris client::ports;

    ...

    server {

        server_name example.org;

        ...

        vhost_traffic_status_filter_by_set_key $uri uris::$server_name;
        vhost_traffic_status_filter_by_set_key $remote_port client::ports::$server_name;
        vhost_traffic_status_filter_by_set_key $geoip_country_code country::$server_name;

    }
}
```

`$ for i in {0..1000}; do curl -H 'Host: example.org' -i "http://localhost:80/test$i"; done`

![screenshot-vts-filter-max-node](https://user-images.githubusercontent.com/3648408/41475027-96c96136-70f8-11e8-8dd6-ed1825d7b216.png)

In the above example, the `/^uris.*/` and `/^client::ports.*/` group string patterns are limited to a total of 16 nodes.
The other filters like `country::.*` are not limited.

### vhost_traffic_status_limit

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_limit** \<on\|off\> |
| **Default** | on |
| **Context** | http, server, location |

`Description:` Enables or disables the limit features.

### vhost_traffic_status_limit_traffic

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_limit_traffic** *member*:*size* [*code*] |
| **Default** | - |
| **Context** | http, server, location |

`Description:` Enables the traffic limit for specified *member*.
The *member* is a member string to limit traffic.
The *size* is a size(k/m/g) to limit traffic.
The *code* is a code to return in response to rejected requests.(Default: 503)

The available *`member`* strings are as follows:
* **request**
  * The total number of client requests received from clients.
* **in**
  * The total number of bytes received from clients.
* **out**
  * The total number of bytes sent to clients.
* **1xx**
  * The number of responses with status codes 1xx.
* **2xx**
  * The number of responses with status codes 2xx.
* **3xx**
  * The number of responses with status codes 3xx.
* **4xx**
  * The number of responses with status codes 4xx.
* **5xx**
  * The number of responses with status codes 5xx.
* **cache_miss**
  * The number of cache miss.
* **cache_bypass**
  * The number of cache bypass.
* **cache_expired**
  * The number of cache expired.
* **cache_stale**
  * The number of cache stale.
* **cache_updating**
  * The number of cache updating.
* **cache_revalidated**
  * The number of cache revalidated.
* **cache_hit**
  * The number of cache hit.
* **cache_scarce**
  * The number of cache scare.

### vhost_traffic_status_limit_traffic_by_set_key

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_limit_traffic_by_set_key** *key* *member*:*size* [*code*] |
| **Default** | - |
| **Context** | http, server, location|

`Description:` Enables the traffic limit for specified *key* and *member*.
The *key* is a key string to limit traffic.
The *member* is a member string to limit traffic.
The *size* is a size(k/m/g) to limit traffic.
The *code* is a code to return in response to rejected requests.(Default: 503)


The *`key`* syntax is as follows:
* *`group`*@[*`subgroup`*@]*`name`*

The available *`group`* strings are as follows:
* **NO**
  * The group of server.
* **UA**
  * The group of upstream alone.
* **UG**
  * The group of upstream group.(use *`subgroup`*)
* **CC**
  * The group of cache.
* **FG**
  * The group of filter.(use *`subgroup`*)

The available *`member`* strings are as follows:
* **request**
  * The total number of client requests received from clients.
* **in**
  * The total number of bytes received from clients.
* **out**
  * The total number of bytes sent to clients.
* **1xx**
  * The number of responses with status codes 1xx.
* **2xx**
  * The number of responses with status codes 2xx.
* **3xx**
  * The number of responses with status codes 3xx.
* **4xx**
  * The number of responses with status codes 4xx.
* **5xx**
  * The number of responses with status codes 5xx.
* **cache_miss**
  * The number of cache miss.
* **cache_bypass**
  * The number of cache bypass.
* **cache_expired**
  * The number of cache expired.
* **cache_stale**
  * The number of cache stale.
* **cache_updating**
  * The number of cache updating.
* **cache_revalidated**
  * The number of cache revalidated.
* **cache_hit**
  * The number of cache hit.
* **cache_scarce**
  * The number of cache scare.

The *member* is the same as `vhost_traffic_status_limit_traffic` directive.

### vhost_traffic_status_limit_check_duplicate

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_limit_check_duplicate** \<on\|off\> |
| **Default** | on |
| **Context** | http, server, location |

`Description:` Enables or disables the deduplication of vhost_traffic_status_limit_by_set_key.
It is processed only one of duplicate values(`member` | `key` + `member`)
in each directives(http, server, location) if this option is enabled.

### vhost_traffic_status_set_by_filter

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_set_by_filter** *$variable* *group*/*zone*/*name* |
| **Default** | - |
| **Context** | http, server, location, if |

`Description:` Get the specified status value stored in shared memory.
It can acquire almost all status values and the obtained value is stored in *$variable* which is first argument.

* **group**
  * server
  * filter
  * upstream@alone
  * upstream@group
  * cache
* **zone**
  * server
    * *name*
  * filter
    * *filter_group*@*name*
  * upstream@group
    * *upstream_group*@*name*
  * upstream@alone
    * @*name*
  * cache
    * *name*
* **name**
  * requestCounter
    * The total number of client requests received from clients.
  * requestMsecCounter
    * The number of accumulated request processing time in milliseconds.
  * requestMsec
    * The average of request processing times in milliseconds.
  * responseMsecCounter
    * The number of accumulated only upstream response processing time in milliseconds.
  * responseMsec
    * The average of only upstream response processing times in milliseconds.
  * inBytes
    * The total number of bytes received from clients.
  * outBytes
    * The total number of bytes sent to clients.
  * 1xx, 2xx, 3xx, 4xx, 5xx
    * The number of responses with status codes 1xx, 2xx, 3xx, 4xx, and 5xx.
  * cacheMaxSize
    * The limit on the maximum size of the cache specified in the configuration.
  * cacheUsedSize
    * The current size of the cache.
  * cacheMiss
    * The number of cache miss.
  * cacheBypass
    * The number of cache bypass.
  * cacheExpired
    * The number of cache expired.
  * cacheStale
    * The number of cache stale.
  * cacheUpdating
    * The number of cache updating.
  * cacheRevalidated
    * The number of cache revalidated.
  * cacheHit
    * The number of cache hit.
  * cacheScarce
    * The number of cache scare.
  * weight
    * Current weight setting of the server.
  * maxFails
    * Current max_fails setting of the server.
  * failTimeout
    * Current fail_timeout setting of the server.
  * backup
    * Current backup setting of the server.(0\|1)
  * down
    * Current down setting of the server.(0\|1)

`Caveats:` The *name* is case sensitive. All return values take the integer type.

For examples:
* requestCounter in serverZones
  * **vhost_traffic_status_set_by_filter** `$requestCounter` `server/example.org/requestCounter`
* requestCounter in filterZones
  * **vhost_traffic_status_set_by_filter** `$requestCounter` `filter/country::example.org@KR/requestCounter`
* requestCounter in upstreamZones
  * **vhost_traffic_status_set_by_filter** `$requestCounter` `upstream@group/backend@10.10.10.11:80/requestCounter`
* requestCounter in upstreamZones::nogroups
  * **vhost_traffic_status_set_by_filter** `$requestCounter` `upstream@alone/10.10.10.11:80/requestCounter`
* cacheHit in cacheZones
  * **vhost_traffic_status_set_by_filter** `$cacheHit` `cache/my_cache_name/cacheHit`

### vhost_traffic_status_average_method

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_average_method** \<AMM\|WMA\> [*period*] |
| **Default** | AMM 60s |
| **Context** | http, server, location |

`Description:` Sets the method which is a formula that calculate the average of response processing times.
The *period* is an effective time of the values used for the average calculation.(Default: 60s)
If *period* set to 0, effective time is ignored.
In this case, the last average value is displayed even if there is no requests and after the elapse of time.
The corresponding values are `requestMsec` and `responseMsec` in JSON.

* **AMM**
  * The AMM is the [arithmetic mean](https://en.wikipedia.org/wiki/Arithmetic_mean).
* **WMA**
  * THE WMA is the [weighted moving average](https://en.wikipedia.org/wiki/Moving_average#Weighted_moving_average).

### vhost_traffic_status_histogram_buckets

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_histogram_buckets** *second* ... |
| **Default** | - |
| **Context** | http, server, location |

`Description:` Sets the observe buckets to be used in the histograms.
By default, if you do not set this directive, it will not work.
The *second* can be expressed in decimal places with a minimum value of 0.001(1ms).
The maximum size of the buckets is 32. If this value is insufficient for you,
change the `NGX_HTTP_VHOST_TRAFFIC_STATUS_DEFAULT_BUCKET_LEN` in the `src/ngx_http_vhost_traffic_status_node.h`

For examples:
* **vhost_traffic_status_histogram_buckets** `0.005` `0.01` `0.05` `0.1` `0.5` `1` `5` `10`
  * The observe buckets are [5ms 10ms 50ms 1s 5s 10s].
* **vhost_traffic_status_histogram_buckets** `0.005` `0.01` `0.05` `0.1`
  * The observe buckets are [5ms 10ms 50ms 100ms].

`Caveats:` By default, if you do not set this directive, the histogram statistics does not work.
The restored histograms by `vhost_traffic_status_dump` directive have no affected by changes to the buckets
by `vhost_traffic_status_histogram_buckets` directive.
So you must first delete the zone or the dump file before changing the buckets
by `vhost_traffic_status_histogram_buckets` directive.
Similar to the above, delete the dump file when using the histogram for the first time.

### vhost_traffic_status_bypass_limit

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_bypass_limit** \<on\|off\> |
| **Default** | off |
| **Context** | http, server, location |

`Description:` Enables or disables to bypass `vhost_traffic_status_limit` directives.
The limit features is bypassed if this option is enabled.
This is mostly useful if you want to connect the status web page like `/status` regardless of `vhost_traffic_status_limit` directives as follows:

```Nginx
http {
    vhost_traffic_status_zone;

    ...

    server {

        ...

        location /status {
            vhost_traffic_status_bypass_limit on;
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

### vhost_traffic_status_bypass_stats

| -   | - |
| --- | --- |
| **Syntax**  | **vhost_traffic_status_bypass_stats** \<on\|off\> |
| **Default** | off |
| **Context** | http, server, location |

`Description:` Enables or disables to bypass `vhost_traffic_status`.
The traffic status stats features is bypassed if this option is enabled.
In other words, it is excluded from the traffic status stats.
This is mostly useful if you want to ignore your request in status web page like `/status` as follows:

```Nginx
http {
    vhost_traffic_status_zone;

    ...

    server {

        ...

        location /status {
            vhost_traffic_status_bypass_stats on;
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
```

## Releases

To cut a release, create a changelog entry PR with [git-chglog](https://github.com/git-chglog/git-chglog)

    version="v0.2.0"
    git checkout -b "cut-${version}"
    git-chglog -o CHANGELOG.md --next-tag "${version}"
    git add CHANGELOG.md
    sed -i "s/NGX_HTTP_VTS_MODULE_VERSION \".*/NGX_HTTP_VTS_MODULE_VERSION \"${version}\"/" src/ngx_http_vhost_traffic_status_module.h
    git add src/ngx_http_vhost_traffic_status_module.h
    git-chglog -t .chglog/RELNOTES.tmpl --next-tag "${version}" "${version}" | git commit -F-
    
After the PR is merged, create the new tag and release on the [GitHub Releases](https://github.com/vozlt/nginx-module-vts/releases).

## See Also
* Stream traffic status
  * [nginx-module-sts](https://github.com/vozlt/nginx-module-sts)
  * [nginx-module-stream-sts](https://github.com/vozlt/nginx-module-stream-sts)

* Prometheus
  * [nginx-vts-exporter](https://github.com/hnlq715/nginx-vts-exporter)

* System protection
  * [nginx-module-sysguard](https://github.com/vozlt/nginx-module-sysguard)

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-vts](https://github.com/vozlt/nginx-module-vts){target=_blank}.

# *webp*: NGINX WebP module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-webp
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_webp_module.so;
```


This document describes nginx-module-webp [v0.1.1.5](https://github.com/dvershinin/ngx_webp/releases/tag/0.1.1.5){target=_blank} 
released on Dec 30 2019.

<hr />

Webp is new (and smaller) image format. This module will convert jpg/png image on fly and send webp response.

## Status

Under development. To be continued.

## Configuration directives

### `webp`

- **syntax**: `webp`
- **context**: `location`

Enables or disables module.

### Example

location ~ "\.jpg" {
webp;
}

$ curl -SLIXGET -H "accept:image/webp" http://127.0.0.1/1.jpg

HTTP/1.1 200 OK

Server: nginx/1.13.12

Date: Wed, 25 Apr 2018 10:16:45 GMT

Content-Length: 223980

Last-Modified: Wed, 25 Apr 2018 10:16:45 GMT

Connection: keep-alive

Content-Type: image/webp



$ curl -SLIXGET -H "accept:image/*" http://127.0.0.1/1.jpg

HTTP/1.1 200 OK

Server: nginx/1.13.12

Date: Wed, 25 Apr 2018 10:17:53 GMT

Content-Length: 325991

Last-Modified: Wed, 18 Apr 2018 19:55:14 GMT

Connection: keep-alive

Content-Type: image/jpeg

### Notice
As webp convertion takes some CPU usage I recommend to use some kind of caching of nginx responses, like Varnish.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-webp](https://github.com/dvershinin/ngx_webp){target=_blank}.

# *xslt*: NGINX XSLT dynamic module


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-xslt
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_xslt_filter_module.so;
```

<hr />


## Directives

You may find information about configuration directives for this module at the following links:        

*   https://nginx.org/en/docs/http/ngx_http_xslt_module.html#directives

# *xss*: Native cross-site scripting support in NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-xss
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_xss_filter_module.so;
```


This document describes nginx-module-xss [v0.6](https://github.com/dvershinin/xss-nginx-module/releases/tag/v0.6){target=_blank} 
released on Dec 26 2022.

<hr />

xss-nginx-module - Native cross-site scripting support in nginx

## Synopsis

```nginx
## accessing /foo?callback=process gives the response
## body "process(...);" (without quotes) where "..."
## is the original response body of the /foo location.
server {
    location /foo {
        # your content handler goes here...

        xss_get on;
        xss_callback_arg 'callback';
        xss_input_types 'application/json'; # default
        xss_output_type 'application/x-javascript'; # default
    }
    ...
}
```

## Description

This module adds cross-site AJAX support to nginx. Currently only
cross-site GET is supported. But cross-site POST will be added
in the future.

The cross-site GET is currently implemented as JSONP
(or "JSON with padding"). See http://en.wikipedia.org/wiki/JSON#JSONP
for more details.

## Directives


## xss_get
**syntax:** *xss_get on | off*

**default:** *xss_get off*

**context:** *http, server, location, if location*

Enables JSONP support for GET requests.


## xss_callback_arg
**syntax:** *xss_callback_arg &lt;name&gt;*

**default:** *none*

**context:** *http, http, location, if location*

Specifies the JavaScript callback function name
used in the responses.

For example,

```nginx
location /foo {
    xss_get on;
    xss_callback_arg c;
    ...
}
```

then

```
GET /foo?c=blah
```

returns

```javascript
blah(...);
```


## xss_override_status
**syntax:** *xss_override_status on | off*

**default:** *xss_check_status on*

**context:** *http, server, location, if location*

Specifies whether to override 30x, 40x and 50x status to 200
when the response is actually being processed.


## xss_check_status
**syntax:** *xss_check_status on | off*

**default:** *xss_check_status on*

**context:** *http, server, location, if location*

By default, ngx_xss only process responses with the status code
200 or 201.


## xss_input_types
**syntax:** *xss_input_types [mime-type]...*

**default:** *xss_input_types application/json*

**context:** *http, server, location, if location*

Only processes the responses of the specified MIME types.

Example:

```nginx
xss_input_types application/json text/plain;
```


## Limitations

* ngx_xss will not work with [ngx_echo](https://github.com/openresty/echo-nginx-module)'s
subrequest interfaces, due to the underlying
limitations imposed by subrequests' "postponed chain" mechanism in the nginx core.
The standard ngx_addition module also falls into this category.  You're recommended,
however, to use [ngx_lua](https://github.com/openresty/lua-nginx-module) as the content
handler to issue subrequests *and* ngx_xss
to do JSONP, because [ngx_lua](https://github.com/openresty/lua-nginx-module)'s
[ngx.location.capture()](https://github.com/openresty/lua-nginx-module#ngxlocationcapture)
interface does not utilize the "postponed chain" mechanism, thus getting out of this
limitation. We're taking this approach in production and it works great.


## Trouble Shooting

Use the "info" error log level (or lower) to get more
diagnostics when things go wrong.


## See Also

* [Introduction to JSONP](http://en.wikipedia.org/wiki/JSONP)
* [ngx_lua](https://github.com/openresty/lua-nginx-module)


## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-xss](https://github.com/dvershinin/xss-nginx-module){target=_blank}.

# *zip*: Streaming ZIP archiver for NGINX


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-zip
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_zip_module.so;
```


This document describes nginx-module-zip [v1.2.1](https://github.com/dvershinin/mod_zip/releases/tag/1.2.1){target=_blank} 
released on Jul 20 2022.

<hr />

mod_zip assembles ZIP archives dynamically. It can stream component files from
upstream servers with nginx's native proxying code, so that the process never
takes up more than a few KB of RAM at a time, even while assembling archives that
are (potentially) gigabytes in size.

mod_zip supports a number of "modern" ZIP features, including large files, UTC
timestamps, and UTF-8 filenames. It allows clients to resume large downloads using
the "Range" and "If-Range" headers, although these feature require the server
to know the file checksums (CRC-32's) in advance. See "Usage" for details.

To unzip files on the fly, check out [nginx-unzip-module](https://github.com/youzee/nginx-unzip-module).


## Usage

The module is activated when the original response (presumably from an
upstream) includes the following HTTP header:

    X-Archive-Files: zip

It then scans the response body for a list of files. The syntax is a 
space-separated list of the file checksum (CRC-32), size (in bytes), location
(properly URL-encoded), and file name. One file per line.  The file location
corresponds to a location in your nginx.conf; the file can be on disk, from an
upstream, or from another module.  The file name can include a directory path,
and is what will be extracted from the ZIP file. Example:

    1034ab38 428    /foo.txt   My Document1.txt
    83e8110b 100339 /bar.txt   My Other Document1.txt
    0        0      @directory My empty directory

Files are retrieved and encoded in order. If a file cannot be found or the file
request returns any sort of error, the download is aborted.

The CRC-32 is optional. Put "-" if you don't know the CRC-32; note that in this
case mod_zip will disable support for the `Range` header.

A special URL marker `@directory` can be used to declare a directory entry
within an archive. This is very convenient when you have to package a tree of
files, including some empty directories. As they have to be declared explicitly.

If you want mod_zip to include some HTTP headers of the original request, in the
sub-requests that fetch content of files, then pass the list of their names in
the following HTTP header:

    X-Archive-Pass-Headers: <header-name>[:<header-name>]*


## Re-encoding filenames

To re-encode the filenames as UTF-8, add the following header to the upstream
response:

    X-Archive-Charset: [original charset name]

The original charset name should be something that iconv understands. (This feature
only works if iconv is present.)

If you set original charset as `native`:

    X-Archive-Charset: native;

filenames from the file list are treated as already in the system native charset.
Consequently, the ZIP general purpose flag (bit 11) that indicates UTF-8 encoded
names will not be set, and archivers will know it's a native charset.

Sometimes there is problem converting UTF-8 names to native(CP866) charset that
causes popular archivers to fail to recognize them. And at the same time you want
data not to be lost so that smart archivers can use Unicode Path extra field.
You can provide you own, adapted representation of filename in native charset along
with original UTF-8 name in one string. You just need to add following header:

    X-Archive-Name-Sep: [separator];

So your file list should look like:

    <CRC-32> <size> <path> <native-filename><separator><utf8-filename>
    ...

then filename field will contatin `native-filename` and Unicode Path extra field
will contain `utf8-filename`.

## Tips

1. Add a header "Content-Disposition: attachment; filename=foobar.zip" in the
upstream response if you would like the client to name the file "foobar.zip"

1. To save bandwidth, add a "Last-Modified" header in the upstream response; 
mod_zip will then honor the "If-Range" header from clients.

1. To wipe the X-Archive-Files header from the response sent to the client,
use the headers_more module: http://wiki.nginx.org/NginxHttpHeadersMoreModule

1. To improve performance, ensure the backends are not returning gzipped
files. You can achieve this with `proxy_set_header Accept-Encoding "";`
in the location blocks for the component files.

Questions/patches may be directed to Evan Miller, emmiller@gmail.com.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-zip](https://github.com/dvershinin/mod_zip){target=_blank}.

# *zstd*: NGINX module for the Zstandard compression


## Installation

You can install this module in any RHEL-based distribution, including, but not limited to:

* RedHat Enterprise Linux 7, 8, 9
* CentOS 7, 8, 9
* AlmaLinux 8, 9
* Rocky Linux 8, 9
* Amazon Linux 2 and Amazon Linux 2023

```bash
yum -y install https://extras.getpagespeed.com/release-latest.rpm
yum -y install nginx-module-zstd
```

Enable the module by adding the following at the top of `/etc/nginx/nginx.conf`:

```nginx
load_module modules/ngx_http_zstd_filter_module.so;
```
```nginx
load_module modules/ngx_http_zstd_static_module.so;
```


This document describes nginx-module-zstd [v0.1.1](https://github.com/tokers/zstd-nginx-module/releases/tag/0.1.1){target=_blank} 
released on Oct 23 2023.

<hr />
zstd-nginx-module - Nginx module for the [Zstandard compression](https://facebook.github.io/zstd/).

## Table of Content

* [Name](#name)
* [Status](#status)
* [Synopsis](#synopsis)
* [Installation](#installation)
* [Directives](#directives)
  * [ngx_http_zstd_filter_module](#ngx_http_zstd_filter_module)
    * [zstd_dict_file](#zstd_dict_file)
    * [zstd](#zstd)
    * [zstd_comp_level](#zstd_comp_level)
    * [zstd_min_length](#zstd_min_length)
    * [zstd_types](#zstd_types)
    * [zstd_buffers](#zstd_buffers)
  * [ngx_http_zstd_static_module](#ngx_http_zstd_static_module)
    * [zstd_static](#zstd_static)
* [Variables](#variables)
  * [ngx_http_zstd_filter_module](#ngx_http_zstd_filter_module)
    * [$zstd_ratio](#$zstd_ratio)
* [Author](#author)

## Status

This Nginx module is currently considered experimental. Issues and PRs are welcome if you encounter any problems.

## Synopsis

```nginx

## specify the dictionary
zstd_dict_file /path/to/dict;

server {
    listen 127.0.0.1:8080;
    server_name localhost;

    location / {
        # enable zstd compression
        zstd on;
        zstd_min_length 256; # no less than 256 bytes
        zstd_comp_level 3; # set the level to 3

        proxy_pass http://foo.com;
    }
}

server {
    listen 127.0.0.1:8081;
    server_name localhost;

    location / {
        zstd_static on;
        root html;
    }
}
```

## Directives

## ngx_http_zstd_filter_module

The `ngx_http_zstd_filter_module` module is a filter that compresses responses using the "zstd" method. This often helps to reduce the size of transmitted data by half or even more.

### zstd_dict_file

**Syntax:** *zstd_dict_file /path/to/dict;*  
**Default:** *-*  
**Context:** *http*  

Specifies the external dictionary.

**WARNING:** Be careful! The content-coding registration only specifies a means to signal the use of the zstd format, and does not additionally specify any mechanism for advertising/negotiating/synchronizing the use of a specific dictionary between client and server. Use the `zstd_dict_file` only if you can insure that both ends (server and client) are capable of  using the same dictionary (e.g. advertise with a HTTP header). See https://github.com/tokers/zstd-nginx-module/issues/2 for the details.

### zstd

**Syntax:** *zstd on | off;*  
**Default:** *zstd off;*  
**Context:** *http, server, location, if in location*

Enables or disables zstd compression for response.

### zstd_comp_level

**Syntax:** *zstd_comp_level level;*  
**Default:** *zstd_comp_level 1;*  
**Context:** *http, server, location*

Sets a zstd compression level of a response. Acceptable values are in the range from 1 to `ZSTD_maxCLevel()`.

### zstd_min_length

**Syntax:** *zstd_min_length length;*  
**Default:** *zstd_min_length 20;*  
**Context:** *http, server, location*

Sets the minimum length of a response that will be compressed by zstd. The length is determined only from the "Content-Length" response header field.

### zstd_types

**Syntax:** *zstd_types mime-type ...;*  
**Default:** *zstd_types text/html;*  
**Context:** *http, server, location*

Enables ztd of responses for the specified MIME types in addition to "text/html". The special value "*" matches any MIME type.

### zstd_buffers

**Syntax:** *zstd_buffers number size;*  
**Default:** *zstd_buffers 32 4k | 16 8k;*  
**Context:** *http, server, location*

Sets the number and size of buffers used to compress a response. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.

## ngx_http_zstd_static_module

The `ngx_http_zstd_static_module` module allows sending precompressed files with the ".zst" filename extension instead of regular files.

### zstd_static

**Syntax:**	*zstd_static on | off | always;*  
**Default:** *zstd_static off;*  
**Context:** *http, server, location*  

Enables ("on") or disables ("off") checking the existence of precompressed files. The following directives are also taken into account: gzip_vary.

With the "always" value, "zsted" file is used in all cases, without checking if the client supports it.


## Variables

## ngx_http_zstd_filter_module

### $zstd_ratio

Achieved compression ratio, computed as the ratio between the original and compressed response sizes.

## GitHub

You may find additional configuration tips and documentation for this module in the [GitHub 
repository for 
nginx-module-zstd](https://github.com/tokers/zstd-nginx-module){target=_blank}.

